{"posts":[{"title":"2020 ATAC-seq analysis","text":"ATAC-seq最新处理流程 2020 ATAC-seq analysis一、ATAC-seq个性化的conda environment的搭建1、git clonegit clone文章中提到的atac-seq-pipeline (里面有你接下来分析所需要的绝大部分东西。另外一定要去看README.md，这个文档会告诉你分析ATAC-seq需要做些准备，进行些什么工作。) 12$ cd ~/ATAC$ git clone https://github.com/ENCODE-DCC/atac-seq-pipeline 2、环境搭建（1）conda base environment搭建 1234#一路yes下去$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh$ bash Miniconda3-4.6.14-Linux-x86_64.sh#最后一个yes时你同意了每次自动启动conda的base环境，显示安装完成后，一定要退出当前的终端，再重新打开一次，你就会发现在游标的最左边多了一个（base）,你的base conda environment就激活啦！你的conda环境搭建就成功了一小步(*^__^*) 嘻嘻…… （2）失活掉base conda environment自动激活的设置 12$ conda config --set auto_activate_base false#同样的完成上诉操作后，记得退出当前终端，再次打开一个新的终端。你就会发现游标的最左边没有（base）了 （3）安装pipeline’s conda environment ！？你是不是充满了疑问！？下面的两个shell脚本哪里冒出来的，干什么用的。不要忘了，我们第一步git clone了这个atac-seq-pipeline呢！这两个脚本就是就是来源于这个clone下来的pipeline的。 1234# 网不给力的朋友，比如说我，这一部就是整个分析流程的限速步骤！！！网络给力这一步也需要花一些时间，读一下两个shell文件就知道为什么啦。$ bash scripts/uninstall_conda_env.sh # uninstall it for clean-install#下面这个shell脚本会创建两个分析ATAC-seq数据所需要的环境，一个基于python3，一个基于python2的$ bash scripts/install_conda_env.sh （4）激活安装的conda环境 12$ conda activate encode-atac-seq-pipeline#你一定会疑问为什么是encode-atac-seq-pipeline,阅读一下scripts/install_conda_env.sh这个脚本就知道啦。因为源文档作者把这个环境名命名为encode-atac-seq-pipeline。 （5）查看所有的conda环境 1conda env list 我们会看到有一个基于python2的环境，没有标明python版本的那个encode-atac-seq-pipeline是基于python3的。有些软件是要基于特定版本的python运行的。 二、数据下载123456789#一次性下载所有的......_1.fastq.gz和......_2.fastq.gz样本dir=/home/gongyuqi/.aspera/connect/etc/asperaweb_id_dsa.opensshx=_1y=_2for id in {93,98,99}doascp -QT -l 300m -P33001 -i $dir era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR126/0$id/SRR126920$id/SRR126920$id$x.fastq.gz .ascp -QT -l 300m -P33001 -i $dir era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR126/0$id/SRR126920$id/SRR126920$id$y.fastq.gz . done 三、bowtie2比对1、 fastqc+multiqc查看原始数据的质量情况1234conda install -y fastqc multiqccd raw fastqc -t 8 *gz -o ./multiqc *.gzip -o ./ multiqc结果表明原始数据质量已经很好了，完全没有adaptor残留,连PCR重复都是勉强过关的。per base sequence content和kmer content两项指标不合格，但是并不会对后续的比对造成多大的负面影响，同时这两项指标通过过滤软件的处理可能并不会后得到改善。这样的数据其实可以直接比对的。 2、 过滤低质量的reads（如果原始数据需要进行过滤处理，那么走下面这个过滤流程，我们这里就不过滤了） 1234567891011nohup cat rawdata.txt|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}trim_galore -j 35 --phred33 --length 35 -e 0.1 --stringency 4 --paired -o ../clean $fq1 $fq2done &amp;#过滤后的数据质量控制，看数据是否有得到改善，有没有去掉adaptor非常重要cd cleanfastqc -t 20 *gz -o ./multiqc *.gzip -o ./ 3、 比对参考基因组 比对 12345678nohup cat sample.txt | while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}sample=${fq1%%_*}.sort.bambowtie2 --very-sensitive -X2000 --mm --threads 8 -x /home/gongyuqi/ref/mm10/index/mm10 -1 $fq1 -2 $fq2|samtools sort -O bam -@ 8 - -o ../align/$sample done &amp; 比对情况统计及查看 12345ls *.bam | while read iddonohup samtools flagstat -@ 4 $id &gt; ${id%%.*}.stat &amp;donels *.stat | while read id; do cat $id | grep &quot;mapped (&quot;;done 生成bam文件的索引文件 1nohup ls *sort.bam|while read id; do samtools index -@ 10 $id;done &amp; 四、PCR去除重及质控制1、初步过滤#=============================#Remove unmapped, mate unmapped#not primary alignment, reads failing platform#Only keep properly paired reads#Obtain name sorted BAM file#============================================= 123456#fixmate要求输入的bam文件按名称排序，所以第一步初步过滤后要按reads名称排序一下nohup ls *.sort.bam|while read id; do samtools view -@ 8 -F 524 -f 2 -u $id | samtools sort -@ 8 -n /dev/stdin -o ../filter/${id%%.*}.filt.nmsrt.bam;done &amp;#multimapping=4是bowtie2的默认设置cd ../filtermultimapping=4 nohup ls *filt.nmsrt.bam|while read id;do samtools view -@ 8 -h $id|assign_multimappers.py -k $multimapping --paired-end |samtools fixmate -@ 8 -r /dev/stdin ${id%%.*}.filt.nmsrt.fixmate.bam; done &amp; 2、进一步#===============================================#Remove orphan reads (pair was removed)#and read pairs mapping to different chromosomes#Obtain position sorted BAM#=============================================== 12#多个样本写循环nohup ls *.filt.nmsrt.fixmate.bam|while read id; do samtools view -@ 8 -F 1804 -f 2 -u $id | samtools sort -@ 8 /dev/stdin -o ${id%%.*}.prefilt.bam;done &amp; 3、标记PCR重复#===============#Mark duplicates#=============== 123MARKDUP=/home/gongyuqi/miniconda3/envs/encode-atac-seq-pipeline/share/picard-2.20.7-0/picard.jar#多个样本写循环nohup ls *.prefilt.bam|while read id;do java -Xmx16G -jar $MARKDUP MarkDuplicates INPUT=$id OUTPUT=${id%%.*}.prefilt.dupmark.bam METRICS_FILE=${id%%.*}.prefilt.dupmark.qc VALIDATION_STRINGENCY=LENIENT ASSUME_SORTED=true REMOVE_DUPLICATES=false;done &amp; 4、去除PCR重复、建立索引文件、按name排序的bam文件、质控文件#============================#Remove duplicates#Index final position sorted BAM#Create final name sorted BAM#============================ 12345#多个样本写循环#去重nohup ls *.prefilt.dupmark.bam|while read id;do samtools view -@ 16 -F 1804 -f 2 -b $id &gt; ${id%%.*}.nodup.bam;done &amp;#构建索引文件nohup ls *nodup.bam|while read id;do samtools index $id;done &amp; 5、测序文库复杂度的检验#=============================#Compute library complexity#=============================#Sort by name#convert to bedPE and obtain fragment coordinates#sort by position and strand#Obtain unique count statistics#================================================ 123#多个样本写循环nohup ls *.nodup.bam|while read id;do samtools sort -@ 16 -n $id -o ${id%%.*}.nodup.srt.name.bam;done &amp;nohup ls *.nodup.srt.name.bam|while read id; do bedtools bamtobed -bedpe -i $id | awk 'BEGIN{OFS=&quot;\\t&quot;}{print $1,$2,$4,$6,$9,$10}' | grep -v 'chrM' | sort | uniq -c | awk 'BEGIN{mt=0;m0=0;m1=0;m2=0} ($1==1){m1=m1+1} ($1==2){m2=m2+1} {m0=m0+1} {mt=mt+$1} END{m1_m2=-1.0; if(m2&gt;0) m1_m2=m1/m2;printf &quot;%d\\t%d\\t%d\\t%d\\t%f\\t%f\\t%f\\n&quot;,mt,m0,m1,m2,m0/mt,m1/m0,m1_m2}' &gt; ${id%%.*}.nodup.pbc.qc;done &amp; Library complexity measures计算结果如下，…nodup.pbc.qc文件格式为：TotalReadPairsDistinctReadPairsOneReadPairTwoReadPairsNRF=Distinct/TotalPBC1=OnePair/DistinctPBC2=OnePair/TwoPair 针对NRF、PBC1、PBC2这几个指标，ENCODE官网提供的标准如下: 我们此次4个样本的分析结果显示这几个指标的值如下所示： 除了第一个样本（SRR12692092），其他样本计算结果显示NRF、PBC1、PBC2的值都非常完美，说明我们进行过滤和PCR去重的bam文件质量上没有问题，可以用于后续的分析。 6、插入片段质控检测用于评估ATAC/Chip实验质量好坏的一个重要指标 123#多个样本循环picard_jar=/home/gongyuqi/miniconda3/envs/encode-atac-seq-pipeline/share/picard-2.20.7-0/picard.jarnohup ls *.nodup.srt.name.bam | while read id; do java -Xmx6G -XX:ParallelGCThreads=1 -jar ${picard_jar} CollectInsertSizeMetrics INPUT=$id OUTPUT=${id%%.*}.INSERT.DATA H=${id%%.*}.INSERT.PLOT.pdf VERBOSITY=ERROR QUIET=TRUE USE_JDK_DEFLATER=TRUE USE_JDK_INFLATER=TRUE W=1000 STOP_AFTER=5000000; done &amp; 成功的ATAC-seq实验会生成具有逐级递减和周期性的峰，对应无核小体区域（Nucleosome free region, NRF,100bp）,单核区域（mono-nucleosome,200bp），双核区域（400bp），三核区域（600bp）。 本次结果显示样本质量OK 7、TSS富集以BRM014-10uM_24h_wt样本为例，可视化peaks在基因组上的分布特征(TSS区域；TSS、gene-body、TSE区域) both -R and -S can accept multiple files UCSC官网下载小鼠参考基因组注释文件，详情参考：http://www.bio-info-trainee.com/2494.html 123456789101112131415161718192021222324252627282930#生成bw文件用于后续的computeMatrixnohup ls *.nodup.bam |while read iddonohup bamCoverage --normalizeUsing CPM -b $id -o ${id%%.*}.nodup.bwdone &amp;#注释文件绝对路劲refseq=/home/gongyuqi/ref/mm10/mouse.ucsc.refseq.bed#TSS##生成plotHeatmap需要的gz文件nohup computeMatrix reference-point --referencePoint TSS -p 30 \\-b 3000 -a 3000 \\-R $refseq \\-S SRR12692098.nodup.bw SRR12692099.nodup.bw \\--skipZeros -o BRM014-10uM_24h_wt_TSS.gz \\--outFileSortedRegions BRM014-10uM_24h_wt_TSS.bed &amp;##可视化文件nohup plotHeatmap -m BRM014-10uM_24h_wt_TSS.gz -out BRM014-10uM_24h_wt_TSS_rainbow.png --colorMap rainbow &amp;#TSS_gene-body_TSE##生成plotHeatmap需要的gz文件nohup computeMatrix scale-regions -p 30 \\-a 3000 -b 3000 -m 5000 \\-R $refseq \\-S SRR12692098.nodup.bw SRR12692099.nodup.bw \\--skipZeros -o BRM014-10uM_24h_wt_TSS_gene0body_TSE.gz \\--outFileSortedRegions BRM014-10uM_24h_wt_TSS.bed &amp;##可视化文件nohup plotHeatmap -m BRM014-10uM_24h_wt_TSS_genebody_TSE.gz -out BRM014-10uM_24h_wt_TSS_genebody_TSE.png --colorMap rainbow &amp; 五、生成tagAlign格式文件1. Convert PE BAM to tagAlign 对于单端序列。直接用bed格式就可以；对于双端序列，推荐用bedpe格式。这两种格式都可以称之为tagAlign，可以作为macs的输入文件。 tagAligen格式相比bam，文件大小会小很多，更加方便文件的读取。在转换得到tagAlign格式之后，我们就可以很容易的将坐标进行偏移 12345nohup ls *nodup.srt.name.bam|while read id; do bedtools bamtobed -bedpe -mate1 -i $id | gzip -nc &gt; ${id%%.*}.nodup.srt.name.bedpe.gz;done &amp;#含有chrM的染色体的TagAlign文件nohup ls *.nodup.srt.name.bedpe.gz | while read id; do zcat $id | awk 'BEGIN{OFS=&quot;\\t&quot;}{printf &quot;%s\\t%s\\t%s\\tN\\t1000\\t%s\\n%s\\t%s\\t%s\\tN\\t1000\\t%s\\n&quot;,$1,$2,$3,$9,$4,$5,$6,$10}' | gzip -nc &gt; ${id%%.*}.nodup.srt.name.tagAlign.gz; done &amp;#去除chrM的染色体的TagAlign文件nohup ls *nodup.srt.name.bedpe.gz|while read id; do zcat $id | grep -P -v &quot;^chrM&quot; | awk 'BEGIN{OFS=&quot;\\t&quot;}{printf &quot;%s\\t%s\\t%s\\tN\\t1000\\t%s\\n%s\\t%s\\t%s\\tN\\t1000\\t%s\\n&quot;,$1,$2,$3,$9,$4,$5,$6,$10}' | gzip -nc &gt; ${id%%.*}.nodup.nomit.srt.name.tagAlign.gz; done 2. Stand Cross Correlation analysis用于评估ATAC/Chip实验质量好坏的一个重要指标 123456NREADS=25000000nohup ls *.nodup.srt.name.bedpe.gz | while read id; do zcat $id | grep -v “chrM” | shuf -n ${NREADS} --random-source=&lt;(openssl enc -aes-256-ctr -pass pass:$(zcat -f ${id%%.*}.nodup.srt.name.tagAlign.gz | wc -c) -nosalt &lt;/dev/zero 2&gt;/dev/null) | awk 'BEGIN{OFS=&quot;\\t&quot;}{print $1,$2,$3,&quot;N&quot;,&quot;1000&quot;,$9}' | gzip -nc &gt; ${id%%.*}.nodup.nomit.srt.name.$((NREADS / 1000000)).tagAlign.gz; done &amp;#命令最终会生成交叉相关质量评估文件，*.cc.qc文件中会输出包含11列的信息，重点关注9-11列的信息，cc.plot.pdf文件相当于*.cc.qc文件的可视化nohup ls *$((NREADS / 1000000)).tagAlign.gz | while read id; do Rscript $(which run_spp.R) -c=$id -p=10 -filtchr=chrM -savp=${id%%.*}.cc.plot.pdf -out=${id%%.*}.cc.qc; done &amp;#质控结果查看，主要看NSC,RSC,Quality tag三个值即输出文件的第9列，第10列，第11列。ls *.cc.qc|while read id; do cat $id | awk '{print $9, &quot;\\t&quot;, $10, &quot;\\t&quot;, $11}';done 质控结果解读 Normalized strand cross-correlation coefficent (NSC)：NSC是最大交叉相关值除以背景交叉相关的比率(所有可能的链转移的最小交叉相关值)。NSC值越大表明富集效果越好，NSC值低于1.1表明较弱的富集，小于1表示无富集。NSC值稍微低于1.05，有较低的信噪比或很少的峰，这肯能是生物学真实现象，比如有的因子在特定组织类型中只有很少的结合位点；也可能确实是数据质量差。 Relative strand cross-correlation coefficient (RSC)：RSC是片段长度相关值减去背景相关值除以phantom-peak相关值减去背景相关值。RSC的最小值可能是0，表示无信号；富集好的实验RSC值大于1；低于1表示质量低。 QualityTag: Quality tag based on thresholded RSC (codes: -2:veryLow,-1:Low,0:Medium,1:High,2:veryHigh) 查看交叉相关性质量评估结果，主要看NSC,RSC,Quality tag三个值，这三个值分别对应输出文件的第9列，第10列，第11列。 六、Call Peaks1、去除线粒体基因的TagAlign格式文件进行shift操作，输入macs2软件去callpeak12345678smooth_window=150 # defaultshiftsize=$(( -$smooth_window/2 ))pval_thresh=0.01nohup ls *nodup.nomit.srt.name.tagAlign.gz | while read id; \\do macs2 callpeak \\-t $id -f BED -n &quot;${id%%.*}&quot; -g mm -p $pval_thresh \\--shift $shiftsize --extsize $smooth_window --nomodel -B --SPMR --keep-dup all --call-summits; \\done &amp; 2、去除ENCODE列入黑名单的区域 去除黑名单的bed文件，用于后续的peaks注释 1234567BLACKLIST=/home/gongyuqi/project/ATAC/mm10.blacklist.bed.gz#*_summits.bed为macs2软件callpeak的结果文件之一nohup ls *_summits.bed | while read id; do bedtools intersect -a $id -b $BLACKLIST -v &gt; ${id%%.*}_filt_blacklist.bed; done &amp;#查看过滤黑名单的区域前后的bed文件的peaks数ls *summits.bed|while read id; do cat $id |wc -l &gt;&gt;summits.bed.txt;donels *summits_filt_blacklist.bed|while read id; do cat $id |wc -l &gt;&gt;summits_filt_blacklist.bed.txt;donepast summits.bed.txt summits_filt_blacklist.bed.txt 去除黑名单的narrowPeaks文件，用于后续的IDR评估 123456789101112#使用IDR需要先对MACS2的结果文件narrowPeak根据-log10(p-value)进行排序,-log10(p-value)在第八列。# Sort by Col8 in descending order and replace long peak names in Column 4 with Peak_&lt;peakRank&gt;#*_peaks.narrowPeak为macs2软件callpeak的结果文件之一NPEAKS=300000 ls *_peaks.narrowPeak | while read id; do sort -k 8gr,8gr $id | awk 'BEGIN{OFS=&quot;\\t&quot;}{$4=&quot;Peak_&quot;NR ; print $0}' | head -n ${NPEAKS} | gzip -nc &gt; ${id%%_*}.narrowPeak.gz; doneBLACKLIST=../BLACKLIST/mm10.blacklist.bed.gz#生成不压缩文件ls *.narrowPeak.gz | while read id; do bedtools intersect -v -a $id -b ${BLACKLIST} | awk 'BEGIN{OFS=&quot;\\t&quot;} {if ($5&gt;1000) $5=1000; print $0}' | grep -P 'chr[\\dXY]+[ \\t]' &gt; ${id%%.*}.narrowPeak.filt_blacklist; done#生成压缩文件#ls *.narrowPeak.gz | while read id; do bedtools intersect -v -a $id -b ${BLACKLIST} | awk 'BEGIN{OFS=&quot;\\t&quot;} {if ($5&gt;1000) $5=1000; print $0}' | grep -P 'chr[\\dXY]+[ \\t]' | gzip -nc &gt; ${id%%.*}.narrowPeak.filt_blacklist.gz; done 3、Irreproducibility Discovery Rate (IDR)评估用于评估重复样本间peaks一致性的重要指标 首先生成narrowPeak_sample.txt文件，方便后续循环处理，生成文件内容如下： 12345678nohup cat narrowPeak_sample.txt | while read iddoarr=(${id})Rep1=${arr[0]}Rep2=${arr[1]}sample=${Rep1%%.*}_${Rep2%%.*}_idridr --samples $Rep1 $Rep2 --input-file-type narrowPeak -o $sample --plot done &amp; DMSO_24h_wt （样本处理情况） SRR12692092.filt_blacklist.narrowPeak SRR12692093.filt_blacklist.narrowPeak 没有通过IDR阈值的显示为红色 BRM014-10uM_24h_wt （样本处理情况） SRR12692098.filt_blacklist.narrowPeak SRR12692099.filt_blacklist.narrowPeak 没有通过IDR阈值的显示为红色 IDR评估会同时考虑peaks间的overlap和富集倍数的一致性。通过IDR阈值（0.05）的占比越大，说明重复样本间peaks一致性越好。从idr的分析结果看，我们的测试数据还可以的呢。 IDR评估相关参考资料： 重复样本的处理——IDR 4、Fraction of reads in peaks (FRiP)评估反映样本富集效果好坏的评价指标 123456789101112#生成bed文件nohup ls *.nodup.bam|while read id;do (bedtools bamtobed -i $id &gt;${id%%.*}.nodup.bed) ;done &amp;#批量计算FRiPls *_summits_filt_blacklist.bed|while read id;doecho $idbed=${id%%_*}.nodup.bed Reads=$(bedtools intersect -a $bed -b $id |wc -l|awk '{print $1}')totalReads=$(wc -l $bed|awk '{print $1}')echo $Reads $totalReads echo '==&gt; FRiP value:' $(bc &lt;&lt;&lt; &quot;scale=2;100*$Reads/$totalReads&quot;)'%'done FRiP值在5%以上算比较好的。但也不绝对，这是个软阈值，可以作为一个参考。 FRiP评估相关参考资料： https://www.jianshu.com/p/09e05bcd6981?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation 七、Peak annotation1、Feature Distribution1234567891011121314151617181920212223242526setwd(&quot;path to bed file&quot;)library(ChIPpeakAnno)library(TxDb.Mmusculus.UCSC.mm10.knownGene)library(org.Mm.eg.db)library(BiocInstaller)library(ChIPseeker)txdb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGenepromoter &lt;- getPromoters(TxDb=txdb, upstream=3000, downstream=3000)files = list(DMSO_24h_wt_rep1 = &quot;SRR12692092_summits_filt_blacklist.bed&quot;, DMSO_24h_wt_rep2 = &quot;SRR12692093_summits_filt_blacklist.bed&quot;, BRM014_10uM_24h_wt_rep1 = &quot;SRR12692098_summits_filt_blacklist.bed&quot;, BRM014_10uM_24h_wt_rep2 = &quot;SRR12692099_summits_filt_blacklist.bed&quot;)#汇总所有样本#plotAnnoBar和plotDistToTSS这两个柱状图都支持多个数据同时展示peakAnnoList &lt;- lapply(files, annotatePeak, TxDb=txdb, tssRegion=c(-3000, 3000))plotAnnoBar(peakAnnoList,title = &quot; Feature Distribution&quot;)plotDistToTSS(peakAnnoList,title = &quot; Feature Distribution relative to TSS&quot;)#例举单个样本peakAnno &lt;- annotatePeak(files[[1]],# 分别改成2或者3或者4即可，分别对应四个文件 tssRegion=c(-3000, 3000), TxDb=txdb, annoDb=&quot;org.Mm.eg.db&quot;)plotAnnoPie(peakAnnoLipeakAnnost)upsetplot(peakAnno, vennpie=TRUE) 2、查看peaks在全基因组上的分布12345678910111213141516171819202122#输入文件的准备DMSO_24h_wt_rep1&lt;-read.csv(&quot;SRR12692092_summits_filt_blacklist.csv&quot;)DMSO_24h_wt_rep1&lt;-DMSO_24h_wt_rep1[,-4]DMSO_24h_wt_rep2&lt;-read.csv(&quot;SRR12692093_summits_filt_blacklist.csv&quot;)DMSO_24h_wt_rep2&lt;-DMSO_24h_wt_rep2[,-4]BRM014_10uM_24h_wt_rep1&lt;-read.csv(&quot;SRR12692098_summits_filt_blacklist.csv&quot;)BRM014_10uM_24h_wt_rep1&lt;-BRM014_10uM_24h_wt_rep1[,-4]BRM014_10uM_24h_wt_rep2&lt;-read.csv(&quot;SRR12692099_summits_filt_blacklist.csv&quot;)BRM014_10uM_24h_wt_rep2&lt;-BRM014_10uM_24h_wt_rep2[,-4]#以DMSO_24h_wt_rep1为例set.seed(123)circos.initializeWithIdeogram(plotType = c(&quot;axis&quot;, &quot;labels&quot;))circos.track(ylim = c(0, 1), panel.fun = function(x, y) { chr = CELL_META$sector.index xlim = CELL_META$xlim ylim = CELL_META$ylim circos.rect(xlim[1], 0, xlim[2], 1)}, track.height = 0.15, bg.border = NA, bg.col=rainbow(24))text(0, 0, &quot;DMSO_24h_wt_rep1&quot;, cex = 1.5)circos.genomicDensity(DMSO_24h_wt_rep1, col = c(&quot;#000080&quot;), track.height = 0.2)circos.clear() 看到这样的结果，第一反应就是————为什么两种处理情况下染色体开放程度那么像！？难道我代码有问题！？经过反复检查验证（将一个样本chr1上的peaks都删掉，再次运行上述代码，就会发现显著的改变），可以确定分析上是没有问题的。这两种处理导致的差异可能不是很显著。再加上20万+的peaks放在这个小小的circos图上展示，有些差异会被掩盖掉。就如在做TSS富集分析的时候，单独看TSS前后3Kb区域，可以看到有两个峰，但在看TSS-genebody-TSE区域是，TSS处相对微弱的那个峰就被掩盖掉了。 3、拿到每个样本中peaks对应得基因名这一步非常重要，拿到基因名就可以根据课题需要进行差异分析等 12345678910111213141516171819202122232425262728293031323334353637383940414243#以DMSO_24h_wt样本为例#replicate 1peakAnno_DMSO_24h_wt_rep1 &lt;- annotatePeak(files[[1]], tssRegion=c(-3000, 3000), TxDb=txdb, annoDb=&quot;org.Mm.eg.db&quot;)genelist_DMSO_24h_wt_rep1_uniqe&lt;-as.data.frame(unique(peakAnno_DMSO_24h_wt_rep1@anno@elementMetadata@listData[[&quot;SYMBOL&quot;]]))colnames(genelist_DMSO_24h_wt_rep1_uniqe)&lt;-&quot;symbol&quot;#replicate 2peakAnno_DMSO_24h_wt_rep2 &lt;- annotatePeak(files[[2]], tssRegion=c(-3000, 3000), TxDb=txdb, annoDb=&quot;org.Mm.eg.db&quot;)genelist_DMSO_24h_wt_rep2_uniqe&lt;-as.data.frame(unique(peakAnno_DMSO_24h_wt_rep2@anno@elementMetadata@listData[[&quot;SYMBOL&quot;]]))colnames(genelist_DMSO_24h_wt_rep2_uniqe)&lt;-&quot;symbol&quot;#重复样本间共同的开放基因venn.diagram( x=list( DMSO_24h_wt_rep1=genelist_DMSO_24h_wt_rep1_uniqe$symbol, DMSO_24h_wt_rep2=genelist_DMSO_24h_wt_rep2_uniqe$symbol ), filename = &quot;DMSO_24h_wt.png&quot;, lty=&quot;dotted&quot;, lwd=3, col=&quot;transparent&quot;, fill=c(&quot;darkorchid1&quot;,&quot;cornflowerblue&quot;), alpha=0.5, label.col=c(&quot;darkorchid1&quot;,&quot;white&quot;,&quot;darkblue&quot;) , cex=1, fontfamily=&quot;serif&quot;, fontface=&quot;bold&quot;, cat.default.pos=&quot;text&quot;, cat.col=c(&quot;darkorchid1&quot;,&quot;darkblue&quot;), cat.cex=0.6, cat.fontfamily=&quot;serif&quot;, cat.dist=c(0.3,0.3), cat.pos=0)#查看各组内样本间的overlapping reads：DMSO_24h_wt， BRM014_10uM_24h_wt；#以及组间peaks的异同情况：DMSO_24h_wt vs. BRM014_10uM_24h_wt#代码类似上面的，就不一一展示了 从下图可以看出，不管是组间还是组内，差异的peaks数目都不是很多了，这一点也验证了我们上面做的再全基因组范围内查看peaks的分布结果。","link":"/2020%20ATAC-seq%20analysis/"},{"title":"10X GENOMICS pipeline","text":"Cellranger 软件流程 （一）环境搭建下载Cell Ranger,下载10X GENOMICS配套的参考基因组.官网也提供了Cell Ranger Pipeline。 官网下载链接： https://support.10xgenomics.com/single-cell-gene-expression/software/downloads/latest (1)下载、安装Cell Ranger软件1234567891011121314#下载curl -o cellranger-6.0.1.tar.gz &quot;https://cf.10xgenomics.com/releases/cell-exp/cellranger-6.0.1.tar.gz?Expires=1620565659&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZi4xMHhnZW5vbWljcy5jb20vcmVsZWFzZXMvY2VsbC1leHAvY2VsbHJhbmdlci02LjAuMS50YXIuZ3oiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2MjA1NjU2NTl9fX1dfQ__&amp;Signature=IN508xcNVbAtoHul8SUog8KHmlAhg2lbR6-hCQeiwPs~-EfvTL1JmDurZ3dV-k3ry7TO9WP5Ae11yceDSzCfVIqglxHp6Pad4CftaStozIuEU7XJ0JbtDTO3DFVHRkpgNR8k48dNADA~JnhcHx8zOsT23VEeugqc7Z~35~05SOG8GJjxSJ8qGSTBdHYt7kPyqfZVs2apryxLOV-QBho66LvD4NLqNXFvzkPZ8t0e9Cv4rrV1JXseqY6PJnK8TPozTnDBnhfkop30FZ~jOLEY8wObOIA4b~EoyskSGjSfcvnp5urI7xjAyQNC6XtlJmRwDWEwWovgicScei2gkZqOGw__&amp;Key-Pair-Id=APKAI7S6A5RYOXBWRPDA&quot;#安装##解压tar zxvf cellranger-6.0.1.tar.gz##添加环境变量echo 'export PATH=/home/gongyuqi/10X/cellranger-6.0.1:$PATH' &gt;&gt; ~/.bashrc##永久生效环境变量source ~/.bashrc#查看本机的配置，看看是否满足跑单细胞数据的最低配置cellranger sitecheck &gt; sitecheck.txtless -N sitecheck.txt#最后，测试一下下载安装的cellranger能否正常运行cellranger testrun --id=testcellranger 标准输出最后两行为如下结果，表示cellranger可以正常使用 (2)下载人类参考基因组12curl -O https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gztar -zxvf refdata-gex-GRCh38-2020-A.tar.gz 查看一下文件的组成结构 (3)下载小鼠参考基因组12curl -O https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-mm10-2020-A.tar.gztar -zxvf refdata-gex-mm10-2020-A.tar.gz (二)下载测试数据在这里要用到prefetch和ascp的组合，详细的软件安装以及测试数据下载请参考单细胞数据下载 (1)pretch下载SRA文件12345#refetch默认通过https的方式下载原始数据，速度上会有限制#如果存在ascp，prefetch默认通过fasp的方式下载数据，速度一下载从骑自行车升级为开飞机cat sample.txt|while read iddo nohup prefetch $id -O ./ &amp;&amp; echo &quot;${id}.sra done&quot; &amp;done (2)ascp下载123#其中两个样本不在ncbi中，所以我们这里用直接用ascp下载EBI中存储的SRA文件，两个样本就不写循环了ascp -QT -l 300m -P33001 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:vol1/srr/SRR772/009/SRR7722939 ./ascp -QT -l 300m -P33001 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:vol1/srr/SRR772/002/SRR7722942 ./ (三)Cell Ranger 使用(1)SRA格式转成fastq格式12345cd /home/gongyuqi/project/scRNA-seq/raw_data#--split-files生成三个文件，第一个文件的所有序列都是8bp，第二个文件都是26bp，第三个文件都是91bp，第三个文件是测序readsls *.sra|while read iddo nohup fastq-dump --gzip --split-files -A $id &amp;done (2)重命名文件123ls *.sra|while read iddo mv ${id}_1*.gz ${id}_S1_L001_I1_001.fastq.gz; mv ${id}_2*.gz ${id}_S1_L001_R1_001.fastq.gz; mv ${id}_3*.gz ${id}_S1_L001_R2_001.fastq.gzdone 命名前命名后 (3)质控12ls *R1*.gz &gt; sample_fastq.txtls *R2*.gz &gt;&gt; sample_fastq.txt (4)cellranger countcellranger是软件最核心的部分，可以完成细胞鉴定，基因组比对，过滤，UMI计数，细胞降维，聚类，差异分析等功能。内部流程很多，但是使用很简单。 123456789101112131415161718192021222324252627#首先进入到cellranger_count文件夹下面并建立存放各个样本结果的文件夹cd /home/gongyuqi/project/scRNA-seq/cellranger_countcat sample.txt|while read id;do mkdir $id;done#将下列代码编辑到脚本文件cellranger_count.sh中cat sample.txt|while read iddocellranger count --id=$id \\ --transcriptome=/home/gongyuqi/10X/refdata-gex-GRCh38-2020-A \\ --fastqs=/home/gongyuqi/project/scRNA-seq/raw_data \\ --sample=$id \\ --nosecondary \\ --localcores=16 \\ --localmem=64 &gt; ${id}.log 2&gt;&amp;1done#给脚本文件执行权限chmod +x cellranger_count.sh#后台运行脚本文件nohup ./cellranger_count.sh &amp;#参数解析#--id: 指定输出文件存放目录#--transcriptome: cellranger兼容的参考基因组位置#--fastqs：重命名好的fastq.gz文件位置#--sample：fastq.gz文件文件前缀，以作为软件识别的标志#--nosecondary：只获得表达矩阵，不进行后续的降维、聚类和可视化（这些工作用R包做，如Seurat）#--jobmode=local: 指定cellranger在本地运行（cellranger默认也是在本地运行）。它会占用90%的空余内存和所有空余的CPU。如果要进行资源控制，使用--localmem或者--localcores#--localcores: 限定cellranger最多同时使用的cpu个数#--localmem：限定cellranger最多占用的内存大小 以SRR7722937为例，来看一下cellranger count的结果总结：Barcode,UMI,mapping情况，样本的信息（使用v2试剂，参考基因组，cellranger版本），reads与测序饱和度、基因数量的关系（正相关，情理之中~） (5)cellranger aggr第一步构建输入文件信息，文件内容如下。如果整合的样本既有v2试剂也有v3试剂，需要在第三列加上相应的试剂信息。如果都是用的同一种试剂，只需要保留前两列的信息即可。如本次测试数据。 12345#--id为输出文件保存的位置#--csv为输入文件的信息cellranger aggr --id=patient_2586-4 \\ --csv=patient_2586-4_libbrary.csv \\ --normalize=mapped 同样的，来查看一下cellranger aggr的结果报告。报告的消息解读可参考：https://www.jianshu.com/p/30de2aea4b74?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation 1、Count Summary 2、Count Analysis 3、Alerts (四) Cell Ranger输出结果导入Rcellranger aggar运行完成之后，在outs文件夹下面的子文件夹filtered_feature_bc_matrix中的三个文件上传到本地，解压导入R中。 12345678910111213141516library(Matrix)barcode.path&lt;-paste0(&quot;barcodes.tsv&quot;)feature.path&lt;-paste0(&quot;features.tsv&quot;)matrix.path&lt;-paste0(&quot;matrix.mtx&quot;)mat&lt;-readMM(file = matrix.path)feature.names&lt;-read.delim(feature.path, header = FALSE, stringsAsFactors = FALSE)barcode.names&lt;-read.delim(barcode.path, header = FALSE, stringsAsFactors = FALSE)colnames(mat)&lt;-barcode.names$V1rownames(mat)&lt;-feature.names$V2mat[1:4,1:4]dim(mat)","link":"/10X-GENOMICS-pipeline/"},{"title":"ATAC_project","text":"ch1","link":"/ATAC-project/"},{"title":"ATAC-seq Analysis","text":"ATAC-seq旧版处理流程 ATAC-seq Analysis表观测序数据中比较重要的就是ATAC-seq、Chip-seq。这两者的原理和分析过程类似。具体的异同详见此链接：https://www.jianshu.com/p/87bc2002e82c (一)数据下载数据下载是我走过的最曲折的路 强烈不推荐wget、curl——需要向上天再借500年才能等的起。 不推荐prefetch——不知道为什么prefetch在我这里也不起作用！按理说有ascp环境后，prefetch会自动调用ascp进行数据下载呀！可是prefetch依然显示通过https进行数据下载，这就没法下载呀！直接timeout！ 正解——调用ascp下载数据 （1）ascp下载NCBI的SRA文件 此方法亲测不行，但是就在2个月前还是可以的。官网给出的解释大致意思就是——不好意思，这段时间出现无法通过ascp下载NCBI数据属于正常现象…blablabla… （2）ascp下载EBI的fastq文件（强烈推荐，省去了SRA转成fastq的麻烦，很耗资源，很耗时） 在此次数据分析中，下载SRR2927015、SRR2927016、SRR2927018、SRR3545580对应的fastq文件进行ATAC-seq数据分析的实战。 EBI官网输入对应SRR号（以SRR3545580为例），找到相应的fastq文件位置，复制连接地址。 http://ftp.sra.ebi.ac.uk/vol1/fastq/SRR354/000/SRR3545580/SRR3545580_1.fastq.gz http://ftp.sra.ebi.ac.uk/vol1/fastq/SRR354/000/SRR3545580/SRR3545580_2.fastq.gz 下载方式如下（下载速递可达到每秒M级别，快的时候能到60M+，亲测此方法为目前最可取的方法） 12ascp -QT -l 300m -P33001 -i /home/gongyuqi/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR354/000/SRR3545580/SRR3545580_1.fastq.gz .ascp -QT -l 300m -P33001 -i /home/gongyuqi/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR354/000/SRR3545580/SRR3545580_2.fastq.gz . 最终下载结果 为了后续分析，将其改名为下 2-cell-1 SRR29270152-cell-2 SRR29270162-cell-4 SRR29270182-cell-5 SRR3545580 附上测序数据下载方式的相关参考资料 https://www.jianshu.com/p/df34b99cbd43 https://www.bioinfo-scrounger.com/archives/171/ （二）自定义docker容器的启动与数据共享之前创建了具备ATAC-seq数据分析所需环境的docker容器，并push到阿里云。在此只需要pull该镜像，run即可使用。 但是！！！创建容器时并没有装ascp，因为只有普通用户才可以运行ascp。所以数据下载是在ubuntu主机进行的。为了能够在容器中使用下载的测试数据，运用数据卷，实现主机和容器间的数据共享。 12# 查看镜像docker images 12# 启动容器实现数据共享docker run -it -v /home/gongyuqi/Downloads/ATACdata/:/root/project/atac/raw 608eafa789c8 123# 查看容器内是否已经存在fastq数据cd root/project/atac/rawls 123# 激活ATAC环境（该环境配置有处理ATAC-seq数据的所需软件）conda activate ATAC# (base)变成了(ATAC)，即由最初的conda环境变成了ATAC环境 （三）、数据过滤运用软件 Trim Galore 去除接头和质量不合格的reads Trim Galore是对FastQC和Cutadapt的包装。适用于所有高通量测序，包括RRBS(Reduced Representation Bisulfite-Seq ), Illumina、Nextera 和smallRNA测序平台的双端和单端数据。 Trimmomatic是针对Illumina高通量测序平台设计的接头去除和低质量reads清洗软件。 附上trim-galore相关参考资料: https://www.jianshu.com/p/7a3de6b8e503 以下列数据为例 2-cell-1 SRR2927015 1trim_galore -q 7 --phred33 --length 35 -e 0.1 --stringency 4 --paired -o /root/project/atac/clean/ 2-cell-1_1.fastq.gz 2-cell-1_2.fastq.gz trim_galore –q：设置线程 –phred33：使用ASCII+33质量值作为Phred的分 –length：去除长度小于参数值的reads -e：允许的最大误差 –stringency：设置与接头重叠的序列 –paired：对于双段测序文件，正反链质控通过才保留 -o：输出目录 结果如下 因为本次数据是用本机（8线程，16G）跑的，要给机子休息的时间，同时实验样本少，所以一个一个样本手动跑。但是！当样本量多了，在服务器上跑的时候，循环就要省事很多，比如跑的时候去睡个觉… 拿本次样本写个循环，举个例 12345ls /root/project/raw/*1.fq.gz &gt;1ls /root/project/raw/*2.fq.gz &gt;2paste 1 2 &gt; samplepath=/root/project/atac/cleancat sample | while read id; do echo $id arr=($id) fq1=${arr[0]} fq2=${arr[1]} nohup trim_galore -q 30 --phred33 -e 0.1 --stringency 4 --paired -o $path $fq1 $fq2 &amp; done (四)、质量控制测试数据的质控真的非常重要！！！如果测序质量本身就很好，没有质空这一步也影响不大。如果测序质量不好，不进行质控的化，直接影响后续的比对率。一般情况下，90%+的比对率才算比较正常，太低的比对率往往是质控步骤有问题，会非常严重的影响最终结果。亲测过！！！！ 运用软件fastqc、multiqc 进行批处理 数据质控 1234567cd /root/project/atac/qc &amp;&amp; mkdir rawqc trimqc# 对原始数据进行质控cd rawqcfastqc -t 7 /root/project/atac/raw/*gz -o ./# 对过滤数据进行质控cd .. &amp; cd trimqcfastqc -t 7 /root/project/atac/clean/*gz -o ./ （1）原始数据质控结果 （2）过滤后数据质量控制结果 质控结果整合 123456# 原始数据qc结果整合cd /root/project/atac/qc/rawqc &amp;&amp; mkdir multiqc_resultsmultiqc . -o multiqc_results/# 过滤数据qc结果整合cd /root/project/atac/qc/trimqc &amp;&amp; mkdir multiqc_resultsmultiqc . -o multiqc_results/ （1）原始数据整合结果 （2）过滤数据整合结果 附上multiqc相关参考资料https://blog.csdn.net/ada0915/article/details/77201871 mulitiqc整合结果解读 (1)过滤前 （2）过滤后 （3）解读（随便一搜，各种解读教程实在太多了～） trim-galore完美的去除了adaptor;去除了质量不合格的序列使得2-cell-1_2和2-cell-2_2的质量过关；2-cell-1_2的N含量超标的序列被去除了。 因为去除了adptor和质量不合格的序列，所以sequence length自然是不一致的了。另外，还没有PCR去重，所以sequence duplication是不过关的，这一点会在PCR去重步骤中得到解决。至于Per Base Sequence Content,前10个碱基总是异常的波动，比对序列长度远大于10，处理时也可以设定不比对前10个，所以这个关系不大。 （4）以Sequence Length和Per Base N Content展示过滤前后的区别 过滤前 过滤后 (五)比对参考基因组运用软件bowtie2、bedtools 一般对于DNA数据，比对软件一般使用：bowtie2、bwa 对于RNA数据，比对一般使用：hisat2、star 使用bedtools将最终的bam文件转成bed文件 （1）参考基因组及其注释文件的下载 参考基因组（解压后） 参考基因组注释文件（解压后） 本次分析用到的是小鼠的参考基因组及其注释文件 mm10.fa、gencode.vM24.annotation.gtf （2）参考基因组索引构建（将构建的参考基因组索引文件cp到容器中的/root/project/atac/reference/mm10_index目录下） 1bowtie2-build /home/gongyuqi/biosoftware/reference/genome/mm10.fa mm10_index （3）参考基因组的比对，直接生成bam文件 12bowtie2 -p 7 -x /root/project/atac/reference/mm10_index -1 ../clean/2-cell-1_1_val_1.fq.gz -2 ../clean/2-cell-1_2_val_2.fq.gz | samtools sort -@ 7 -O bam -o 2-cell-1.bam -# 此samtools sort默认按照reads比对到染色体上的坐标排序 bowtie2直接比对的结果会生成sam文件，sam文件很大，比较占空间。在比对这一步就直接利用samtools sort将其转成bam文件。 另外，samtool排序过程中并不会丢弃一部分reads，但是排序后的bam文件相比排序前的体积会更小。 比对相关参考资料 https://www.jianshu.com/p/6ed1bfbb7b72 比对结果 以2-cell-1_1_val为例进行结果展示 （4）构建原始bam文件的索引文件、状态文件 构建索引文件 1ls *.bam | while read id; do samtools index -@ 7 $id; done .bai文件即位生成的索引文件 构建状态文件 1ls *.bam | while read id; do samtools flagstat -@ 7 $id &gt; $id.stat; done （5）去除PCR重复、构建去重后的索引文件、状态文件、 sambamba软件去重，会同时生成了去重后的bam和bai文件 1ls *.bam | while read id; do sambamba markdup -r -t 7 $id rmdup.$id; done 构建状态文件 1ls rmdup*.bam | while read id; do samtools flagstat -@ 7 $id &gt; $id.stat; done （6）去除同一样本的两条reads无法落在同一条染色体及测序质量低于30的reads、去除线粒体基因、构建索引文件、构建状态文件 去除不符合要求的reads及线粒体基因 1ls rm*.bam | while read id; do samtools view -@ 7 -h -f 2 -q 30 $id | grep -v chrM |samtools sort -O bam -@7 -o last.$id; done 构建索引文件 1ls last*.bam | while read id; do samtools index -@ 7 $id; done 构建状态文件 1ls last*.bam | while read id; do samtools flagstat -@ 7 $id &gt; $id.stat; done （7）生成bed文件 1ls last*.bam | while read id; do bedtools bamtobed -i $id &gt; $id.bed; done （六）使用macs2找peaks使用软件macs2 以last.rmdup.2-cell-1.bam.bed进行演示 1macs2 callpeak -t last.rmdup.2-cell-1.bam.bed -g mm --nomodel --shift -100 --extsize 200 -n 2-cell-1 --outdir ../peaks/ 最终生成文件 peaks注释12345678910111213setwd(&quot;path\\\\to\\\\.bed&quot;)library(ChIPpeakAnno)library(TxDb.Mmusculus.UCSC.mm10.knownGene)library(org.Mm.eg.db)library(BiocInstaller)library(ChIPseeker)txdb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGenepromoter &lt;- getPromoters(TxDb=txdb, upstream=3000, downstream=3000)files = list(cell_1_summits = &quot;2-cell-1_summits.bed&quot;, cell_2_summits = &quot;2-cell-2_summits.bed&quot;, cell_4_summits = &quot;2-cell-4_summits.bed&quot;, cell_5_summits = &quot;2-cell-5_summits.bed&quot;)peakAnno &lt;- annotatePeak(files[[1]], # 分别改成2或者3或者4即可，分别对应四个文件 tssRegion=c(-3000, 3000), TxDb=txdb, annoDb=&quot;org.Mm.eg.db&quot;) （1）拿到每个样本中peaks对应得基因名 这一步非常重要，拿到基因名就可以根据课题需要进行差异分析等 1genelist&lt;-peakAnno@anno@elementMetadata@listData[[&quot;SYMBOL&quot;]] （2）单个文件作图 图片就不做展示了 12345plotAnnoPie(peakAnno)plotAnnoBar(peakAnno)vennpie(peakAnno)upsetplot(peakAnno)upsetplot(peakAnno, vennpie=TRUE) （3）plotAnnoBar和plotDistToTSS这两个柱状图都支持多个数据同时展示，方便比较 12345678910111213setwd(&quot;path\\\\to\\\\.bed&quot;)library(ChIPpeakAnno)library(TxDb.Mmusculus.UCSC.mm10.knownGene)library(org.Mm.eg.db)library(BiocInstaller)library(ChIPseeker)txdb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGenepromoter &lt;- getPromoters(TxDb=txdb, upstream=3000, downstream=3000)files = list(cell_1_summits = &quot;2-cell-1_summits.bed&quot;, cell_2_summits = &quot;2-cell-2_summits.bed&quot;, cell_4_summits = &quot;2-cell-4_summits.bed&quot;, cell_5_summits = &quot;2-cell-5_summits.bed&quot;)peakAnnoList &lt;- lapply(files, annotatePeak, TxDb=txdb,tssRegion=c(-3000, 3000))plotAnnoBar(peakAnnoList) 1plotDistToTSS(peakAnnoList, title = &quot;Distribution of transcription factor-binding loci\\nrelative to TSS&quot;) （4）查看peaks在全基因组的位置（以2-cell-1样本为例） 12chropeaks&lt;-readPeakFile(&quot;2-cell-1_peaks.narrowPeak&quot;)covplot(chropeaks, weightCol=5, title = &quot;Peaks over chromosomes&quot;) （4）重复样本间共同的peaks和注释基因 也可以用拿到的各个样本的基因名，做韦恩图，会比这个图更美观。 12genes&lt;-lapply(peakAnnoList, function(i) as.data.frame(i)$geneId)vennplot(genes) peaks注释相关参考资料 https://www.jianshu.com/p/c83a38915afc","link":"/ATAC-seq%20Analysis/"},{"title":"LVM逻辑卷管理器","text":"磁盘管理（进阶篇） LVM逻辑卷管理器 前面的磁盘管理（基础篇）介绍的磁盘管理技术虽然能够有效地提高硬盘设备的读写速度以及数据的安全性，但是在硬盘分好区后，再想修改硬盘分区大小就不容易了。当用户想要随着实际需求的变化调整硬盘分区的大小时，会受到硬盘“灵活性”的限制。这时就需要用到另外一项非常普及的硬盘设备资源管理技术了—LVM（逻辑卷管理器）。LVM可以允许用户对硬盘资源进行动态调整。 LVM技术是在硬盘分区和文件系统之间添加了一个逻辑层，它提供了一个抽象的卷组，可以把多块硬盘进行卷组合并。这样一来，用户不必关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区的动态调整。 **PV,Physical Volume** **VG,Volume Group** **LV,Logical Volume** 物理卷处于LVM中的最底层。卷组建立在物理卷之上，一个卷组可以包含多个物理卷，而且在卷组创建之后也可以继续向其中添加新的物理卷。逻辑卷是用卷组中空闲的资源建立的，并且逻辑卷在建立后可以动态地扩展或缩小空间。这就是LVM的核心理念。 部署逻辑卷该实验在VMware中进行，首先拍摄快照，以便在实验结束后恢复原有状态。依次创建物理卷PV、卷组VG、逻辑卷LV 在虚拟机中添加两块新硬盘设备。我们先对这两块新硬盘进行创建物理卷的操作，可以将该操作简单理解成让硬盘设备支持LVM技术。 12fdisk -l #此时可以看出多出了/dev/sdb /dev/sdc的信息pvcreate /dev/sdb /dev/sdc #让硬盘设备支持LVM技术 把两块硬盘设备加入到newvg卷组中，然后查看卷组状态 12vgcreate newvg /dev/sdb /dev/sdcvgdisplay 切割出一个约160M的逻辑卷LV 在对逻辑卷进行切割时有两种计量单位。第一种是以容量为单位，所使用的参数为-L。例如，使用-L 150M生成一个大小为150MB的逻辑卷。另外一种是以基本单元的个数为单位，所使用的参数为-l。每个基本单元的大小默认为4MB。例如，使用-l 37可以生成一个大小为37×4MB=148MB的逻辑卷。 123lvdisplay #查看当前逻辑卷情况lvcreate -n newlv -l 40 newvglvdisplay #创建逻辑卷后查看逻辑卷情况 把生成的逻辑卷格式化，然后挂载使用 Linux系统会把LVM中的逻辑卷设备存放在/dev设备目录中（实际上是做了一个符号链接），同时会以卷组的名称来建立一个目录，其中保存了逻辑卷的设备映射文件（即/dev/卷组名称/逻辑卷名称）。 123mkfs.xfs /dev/newvg/newlvmkdir /newlvmount /dev/newvg/newlv /newlv 查看挂载状态，并写入配置文件 123df -hecho &quot;/dev/newvg/newlv /newlv xfs defaults 0 0&quot; &gt;&gt; /etc/fstabdf -h 扩容逻辑卷 用户在使用存储设备时感知不到设备底层的架构和布局，更不用关心底层是由多少块硬盘组成的，只要卷组中有足够的资源，就可以一直为逻辑卷扩容。扩展前请一定要记得卸载设备和挂载点的关联。 1umount /newlv 把上诉逻辑卷newlv扩展至30G 123456lvextend -L 30G /dev/newvg/newlvmount -adf -h#xfs文件系统需要执行xfs_growfs操作，需要先挂载，所以上面先执行了mount -a命令xfs_growfs /dev/newvg/newlvdf -h 注意使用 resize2fs或xfs_growfs 对挂载目录在线扩容resize2fs 针对文件系统ext2 ext3 ext4xfs_growfs 针对文件系统xfs xfs文件系统是不支持缩减逻辑卷操作，所以为了演示缩小逻辑卷操作，先将xfs文件系统重新格式化成ext4文件系统 ext4文件系统的扩容操作 123umount /newlvmkfs.ext4 /dev/newvg/newlvvim /etc/fstab #将/dev/newvg/newlv对应的xfs改成ext4,改变文件系统类型并不会影响磁盘上的数据 123456lvextend -L 35G /dev/newvg/newlve2fsck -f /dev/newvg/newlv #检查硬盘完整型，针对ext4执行mount -adf -hresize2fs /dev/newvg/newlvdf -h 缩小逻辑卷执行缩容前一定确保最终的大小要大于现有数据所占的空间执行缩容操作前记得先把文件系统卸载掉 1umount /newlv 把逻辑卷newlv的容量减小到100M 1234567umount /newlvdf -he2fsck -f /dev/newvg/newlvresize2fs /dev/newvg/newlv 5G lvreduce -L 5G /dev/newvg/newlv ##奇怪，我之前运行的时候没有加此操作，依然能够成功mount -adf -h 逻辑卷快照可以对某一个逻辑卷设备做一次快照，如果日后发现数据被改错了，就可以利用之前做好的快照卷进行覆盖还原。LVM的快照卷功能有两个特点： 快照卷的容量必须等同于逻辑卷的容量； 快照卷仅一次有效，一旦执行还原操作后则会被立即自动删除。 查看卷组信息卷组输出结果显示已经使用5G的容量，空闲容量还有34.99GB. 1echo &quot;This is a test about LVNSnap&quot; &gt; /LVMtest/file 使用-s参数生成一个快照卷，使用-L参数指定切割的大小。另外，还要在命令后面写上是针对哪个逻辑卷执行的快照操作。 12lvcreate -L 5G -s -n SNAP /dev/volumegroup/logicalvolumevgdispaly 在逻辑卷所挂载的目录中创建一个100MB的垃圾文件，然后再查看快照卷的状态。可以发现存储空间占的用量上升了。 12dd if=/dev/zero of=/LVMtest/FILE count=1 bs=100Mvgdisplay 为了校验SNAP快照卷的效果，需要对逻辑卷进行快照还原操作。在此之前记得先卸载掉逻辑卷设备与目录的挂载 123456df -humount /LVWtestlvconvert --merge /dev/volumegroup/SNAP mount -als /LVMtest #此时我们之前创建的100M大小的垃圾文件FILE不见了，因为这个文件是LVM快照之后创建的文件 删除逻辑卷依次删除逻辑卷、卷组、物理卷设备，顺序不可颠倒 123456789umount /newlvvim /etc/fstab #删除掉/dev/newvg/newlv相关信息lvremove /dev/newvg/newlvvgremove newvgpvremove /dev/sdb /dev/sdc#执行下面的命令分别查看LV、VG、PV的情况，发现/dev/newvg/newlv相关信息消失lvdisplayvgdisplaypvdisplay 参考资料https://blog.csdn.net/qq_33932782/article/details/76612965","link":"/LVM%E9%80%BB%E8%BE%91%E5%8D%B7%E7%AE%A1%E7%90%86%E5%99%A8/"},{"title":"Markdown学习教程","text":"基础教程，随便写写 Markdown学习教程 Markdown语言在2004年由John Gruber创建，是以一种轻量级标记语言，它允许人们使用易读写的纯文本格式编写文档。Markdown编写的文档可以导出HTML、Word、PDF、Epub等多种格式的文档，编写的文档后缀为.md，.markdown。 本学习教程在Windows下使用visual studio code编辑器讲解Markdown的语法。Visual Studio Code官网：https://aka.ms/win32-x64-user-stableVisual Studio Code(for windows)下载地址：https://aka.ms/win32-x64-user-stable Markdown 标题1、使用#号可表示1-6级标题，一级标题对应一个#号，六级标题对应六个#号。123456789101112131415# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题``` # 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 **2、使用=和-标记一级标题和二级标题** 一级标题二级标题12345678910111213一级标题======= 二级标题-------## Markdown 段落 **Markdown段落的换行是使用两个以上空格加上回车。** **Markdown段落的换行也可以使用一个空行来开始新的一行** ## Markdown 列表Markdown支持无序列表和有序列表。**无序列表使用星号(*)、加号(+)、或是减号(-)作为列表标记。** 符号后面一定要空一格。 第一项 第二项 第三项 第一项 第二项 第三项 第一项 第二项 第三项1234567891011* 第一项* 第二项* 第三项 + 第一项+ 第二项+ 第三项 - 第一项- 第二项- 第三项 **有序列表使用数字并加上`.`号表示。** 第一项 第二项 第三项1234561. 第一项 2. 第二项3. 第三项 **列表嵌套**(注意使用tab键) 第一项 第一项嵌套的第一个元素 第一项嵌套的第二个元素 第二项 第二项嵌套的第一个元素 第二项嵌套的第二个元素 12345678910111. 第一项 - 第一项嵌套的第一个元素 - 第一项嵌套的第二个元素 2. 第二项 - 第二项嵌套的第一个元素 - 第二项嵌套的第二个元素 ## Markdown 区块 Markdown区块引用是在段落开头使用`&gt;`符号，然后后面紧跟一个**空格**符号(为什么亲测`&gt;`符号后面跟不跟空格都没关系呢)。 区块引用区块引用区块引用 12345&gt; 区块引用 &gt; 区块引用 &gt; 区块引用 区块也可以嵌套，一个`&gt;`符号是最外层，两个`&gt;`符号是第一层嵌套，以此类推。 最外层 第一层嵌套 第二层嵌套 12345&gt; 最外层 &gt;&gt; 第一层嵌套 &gt;&gt;&gt; 第二层嵌套 区块中使用列表 区块中使用列表 第一项 第二项 第一项 第二项 第三项 12345678&gt; 区块中使用列表 &gt; 1. 第一项 &gt; 2. 第二项 &gt; + 第一项 &gt; + 第二项 &gt; + 第三项 列表中使用区块 第一项 第一项的第一步第一项的第二部 第二项 123456789* 第一项 &gt; 第一项的第一步 &gt; 第一项的第二部 * 第二项 &gt; 第二项的第一步 &gt; 第二项的第二部 ## Markdown 代码如果是段落上的一个函数或者片段的代码可以用引号把它包起来(``). printf() 函数 12345678910111213141516171819`printf()`函数 **代码区块** 代码区块使用4个空格或者一个制表符(tab键)。 &gt; mkdir test &gt; touch test.txt &gt; echo &quot;This is a test&quot; mkdir test touch test.txt echo &quot;This is a test&quot; 代码区块也可以用两个` ``` `包裹一段代码，并指定一种语言（也可以不指定） *```* mkdir test touch test.txt echo &quot;This is a test&quot; *```* mkdir testtouch test.txtecho “This is a test” 12345678 ## Markdown 链接 **显示关键词，指向一个链接** 关键词百度 123[百度](https://www.baidu.com/) **展示链接地址** &lt;链接地址&gt;或者链接地址 12345678910111213141516171819202122232425&lt;www.baidu.com&gt; https://www.baidu.com/ **引用** 如需多次添加某个链接，可以先给链接取个名字，下次再用到的时候就可以直接使用这个链接的名字，不用一直记住链接地址。 ``` 给这个链接取个名字：[李老师]: https://lzqblog.top/2020-01-31/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B-%E7%89%B9%E4%BE%9B%E7%89%88/#more 使用这个链接名字 [一篇美好小文章][李老师]``` [李老师]: https://lzqblog.top/2020-01-31/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B-%E7%89%B9%E4%BE%9B%E7%89%88/#more 重要的事情说三遍，嘿嘿 [一篇美好小文章][李老师] [一篇美好小文章][李老师] [一篇美好小文章][李老师]**将所有链接放文末**``` I get 10 times more traffic from [Google] [1] than from[Yahoo] [2] or [MSN] [3]. [1]: http://google.com/ &quot;Google&quot; [2]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [3]: http://search.msn.com/ &quot;MSN Search&quot; I get 10 times more traffic from [Google] 1 than from[Yahoo] 2 or [MSN] 3. 插入图片123456789101112![]()![图片名称](图片地址 &quot;option title&quot;)``` + 开头一个感叹号！+ 接着一个方括号，里面放上图片的替代文字，可以不用写。+ 接着一个普通括号，里面放上图片地址。&quot;option title&quot;:鼠标悬置于图片上会出现的标题文字，可以不写。 目前有很多免费的在线图床网站。这里使用[ImgURL](https://imgurl.org/)在线工具,上传要插入的图片，网页会自动生成URL等地址，如下图所示。 ![](https://ftp.bmp.ovh/imgs/2020/02/160e2425ef881f4f.jpg)用上诉方法插图一张萌萌的图片 12345![](https://ftp.bmp.ovh/imgs/2020/02/4c9db7d1991a2aa7.jpg) 如果是firefox浏览器，右键鼠标点击复制图像地址，有的地址很长很长很长，但也不是每次复制的图像地址都能成功，所以这个方法比较悬。当然不同浏览器会有一定的区别。 123456789101112131415161718192021222324252627282930313233343536373839404142434445但是呢，我们家**可爱的李老师**用阿里oss搭建了自己的图床，还非常&lt;font color=pink&gt;sweet&lt;/font&gt;的教我用。所以呢，我现在就不用这么麻烦得用上面得方法啦！ **设置图片大小** ``` 设置图片大小方法一&lt;img src=&quot;图片地址&quot; width=&quot;30%&quot; height=&quot;30%&quot;&gt; 原图大小 ![](https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif) 改变图片大小 &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;20%&quot; height=&quot;20%&quot;&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;15%&quot; height=&quot;15%&quot;&gt; &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;10%&quot; height=&quot;20%&quot;&gt; 设置图片大小方法二&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt; &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;200&quot; height=&quot;200&quot;&gt; &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;150&quot; height=&quot;150&quot;&gt; （方法二的结果就不展示了）``` 原图大小 ![](https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif) 调整后大小 &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;20%&quot; height=&quot;20%&quot;&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;15%&quot; height=&quot;10%&quot;&gt; &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot; width=&quot;10%&quot; height=&quot;15%&quot;&gt; ``` 图片位置设置 &lt;div align=left&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt;&lt;/div&gt;&lt;div align=center&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt;&lt;/div&gt;&lt;div align=right&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt;&lt;/div&gt;``` &lt;div align=left&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt;&lt;/div&gt;&lt;div align=center&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt;&lt;/div&gt;&lt;div align=right&gt;&lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pooh.gif&quot;&gt;&lt;/div&gt;## Markdown 表格 Markdown制作表格使用`|`来分隔不同的单元格，使用`-`来分隔表头和其他行。 ``` | 表头 | 表头 | | ---- | ---- | | 单元格 | 单元格 | | 单元格 | 单元格 | 表头 表头 单元格 单元格 单元格 单元格 Markdown还可以设置表格的对其方式-:设置内容和标题栏居右对齐。:-设置内容和标题栏居左对齐。:-:设置内容和标题栏居中对齐。 123456789101112131415161718192021222324252627| 左对齐 | 居中 | 右对齐| | :---- | :----: | ----: | | 单元格为左对齐 | 单元格居中 | 单元格右对齐 | | 单元格为左对齐 | 单元格居中 | 单元格右对齐 |``` | 左对齐 | 居中 | 右对齐| | :---- | :----: | ----: | | 单元格为左对齐 | 单元格居中 | 单元格右对齐 | | 单元格为左对齐 | 单元格居中 | 单元格右对齐 | ## Markdown 初级技巧 **斜体、粗体**``` *single asterisks*_single underscores_**double asterisks**__double underscores__ &lt;em&gt;single asterisks&lt;/em&gt;&lt;em&gt;single underscores&lt;/em&gt;&lt;strong&gt;double asterisks&lt;/strong&gt;&lt;strong&gt;double underscores&lt;/strong&gt; single asterisks single underscores double asterisks double underscores single asterisks single underscores double asterisks double underscores 删除线 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869~~这是要删除的内容~~ ``` ~~这是要删除的内容~~ **字体添加下划线** ``` &lt;u&gt;需要添加下划线的字体&lt;/u&gt;``` &lt;u&gt;需要添加下划线的字体&lt;/u&gt;## Markdown 高级技巧 **转义** Markdown使用很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdown使用反斜杠转义字符。 ``` **文本加粗** \\*\\*正常显示星号\\*\\*``` **文本加粗** \\*\\*正常显示星号\\*\\* Markdown支持以下这些符号前面加上反斜杠来帮助插入普通的符号。 \\ 反斜线 ` 反引号 \\* 星号 \\- 下划线 {} 花括号 [] 方括号 () 小括号 \\# 井号键 \\+ 加号 \\- 减号 . 英文句点 ! 感叹号 **更改字体、大小、颜色** ``` 字体 &lt;font face=&quot;字体名称&quot;&gt;显示的内容&lt;/font&gt;&lt;font face=&quot;黑体&quot;&gt;黑体&lt;/font&gt; &lt;font face=&quot;Times New Roman&quot;&gt;Times New Roman&lt;/font&gt; &lt;font face=&quot;Arial&quot;&gt;Arial&lt;/font&gt; 颜色 &lt;font color=red&gt;红色&lt;/font&gt; &lt;font color=orange&gt;橙色&lt;/font&gt; &lt;font color=yellow&gt;黄色&lt;/font&gt; $\\color{#FF0000}{红}$ $\\color{#FF7D00}{橙}$ $\\color{#FF0000}{黄}$ $\\color{#00FF00}{绿}$ $\\color{#0000FF}{蓝}$ $\\color{#00FFFF}{靛}$ $\\color{#FF00FF}{紫}$尺寸（可能的值：从 1 到 7 的数字。浏览器默认值是 3） &lt;font size=1&gt;字体尺寸大小为1&lt;/font&gt; &lt;font size=2&gt;字体尺寸大小为2&lt;/font&gt; &lt;font size=3&gt;字体尺寸大小为3&lt;/font&gt;``` &lt;font face=&quot;黑体&quot;&gt;黑体&lt;/font&gt; &lt;font face=&quot;Times New Roman&quot;&gt;Times New Roman&lt;/font&gt; &lt;font face=&quot;Arial&quot;&gt;Arial&lt;/font&gt; &lt;font color=red&gt;红色&lt;/font&gt; &lt;font color=orange&gt;橙色&lt;/font&gt; &lt;font color=yellow&gt;黄色&lt;/font&gt; $\\color{#FF0000}{红}$ $\\color{#FF7D00}{橙}$ $\\color{#FF0000}{黄}$ $\\color{#00FF00}{绿}$ $\\color{#0000FF}{蓝}$ $\\color{#00FFFF}{靛}$ $\\color{#FF00FF}{紫}$ &lt;font size=1&gt;字体尺寸大小为1&lt;/font&gt; &lt;font size=2&gt;字体尺寸大小为2&lt;/font&gt; &lt;font size=3&gt;字体尺寸大小为3&lt;/font&gt; **为文字添加背景色** 借助table,tr,td等表格标签的bgcolor属性来实现背景色。这里对于文字背景色的设置，只是讲那一整行看作一个表格，更改了那个格子的背景色。 GreenYellow 12345678910&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=GreenYellow&gt;GreenYellow&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;**HTML标签'&lt; span&gt;'可同时实现更改字体、大小、颜色等要求** ``` &lt;span style=&quot;color:Hotpink;font-family:Impact;font-style:Arial;cursor:crosshair;font-size:20px&quot;&gt;Impact font Hotpink Arial 20px with crosshair pointer&lt;/span&gt; &lt;u&gt;&lt;font size=1&gt;鼠标放上去试一试&lt;/font&gt;&lt;/u&gt; &lt;span style=&quot;color:Hotpink; font-family:Georgia; font-size:20px;&quot;&gt;Mr Li is so cute.&lt;/span&gt; &lt;span style=&quot;color:Hotpink; font-family:Georgia;cursor:crosshair; font-size:20px;&quot;&gt;Mr Li is so cute.&lt;/span&gt; &lt;u&gt;&lt;font size=1&gt;鼠标放上去试一试&lt;/font&gt;&lt;/u&gt; Impact font Hotpink Arial 20px with crosshair pointer 鼠标放上去试一试 Mr Li is so cute. Mr Li is so cute. 鼠标放上去试一试 复选框使用- [ ]和- [×]语法可以创建复选框 123456- [x] Markdown - [ ] JavaScript + [x] Markdown + [ ] JavaScript * [x] Markdown * [ ] JavaScript Markdown JavaScript Markdown JavaScript Markdown JavaScript 没有的以后想到再说吧~~~ 参考资料https://lzqblog.top/2018-11-24/Markdown-frequently-used-syntax/https://www.runoob.com/markdown/md-tutorial.htmlhttps://daringfireball.net/projects/markdown/syntax#htmlhttps://blog.csdn.net/lewky_liu/article/details/85010827https://blog.csdn.net/qq_43731019/article/details/89385836https://www.jianshu.com/p/cdd313eebfd9https://blog.csdn.net/heimu24/article/details/81189700","link":"/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E6%95%99%E7%A8%8B/"},{"title":"MeRIP-Seq","text":"MeRIP-Seq数据分析 MeRIP-Seq数据分析 看看推荐的MeRIP-seq数据分析流程 今天要复现下面这篇文章中MeRIP-Seq相关的数据。 看看文章作者如何处理MeRIP-seq数据的 (一)数据下载123456789101112#GSE号：GSE133857#简单的3个样本就不写脚本啦#SRR9646136：HOC313 siControl_m6A_IP_replicate1#SRR9646140：HOC313 siControl_m6A_IP_replicate2#SRR9646137：HOC313 siControl_IgG_IPnohup preftch SRR9646136 &amp;nohup preftch SRR9646140 &amp;nohup preftch SRR9646137 &amp;#SRA文件转成fastq文件fastq-dump /path/to/xxx.sra ls *.sra|while read id; do nohup fastq-dump --split-3 $id; done (二) 数据比对step 1、质量控制1234567891011121314#质控，质控结果显示需要进一步过滤接头#另外，GC含量很高，还出现双峰！！！fastqc -t 6 *fastq -o ./qccd ./qcmultiqc *.zip -o ./#trim-galore过滤cat fq.txt|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}nohup trim_galore -q 20 --phred33 --length 20 -e 0.1 --stringency 3 --paired -o ../clean $fq1 $fq2 &amp;done#过滤后质控,质控结果显示接头去除成功 step 2、HISAT2比对①、下载hisat2索引文件（含转录组信息） 首先去HISAT2官网下载比对所需要的索引文件。注意啦，因为我们是MeRIP-seq数据的比对，构建的索引需要把转录组信息加进去，所以我们这里下载UCSC的hg38：genome_tran。注意了，这里选择UCSC的参考基因组，因为后面peak注释用到的工具——HOMER或者R包ChIPseeker内置的都是UCSC的参考基因组。如果你非要用到其他数据库的参考基因组，如NCBI的GRCh38，你就会在peaks注释阶段遇到染色体号相关的报错。当然，这个报错动动脑子也是可以解决的。 你也可以自己构建带有转录组信息的索引文件，HISAT2官网同样给出了详细的教程。但是，这个操作至少需要160G的内存😂，还是算了吧~~~没必要重复造轮子 ②、下载rRNA的fastq文件,构建rRNA文件的索引 参考资料：https://cloud.tencent.com/developer/article/1805475 1234567891011#HISAT2下载安装#conda下载hisat2今天突然不好使了库😢，手动下载以下wget https://cloud.biohpc.swmed.edu/index.php/s/oTtGWbWjaxsQ2Ho/download#file download发现其是zip文件unzip downloadecho 'export PATH=/home/gongyuqi/biosoft/HISAT2/hisat2-2.2.1:$PATH' &gt;&gt; ~/.bashrcsource ~/.bashrc#构建rRNA文件索引cd /home/gongyuqi/ref/rRNAhisat2-build -p 4 rRNA.fasta ./index_hisat2/rRNA ③、去除rRNAs 123456789101112#--un-conc:输出没有比对到rRNA上的readsfor i in {36,40,37}donohup hisat2 -x /home/gongyuqi/ref/rRNA/index_hisat2/rRNA \\ -1 SRR96461${i}_1_val_1.fq \\ -2 SRR96461${i}_2_val_2.fq \\ --un-conc ../rmr_rRNA/SRR96461${i}_rmr_%.fq \\ -p 16 -S ../rmr_rRNA/SRR96461${i}.sam &amp;done#去除rRNA后再次质控后发现GC含量更加异常了！！！#另外，参考基因组的比对率也挺低的，50-70%之间。这批数据感觉质量不是很OK呀😢先分析下去看看~ ④、HISAT2比对含有转录组信息的参考基因组 123456789101112131415161718192021#hisat2比对ls *_1.fq&gt; rmr_fq1.txtls *_2.fq&gt; rmr_fq2.txtpaste rmr_fq1.txt rmr_fq2.txt &gt; rmr_fq.txtref=/home/gongyuqi/ref/hg38/index/hisat2_UCSC/hg38_tran/genome_trancat rmr_fq.txt|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}sample=${fq1%%_*}.sort.bamnohup hisat2 -p 8 -x $ref -1 $fq1 -2 $fq2 | samtools sort -@ 8 -o ../align/$sample - &amp;donenohup hisat2 -p 8 -x $ref -1 SRR9646139_rmr_1.fq -2 SRR9646139_rmr_2.fq | samtools sort -@ 8 -o ../align/SRR9646139.sort.bam - &amp;#构建索引,bam文件导入IGV时需要索引文件同时存在ls *.bam | while read id; do nohup samtools flagstat $id &amp; donels *.bam | while read id; do nohup samtools index $id &amp; donels *.bam | while read id; do nohup bamCoverage -b $id -o ${id%%_*}.bw &amp; done (三)MACS2 call peaksMACS2官网参考资料 1234567#call peaks: siControl_m6A_IgG_replicate1macs2 callpeak -t SRR9646136.sort.bam -c SRR9646137.sort.bam -f BAM -g hs -n siControl_m6A_IgG_replicate1 -B -q 0.01 --outdir siControl_m6A_IgG#call peaks: siControl_m6A_IgG_replicate2macs2 callpeak -t SRR9646140.sort.bam -c SRR9646137.sort.bam -f BAM -g hs -n siControl_m6A_IgG_replicate2 -B -q 0.01 --outdir siControl_m6A_IgGmacs2 callpeak -t SRR9646138.sort.bam -c SRR9646139.sort.bam -f BAM -g hs -n siFTO_m6A_IgG -B -q 0.01 --outdir ../peaks (四)peaks注释123456789101112131415161718192021222324252627library(org.Hs.eg.db)library(ChIPseeker)library(TxDb.Hsapiens.UCSC.hg38.knownGene)txdb &lt;- TxDb.Hsapiens.UCSC.hg38.knownGenefiles &lt;- list(replicate1=&quot;siControl_m6A_IgG_replicate1_summits.bed&quot;, replicate2=&quot;siControl_m6A_IgG_replicate2_summits.bed&quot;)peakAnno_replicate1 &lt;- annotatePeak(files[[1]], TxDb=txdb, annoDb=&quot;org.Hs.eg.db&quot;)peakAnno_replicate2 &lt;- annotatePeak(files[[2]], TxDb=txdb, annoDb=&quot;org.Hs.eg.db&quot;)#大部分情况下会出现多了peaks对应到同一个SYMBOL上，所以一定要记得unique一下siFTO_siControl_peak&lt;-peakAnno_siFTO_siControl_m6A@anno@elementMetadata@listDatasiFTO_siControl_peak&lt;-as.data.frame(siFTO_siControl_peak)replicate1_peak&lt;-as.data.frame(peakAnno_replicate1@anno@elementMetadata@listData)replicate1_peak_SYMBOL&lt;-peakAnno_replicate1@anno@elementMetadata@listData$SYMBOLreplicate1_peak_SYMBOL&lt;-unique(na.omit(replicate1_peak_SYMBOL))replicate2_peak&lt;-as.data.frame(peakAnno_replicate2@anno@elementMetadata@listData)replicate2_peak_SYMBOL&lt;-peakAnno_replicate2@anno@elementMetadata@listData$SYMBOLreplicate2_peak_SYMBOL&lt;-unique(na.omit(replicate2_peak_SYMBOL))common_peaks&lt;-intersect(replicate1_peak_SYMBOL,replicate2_peak_SYMBOL)table(&quot;CCND1&quot;%in%common_peaks)#在此，结果为TRUE 将bam文件导入IGV,可视化CCND1上m6A的情况","link":"/MeRIP-Seq/"},{"title":"MySQL中事物的隔离级别","text":"···· MySQL中事物的隔离级别123456789101112事物的隔离界别 脏读 不可重复读 幻读read uncommited √ √ √read commited × √ √repeatable read × × √serialization × × ×mysql中默认 repeatable readoracle中默认 read commited查看隔离级别：select @@tx_isolation;设置隔离级别：set session|global transaction isolation level 隔离级别; read uncommitted无隔离脏读（演示）、不可重复读（演示）、幻读（这里不演示） 终端1中显示当前隔离级别 设置当前隔离级别为read committed 在此查看当前隔离级别 进入test数据库 查看accout表信息 开启事物 在此事物中进行列的修改 回退 终端2显示当前隔离级别 设置当前隔离级别为read committed 在此查看当前隔离级别 进入test数据库 查看account表信息（在终端1做出修改前） 查看account表信息（在终端1做出修改后） 查看account表信息（在终端1回退后） read committed不脏读（演示）、不可重复读（不演示）、幻读（这里不演示） 设置终端1类型read committed 开启事物 修改列 结束事务 设置终端2类型read committed 开启事务 查看account表信息（终端做修改后。但未结束事务，不脏读） 查看account表信息（终端1做修改后，结束事务，终端2还处于同一事务中，此隔离级别下不可重复读） 结束事务 repeatable read不脏读（演示）、不可重复读（演示）、幻读（这里不演示） 设置终端1类型repeatable read 开启事物 修改列 结束事务 设置终端2类型repeatable read 开启事务 查看account表信息（终端1做修改后，未结束事务） 查看account表信息（终端1做修改后，结束事务，终端2未结束事务，此隔离级别下可重复读） 结束事务 终端1开启事务 查看account表信息 插入一行信息 查看account表信息 结束事务 终端2开启事务 查看account表信息（终端1做出修改后） 修改表中的列信息（终端1做出修改，但未结束事务，时间长了终端2会出现来纳瑟方框报错） 在此尝试修改表中列信息（终端1做出修改并结束事务，出现红色下划线提示，4个改变，但在终端2事务中最初只有3行信息，此处却出现4行，说明出现幻读） serialization不脏读、可重复度、不幻读 设置终端1类型serialization 查看当前隔离级别 开启事务 查看account表信息 修改表的列信息（此处并未提交执行此语句） 设置终端1类型serialization 开启事务 修改表的列信息（终端1的表修改操作在终端2前，此时终端2列修改无法操作，红色方框未报错信息，等待时间过长，只有端事务1事务结束才能进行此操作）","link":"/MySQL-transaction-isolation/"},{"title":"NGS数据分析之表观","text":"NGS数据分析之表观中的一些注意事项 一、参考基因组及注释文件的准备以果蝇为例 (1)下载果蝇参考基因组 打开谷歌(用谷歌用谷歌)，输入：drosophila_melanogaster ftp ensembl 点击红色箭头所指方向进入正确的下载网页（这个需要自己悟，总是要多试的） ① ② (2)下载参考基因组索引 打开谷歌，输入：drosophila_melanogaster ftp hisat2 点击红色箭头所指方向进入正确的下载网页（这个需要自己悟，总是要多试的） ① ② 总结：参考基因组下载方式很多，版本也不唯一，重要的是知道自己需要那个版本，这一点需要悟……就参考基因组索引而言，不同比对软件需要的索引文件不一样，可以自己构建，也可以网页下载，推荐按直接下载。","link":"/NGS%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E8%A7%82/"},{"title":"RNA-TNBC","text":"… (一)GSE652121234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586setwd(&quot;path to\\\\GSE65212&quot;)#读入样本sample&lt;-read.csv(&quot;sample.csv&quot;)TNBC&lt;-sample$Sample_geo_accession[1:55]normal&lt;-sample$Sample_geo_accession[56:66]group_list&lt;-c(rep('TNBC',55),rep('Normal',11))#读入矩阵GSE65212&lt;-read.table('GSE65212_series_matrix.txt.gz', sep='\\t',quote = &quot;&quot;,fill = T, comment.char=&quot;!&quot;,header=T)colname&lt;-colnames(GSE65212)colname&lt;-gsub(&quot;\\\\.&quot;,&quot;&quot;,colname)colname&lt;-gsub(&quot;X&quot;,&quot;&quot;,colname)colnames(GSE65212)&lt;-colnameGSE65212_TNBC_normal&lt;-GSE65212[,c(TNBC,normal)]rownames(GSE65212_TNBC_normal)&lt;-GSE65212$ID_REFrownames(GSE65212_TNBC_normal)&lt;-gsub('\\\\&quot;',&quot;&quot;,rownames(GSE65212_TNBC_normal))##ID转换GPL14877&lt;-read.table('GPL14877_HGU133Plus2_Hs_ENTREZG_mapping.txt.gz', sep='\\t',quote = &quot;&quot;,fill = T, comment.char=&quot;!&quot;,header=T)table(rownames(GSE65212_TNBC_normal)%in%GPL14877$Probe.Set.Name)#TRUE:18123 exprSet&lt;-GSE65212_TNBC_normal#step1 Probe.Set.Name转成Affy.Probe.Set.NameexprSet&lt;-GSE65212_TNBC_normaltable(rownames(GSE65212_TNBC_normal)%in%GPL14877$Probe.Set.Name)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%GPL14877$Probe.Set.Name,]dim(exprSet)GPL14877&lt;-GPL14877[match(rownames(exprSet),GPL14877$Probe.Set.Name),]head(GPL14877)exprSet[1:5,1:5]tmp&lt;-by(exprSet,GPL14877$Affy.Probe.Set.Name,function(x)rownames(x)[which.max(rowMeans(x))])probes&lt;-as.character(tmp)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%probes,]dim(exprSet)rownames(exprSet)&lt;-GPL14877[match(rownames(exprSet),GPL14877$Probe.Set.Name),7]exprSet[1:5,1:5]#step2 Affy.Probe.Set.Name转成GPL750的IDGPL570&lt;-read.table(&quot;GPL570-55999.txt&quot;,sep='\\t',quote = &quot;&quot;,fill = T, comment.char=&quot;#&quot;,header=T)GPL570&lt;-GPL570[,c(1,11)]table(GPL14877$Affy.Probe.Set.Name%in%GPL570$ID)#TRUE：18123table(rownames(exprSet)%in%GPL570$ID)dim(exprSet)head(GPL570)GPL570&lt;-GPL570[match(rownames(exprSet),GPL570$ID),]head(GPL570)exprSet[1:5,1:5]tmp&lt;-by(exprSet,GPL570$Gene.Symbol,function(x)rownames(x)[which.max(rowMeans(x))])probes&lt;-as.character(tmp)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%probes,]dim(exprSet)rownames(exprSet)&lt;-GPL570[match(rownames(exprSet),GPL570$ID),2]exprSet[1:5,1:5]#limma差异分析suppressMessages(library(limma))design&lt;-model.matrix(~0+factor(group_list))colnames(design)&lt;-levels(factor(group_list))rownames(design)&lt;-colnames(exprSet)designcontrast.matrix&lt;-makeContrasts(paste0(unique(group_list),collapse=&quot;-&quot;),levels = design)contrast.matrixfit&lt;-lmFit(exprSet,design)fit2&lt;-contrasts.fit(fit,contrast.matrix)fit2&lt;-eBayes(fit2)tempOutput&lt;-topTable(fit2,coef = 1,n=Inf)nrDEG=na.omit(tempOutput)head(nrDEG)test&lt;-nrDEGtest$id&lt;-rownames(nrDEG)topT &lt;- as.data.frame(nrDEG)logFC_Cutoff&lt;-with(topT,mean(abs(logFC))+2*sd(abs(logFC)))##2.5#拿到显著差异的基因select_gene=(abs(nrDEG$logFC)&gt;=logFC_Cutoff)&amp;(nrDEG$adj.P.Val&lt;0.05)select_gene.sig=nrDEG[select_gene,]write.csv(select_gene.sig,'GSE65212_TNBC_Normal_select_gene.sig.csv') (二)GSE762501234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889setwd(&quot;path to\\\\GSE76250&quot;)#读入样本sample&lt;-read.csv(&quot;sample.csv&quot;)TNBC&lt;-sample$锘縏NBCTNBC&lt;-gsub(&quot; &quot;,&quot;&quot;,TNBC)normal&lt;-sample$paired_normalnormal&lt;-gsub(&quot; &quot;,&quot;&quot;,normal)group_list&lt;-c(rep('TNBC',33),rep('Normal',33))#读入矩阵GSE76250&lt;-read.table('GSE76250_series_matrix.txt.gz', sep='\\t',quote = &quot;&quot;,fill = T, comment.char=&quot;!&quot;,header=T)colname&lt;-colnames(GSE76250)colname&lt;-gsub(&quot;\\\\.&quot;,&quot;&quot;,colname)colname&lt;-gsub(&quot;X&quot;,&quot;&quot;,colname)colnames(GSE76250)&lt;-colnamerownames(GSE76250)&lt;-GSE76250$ID_REFrownames(GSE76250)&lt;-gsub('\\\\&quot;',&quot;&quot;,rownames(GSE76250))GSE76250&lt;-GSE76250[,-1]GSE76250_TNBC_Normal&lt;-GSE76250[,c(TNBC,normal)]##加载R包#其实这里可以直接下载GPL文件的，多走了一步，就当了解了一个新东西library(GEOquery)gse &lt;- getGEO(filename = &quot;GSE76250_family.soft.gz&quot;,destdir = &quot;.&quot;)str(gse)length(gse)id_probe &lt;- gse@gpls$GPL17586@dataTable@tabledim(id_probe)id_probe[1:4,1:15]probe2gene &lt;- id_probe[,c(2,8)]#提取基因名称library(stringr) probe2gene$symbol=trimws(str_split(probe2gene$gene_assignment,'//',simplify = T)[,2])plot(table(table(probe2gene$symbol)),xlim=c(1,50))dim(probe2gene)View(head(probe2gene))ids2 &lt;- probe2gene[,c(1,3)]View(head(ids2))ids2[1:20,1:2]#含有缺失值table(table(unique(ids2$symbol)))#30907 ,30906个基因，一个空字符save(ids2,probe2gene,file='gse-probe2gene.Rdata')#过滤表达矩阵exprSet&lt;-GSE76250_TNBC_NormalexprSet &lt;- exprSet[rownames(exprSet) %in% ids2$probeset_id,]dim(exprSet)exprSet[1:5,1:5]#ids过滤探针ids &lt;- ids2[match(rownames(exprSet),ids2$probeset_id),]dim(ids)ids[1:5,1:2]tmp&lt;-by(exprSet,ids$symbol,function(x)rownames(x)[which.max(rowMeans(x))])probes&lt;-as.character(tmp)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%probes,]dim(exprSet)rownames(exprSet)&lt;-ids[match(rownames(exprSet),ids$probeset_id),2]exprSet[1:5,1:5]#limmasuppressMessages(library(limma))design&lt;-model.matrix(~0+factor(group_list))colnames(design)&lt;-levels(factor(group_list))rownames(design)&lt;-colnames(exprSet)designcontrast.matrix&lt;-makeContrasts(paste0(unique(group_list),collapse=&quot;-&quot;),levels = design)contrast.matrixfit&lt;-lmFit(exprSet,design)fit2&lt;-contrasts.fit(fit,contrast.matrix)fit2&lt;-eBayes(fit2)tempOutput&lt;-topTable(fit2,coef = 1,n=Inf)nrDEG=na.omit(tempOutput)head(nrDEG)test&lt;-nrDEGtest$id&lt;-rownames(nrDEG)topT &lt;- as.data.frame(nrDEG)logFC_Cutoff&lt;-with(topT,mean(abs(logFC))+2*sd(abs(logFC)))##0.584#拿到显著差异的基因select_gene=(abs(nrDEG$logFC)&gt;=logFC_Cutoff)&amp;(nrDEG$adj.P.Val&lt;0.05)select_gene.sig=nrDEG[select_gene,]write.csv(select_gene.sig,'GSE76250_TNBC_Normal_select_gene.sig.csv') (三)获取两个数据集里面同时上调的差异基因12345678910setwd(&quot;path to\\\\GSE65212&quot;)GSE65212_gene&lt;-read.csv(&quot;GSE65212_TNBC_Normal_select_gene.sig.csv&quot;)GSE65212_gene&lt;-GSE65212_gene[GSE65212_gene$logFC&gt;0,]setwd(&quot;path to\\\\GSE76250&quot;)GSE76250_gene&lt;-read.csv(&quot;GSE76250_TNBC_Normal_select_gene.sig.csv&quot;)GSE76250_gene&lt;-GSE76250_gene[GSE76250_gene$logFC&gt;0,]overlap_gene&lt;-intersect(GSE65212_gene$X,GSE76250_gene$X)write.csv(over_lapgene,&quot;TNBC_upregulation_gene.csv&quot;)","link":"/RNA-TNBC/"},{"title":"RNA-seq数据分析流程","text":"普通RNA-seq数据分析 RNA-seq数据分析流程（一）原始数据的质控以其中一个样本为例（其他样本指控结果基本类似） 12345cd /home/raw-data#对每个样本进行质控fastqc -t 7 /root/project/atac/raw/*gz -o ./#统计合并所有指控结果multiqc *fastqc.zip --ignore *.html 1、帮忙分析的数据，从公司给的结果的样本名中包含“clean”，我猜想公司大概已经进行过一次过滤，而且也完全没有接头。 2、另外，没有过关的这些数据也很难通过过滤得到改善（因为我使用trim_galore过滤，质控结果更差了） 3、但是我还是不死心呀，我分别取了一个原始的和自己过滤的样本的前10000条reads,进行比对。结果发现原始的比对率可以达到95%+，自己过滤后的样本比对率比较低。 4、所以，这批数据，公司应该已经进行过滤操作了。 （二）比对比对软件：hisat2 不同数据类型所需要使用的比对软件也是有讲究的呢，可千万不能乱使用，会出问题的，刚开始接触数据分析时，知道有很多比对软件，但一直用bowtie2进行比对，后面就翻车啦～～～～～～ 比对软件的选择：https://www.jianshu.com/p/849f8ada0ab7https://www.jianshu.com/p/681e02e7f9afhttps://zhuanlan.zhihu.com/p/26506787 NGS—-bwaChip（也包括ATAC）—-bowtie2RNA—-hisat2（推荐）、star、Tophat、subjunc 第一步：序列比对1234567#cat sampleKO1455_FRAS190065880-1a_1.clean.fq.gz KO1455_FRAS190065880-1a_2.clean.fq.gzKO1462_FRAS190065881-1a_1.clean.fq.gz KO1462_FRAS190065881-1a_2.clean.fq.gzKO1471_FRAS190065882-1a_1.clean.fq.gz KO1471_FRAS190065882-1a_2.clean.fq.gzWT1441_FRAS190065878-1a_1.clean.fq.gz WT1441_FRAS190065878-1a_2.clean.fq.gzWT1465_FRAS190065879-1a_1.clean.fq.gz WT1465_FRAS190065879-1a_2.clean.fq.gzWT1474_FRAS190065877-1a_1.clean.fq.gz WT1474_FRAS190065877-1a_2.clean.fq.gz 123456789101112131415161718192021222324#因为不想让本机太辛苦，要让它歇息一会儿,所以将KO和WT分批次跑cat sample | head -n 3 &gt; KO_samplecat sample | tail -n 3 &gt; WT_sampleref=/home/reference/mm10/genomecat KO_sample|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}sample=${fq1%%_*}.sort.bamhisat2 -p 7 -x $ref -1 $fq1 -2 $fq2 | samtools sort -@ 7 -o /home/align/$sample -donecat WT_sample|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}sample=${fq1%%_*}.sort.bamhisat2 -p 7 -x $ref -1 $fq1 -2 $fq2 | samtools sort -@ 7 -o /home/align/$sample -done#各样本比对率都在95%以上，这样的比对结果才可以比较有信心的进行后续分析了。 第二步：构建索引1ls *.bam|xargs -i samtools index {} 第三部：生成状态文件(统计比对情况)1ls *.bam|xargs -i samtools flagstat -@ 7 {} （三）制作表达矩阵参考资料：https://www.bioinfo-scrounger.com/archives/407/ 12ref=/home/reference/gtf/gencode.v33.annotation.gtf featureCounts -T 8 -p -t exon -g gene_id -a $gtf -o RNA-matrix.txt *.bam 1&gt;counts.id.log 2&gt;&amp;1 查看featureCounts的情况 1cat counts.id.log 在此，要记录一个错误。第一次操作时误使用了人类的gtf文件，结果日志文件显示比对率在10%左右……使用小鼠gtf文件后，比对结果正常多了（因为这批数据是小鼠组织细胞测序的结果）。如下所示。 （四）差异分析第一步：将reads矩阵导入R中12345rm(list = ls())options(stringsAsFactors = F)setwd(&quot;G:\\\\seq-analysis-data\\\\RNA-seq&quot;)a=read.table('RNA-matrix.txt',header = T)ckeck_a=a[1:4,1:12] 第二步：获取正确的Geneid 首先看看矩阵中的Geneid长什么样 所以要去掉小数点及后面的数字，获取正确的Geneid1234567library(stringr)class(str_split(a$Geneid,'[.]',simplify = T))a$ensembl_id=str_split(a$Geneid,'[.]',simplify = T)[,1]rownames(a)&lt;-a$ensembl_idcheck_a_again=a[1:4,1:12]exprSet&lt;-a[,7:12]ckeck_exprSet&lt;-head(exprSet) 表达矩阵现在的样子（Geneid名去掉了多余的部分） 第三步：id转换Geneid和symbol并不是完全一一对应的，存在部分多个Geneid对应同一个基因名的情况。 因为rownames必须唯一，所以就要去掉重复的Geneid或者只保留其中一个（这个保留就比较的人为了~） 方法一：使用biomaRt包 此方法几乎可以转换全部的id(推荐) 1234567891011121314151617181920212223242526272829library(biomaRt)library(curl)#此步骤需要网速，网速不好会报错（curl相关的），多试几遍，会有好运气的mart &lt;- useDataset(&quot;mmusculus_gene_ensembl&quot;, useMart(&quot;ensembl&quot;))##人类选择hsapiens_gene_ensemblgene&lt;-row.names(exprSet)#要转换成SYMBOL的Geneidgene_name&lt;-getBM(attributes=c(&quot;description&quot;,&quot;external_gene_name&quot;,&quot;ensembl_gene_id&quot;), filters = &quot;ensembl_gene_id&quot;, values = gene, mart = mart)head(gene_name)exprSet&lt;-a[,7:12]head(exprSet)table(rownames(exprSet)%in%gene_name$ensembl_gene_id)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%gene_name$ensembl_gene_id,]dim(exprSet)symbol&lt;-gene_name[match(rownames(exprSet),gene_name$ensembl_gene_id),]head(symbol)exprSet[1:6,1:6]tmp&lt;-by(exprSet,symbol$external_gene_name,function(x)rownames(x)[which.max(rowMeans(x))])probes&lt;-as.character(tmp)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%probes,]dim(exprSet)rownames(exprSet)&lt;-symbol[match(rownames(exprSet),symbol$ensembl_gene_id),2]exprSet[1:5,1:5]write.csv(exprSet,'raw_exprSet.csv')#exprSet&lt;-read.csv('raw_exprSet.csv')#rownames(exprSet)&lt;-exprSet$X#exprSet&lt;-exprSet[,-1] 方法二：使用org.Mm.eg.db包可能因为此包不怎么更新，id转换率只有50%左右，这样回丢掉很多信息的，可能会导致后续分析不可靠（不推荐） 1234567891011121314151617181920library(clusterProfiler)library(org.Mm.eg.db)symbol&lt;-bitr(rownames(exprSet),fromType = &quot;ENSEMBL&quot;, toType=c(&quot;SYMBOL&quot;,&quot;ENTREZID&quot;),OrgDb = org.Mm.eg.db)colnames(symbol)&lt;-c(&quot;ensembl_id&quot;,&quot;SYMBOL&quot;,&quot;ENTREZID&quot;)check_symbol&lt;-symbol[1:10,1:3]table(rownames(exprSet)%in%symbol$ensembl_id)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%symbol$ensembl_id,]dim(exprSet)symbol&lt;-symbol[match(rownames(exprSet),symbol$ensembl_id),]head(symbol)exprSet[1:6,1:6]tmp&lt;-by(exprSet,symbol$SYMBOL,function(x)rownames(x)[which.max(rowMeans(x))])probes&lt;-as.character(tmp)dim(exprSet)exprSet&lt;-exprSet[rownames(exprSet)%in%probes,]dim(exprSet)rownames(exprSet)&lt;-symbol[match(rownames(exprSet),symbol$ensembl_id),2]exprSet[1:5,1:5] https://www.jianshu.com/p/3a0e1e3e41d0 https://www.jianshu.com/p/c06fea33b60f 在进行后续分析之前，检验目的基因的敲除效果1234567891011121314#Ang表达量探索Ang&lt;-raw_exprSet[rownames(raw_exprSet)==&quot;Ang&quot;,]colnames(Ang)&lt;-c(&quot;KO1455&quot;,&quot;KO1462&quot;,&quot;KO1471&quot;,&quot;WT1441&quot;,&quot;WT1465&quot;,&quot;WT1474&quot;)ang&lt;-as.data.frame(t(Ang))ang$Group&lt;-c(rep(&quot;KO&quot;,3),rep(&quot;WT&quot;,3))ang$Sample&lt;-rownames(ang)colnames(ang)&lt;-c(&quot;Counts&quot;,&quot;Group&quot;,&quot;Sample&quot;)library(ggplot2)p&lt;-ggplot(ang,aes(Sample,Counts,group=Group))+ geom_bar(aes(fill=Group),stat='identity')+ scale_fill_manual(values = c(&quot;#FFA500&quot;,&quot;#228B22&quot;))+ ylab(&quot;The expression of Ang&quot;)+ xlab(&quot;&quot;)p+theme_bw()+theme(panel.border = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),axis.line = element_line(colour = &quot;black&quot;)) 由图可看出：目的基因敲除效果显著；另外，也可以一定程度上反应上游数据分析的可靠性（因为在做测序之前，一定是先实验验证过敲低效率的）。 https://zhuanlan.zhihu.com/p/92473504 https://www.sohu.com/a/319083452_785442 第四步：DESeq2进行差异分析1、DESeq2差异分析的主要步骤 123456789101112131415161718192021222324library(DESeq2)#DEseq2要求输入数据是由整数组成的矩阵。#DESeq2要求矩阵是没有标准化的。##构建一个dds对象exprSet_integer=round(exprSet) (colData &lt;- data.frame(row.names=colnames(exprSet_integer), group_list=group_list) )dds &lt;- DESeqDataSetFromMatrix(countData = exprSet_integer, colData = colData, design = ~ group_list)dds2 &lt;- DESeq(dds)##使用DESeq函数对dds进行normalizeresultsNames(dds2)##查看结果的名称 res &lt;- results(dds2, contrast=c(&quot;group_list&quot;,&quot;KO&quot;,&quot;WT&quot;))##提取想要的差异分析结果，这里是&quot;KO&quot;组对&quot;WT&quot;组进行比较。resOrdered &lt;- res[order(res$padj),]##按padj从小到大排序summary(res)DEG&lt;-data.frame(resOrdered)##得到差异分析的data.framehead(DEG)DEG = na.omit(DEG)write.csv(DEG,'DESeq2_KO_VS_WT DEG.csv')#DEG&lt;-read.csv(&quot;DESeq2_KO_VS_WT DEG.csv&quot;)#rownames(DEG)&lt;-DEG$X#DEG&lt;-DEG[,-1] 2、DESeq2标准化前后的对比 123456789101112131415#数据离散度非常大的RNA-seq的基因的reads的counts矩阵经过normalization后#变成了类似于芯片表达矩阵的表达矩阵。生成的2行2列的图可以看出normalization前后的变化rld&lt;-rlogTransformation(dds2)##得到经过DESeq2软件normalization的表达矩阵。exprSet_normalized&lt;-assay(rld)par(cex = 0.7)n.sample=ncol(exprSet)if(n.sample&gt;40) par(cex = 0.5)cols &lt;- rainbow(n.sample*1.2)par(mfrow=c(2,2))colnames(exprSet_integer)&lt;-c(&quot;KO1455&quot;,&quot;KO1462&quot;,&quot;KO1471&quot;,&quot;WT1441&quot;,&quot;WT1465&quot;,&quot;WT1474&quot;)colnames(exprSet_normalized)&lt;-c(&quot;KO1455&quot;,&quot;KO1462&quot;,&quot;KO1471&quot;,&quot;WT1441&quot;,&quot;WT1465&quot;,&quot;WT1474&quot;)boxplot(exprSet_integer, col = cols,main=&quot;expression value&quot;,las=2)boxplot(exprSet_normalized, col = cols,main=&quot;expression value&quot;,las=2)hist(as.matrix(exprSet))hist(exprSet_normalized) https://zhuanlan.zhihu.com/p/30350531 第五步：可视化 …… 富集分析（可以使用KEGG、GO，这里使用GO）大致看一下受目的基因影响的信号通路12345678910111213141516171819202122232425262728#1、差异基因的提取topT &lt;- as.data.frame(DEG)logFC_Cutoff&lt;-with(topT,mean(abs(log2FoldChange))+2*sd(abs(log2FoldChange)))logFC_cutoff&lt;-logFC_Cutoffselect_gene=(abs(DEG$log2FoldChange)&gt;=logFC_cutoff)&amp;(DEG$pvalue&lt;0.05)select_gene.sig=DEG[select_gene,]write.csv(select_gene.sig,'DESeq2_select_gene.sig.csv')gene=rownames(select_gene.sig)#2、差异基因的通路富集library(clusterProfiler)library(topGO)library(Rgraphviz)library(pathview)library(org.Mm.eg.db)DEG.entrez_id = mapIds(x = org.Mm.eg.db, keys = gene, keytype = &quot;SYMBOL&quot;, column = &quot;ENTREZID&quot;)DEG.entrez_id = na.omit(DEG.entrez_id)erich.go.BP = enrichGO(gene = DEG.entrez_id, OrgDb = org.Mm.eg.db, keyType = &quot;ENTREZID&quot;, ont = &quot;BP&quot;, pvalueCutoff = 0.5, qvalueCutoff = 0.5)##分析完成后，作图dotplot(erich.go.BP) https://www.jianshu.com/p/47b5ea646932?utm_source=desktop&amp;utm_medium=timeline 热图绘制1234567891011library(pheatmap)colnames(exprSet)&lt;-c(&quot;KO1455&quot;,&quot;KO1462&quot;,&quot;KO1471&quot;,&quot;WT1441&quot;,&quot;WT1465&quot;,&quot;WT1474&quot;)choose_upgene=names(sort(apply(up_gene, 1, mad),decreasing = T)[1:40])choose_upgene_matrix=exprSet[choose_upgene,]choose_downgene=names(sort(apply(down_gene, 1, mad),decreasing = T)[1:40])choose_downgene_matrix=exprSet[choose_downgene,]choose_matrix=rbind(choose_upgene_matrix,choose_downgene_matrix)choose_matrix=t(scale(t(choose_matrix)))annotation_col = data.frame(CellType = factor(rep(c(&quot;KO&quot;, &quot;WT&quot;), each=3)),Time = 1:3)rownames(annotation_col)&lt;-colnames(exprSet)pheatmap(choose_matrix,fontsize = 6,cexCol=1,cellwidth=10,cellheigh=5,color = colorRampPalette(c('navy','white','firebrick3'))(50),annotation_col=annotation_col)","link":"/RNA-seq%20analysis/"},{"title":"RNA-seq-Liver","text":"质控1234567input_dir=/home/gongyuqi/project/RNA-seq/liver-mouse/cleanoutput_dir=/home/gongyuqi/project/RNA-seq/liver-mouse/clean/QC#对每个样本进行质控nohup fastqc -t 18 $input_dir/*fq.gz -o $output_dir &amp;#统计合并所有指控结果dir=/home/gongyuqi/project/RNA-seq/liver-mouse/clean/QCmultiqc $dir/*fastqc.zip --ignore $dir/*.html Hisat2 进行比对1234567891011121314151617#因为不想让本机太辛苦，要让它歇息一会儿,所以将KO和WT分批次跑ls *1.clean.fq.gz &gt;fq1.txtls *2.clean.fq.gz &gt;fq2.txtpaste fq1.txt fq2.txt &gt;sample.txt #将下列脚本存放在hisat2-alignment.sh文件中ref=/home/gongyuqi/ref/mm10/index/hisat2/mm10cat sample.txt|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}sample=${fq1%%_*}.sort.bamhisat2 -p 8 -x $ref -1 $fq1 -2 $fq2 | samtools sort -@ 8 -o /home/gongyuqi/project/RNA-seq/liver-mouse/align/$sample -done#运行比对程序chmod +x hisat2-alignment.shnohup bash hisat2-alignment.sh &amp; 统计比对情况123456#建立索引ls *.bam|xargs -i samtools index {}#生成状态文件，统计比对情况ls *.bam|xargs -i samtools flagstat -@ 8 {} | grep &quot;mapped (&quot; &gt; mapping_ratio.txtls *.bam &gt; bam.txtpaste bam.txt mapping_ratio.txt|gawk 'BEGIN{FS=&quot; &quot;}{print $1,$6}'|sed 's/(/: /g' &gt; mapping_results.txt 比对后质量控制qualimap bamqc &amp; qualimap multi-bamqc123qualimap --java-mem-size=16G multi-bamqc -d multi-bamqc.txt -outdir qualimap_multi-bamqc -outformat PDF:HTML qualimap rnaseq &amp; multiqc123456cat qualimap_rnaseq.shgtf=/home/gongyuqi/ref/mm10/GTF/gencode.vM24.annotation.gtfls *.bam | while read id; do qualimap --java-mem-size=16G rnaseq -pe -bam $id -gtf $gtf -outdir ${id%%.*}_qualimap_rnaseq -outformat PDF:HTML -oc ${id%%.*}; donenohup ./qualimap_rnaseq.sh &amp;#对qualimap rnaseq生成的文件进行multiqcmultiqc *_rnaseq featureCounts生成表达矩阵1234gtf=/home/gongyuqi/ref/mm10/GTF/gencode.vM24.annotation.gtfnohup featureCounts -T 8 -p -t exon -g gene_id -a $gtf -o ../matrix/RNA-matrix.txt *.bam 1&gt;counts.id.log 2&gt;&amp;1 &amp;#生成counts的质控文件multiqc RNA-matrix.txt.summary","link":"/RNA-seq-Liver/"},{"title":"R语言apply、lapply、strsplit、&amp;#37in&amp;#37用法介绍","text":"生信分析下游过程中，为了构建好可用于个性化分析的数据，往往要对原有数据进行整理，这个时候apply、lapply、strsplit、%in%就会在不同的地方发光✨发热🔥啦 R语言apply、lapply、strsplit、%in%用法介绍apply函数用于遍历数组中的行或列，并且使用指定函数来对其元素进行处理 1apply(x,MARGIN,FUN,...) x：数组、矩阵、数据框等MARGIN：1表示行，2表示列FUN：自定义的函数 123456x&lt;-matrix(rnorm(20),4,5)xrowmean&lt;-apply(x, 1, mean)rowmeancolmean&lt;-apply(x, 2, mean)colmean lapply函数1lapply(x,FUN) lapply()函数中多出来的l代表的是list，所以lapply()和apply()的区别在于输出的格式，lapply()的输出是一个列表（list），所以lapply()函数不需要MARGIN参数 遍历列表向量内的每个元素，并且使用指定函数来对其元素进行处理。返回列表向量。 1234y&lt;-matrix(1:10,2,5)plus3&lt;-lapply(y, function(x) x+3)plus3unlist(plus3) 1234y&lt;-c(&quot;BABA&quot;,&quot;MAMA&quot;,&quot;YEYE&quot;,&quot;NAINAI&quot;)lower&lt;-lapply(y, tolower)lowerunlist(lower) 特定情况下，apply族函数比for、while函数更方便。 strsplit函数12strsplit(x, split, fixed= F, perl= F, useBytes= F) 参数x为字符串格式向量，函数依次对向量的每个元素进行拆分。 参数split为拆分位置的字串向量，即在哪个字串处开始拆分。 fixed= T则表示是用普通文本匹配或者正则表达式的精确匹配,用普通文本来匹配的运算速度要快些。 参数useBytes表示是否逐字节进行匹配，默认为FALSE，表示是按字符匹配而不是按字节进行匹配。 12345678910111213141516s&lt;-as.data.frame(c(&quot;444.dady_good-tempered.444&quot;,&quot;222.mom_bad-tempered.222&quot;,&quot;111.me_completely-lazy.111&quot;))colnames(s)&lt;-&quot;family&quot;rownames(s)&lt;-c(&quot;dady&quot;,&quot;mom&quot;,&quot;me&quot;)list&lt;-strsplit(as.character(s$family),&quot;[.]&quot;)listfinal&lt;-as.data.frame(list)finaldisposition&lt;-final[2,]dispositionx&lt;-as.data.frame(t(final[2,]))xcolnames(x)&lt;-&quot;strsplit&quot;rownames(x)&lt;-c(&quot;dady&quot;,&quot;mom&quot;,&quot;me&quot;)xs$disposition&lt;-x$strsplits %in%函数%in%函数在不同数据集间取相同的差异基因中非常有用！ 1234567rm(list = ls())a&lt;-c('p','h','o','r','k','s','k','l')b&lt;-c('w','l','k','y','o','d','a','b','l','p','w','r')a%in%ba[a%in%b]b%in%ab[b%in%a]","link":"/R%E8%AF%AD%E8%A8%80apply-lapply-strsplit-in-%E7%94%A8%E6%B3%95%E4%BB%8B%E7%BB%8D/"},{"title":"SSH服务","text":"远程控制服务：配置sshd服务、 安全密钥验证、远程传输命令不间断会话服务：管理远程会话、会话共享功能 远程控制服务 linux中一切皆是文件，配置服务即是对配置文件进行修改 主配置文件：最重要的配置文件一般情况下在/etc/服务名称/服务名称.config 普通文件：主要被调用，常规参数 （一）配置sshd服务想要使用SSH协议来远程管理Linux系统，则需要部署配置sshd服务程序。 一般的服务程序并不会在配置文件修改之后立即获得最新的参数。如果想让新配置文件生效，则需要手动重启相应的服务程序。最好也将这个服务程序加入到开机启动项中，这样系统在下一次启动时，该服务程序便会自动运行，继续为用户提供服务。 查看主配置文件 1cat /etc/ssh/sshd_config Xshell远程登陆服务器,此时是需要密码验证的。 1ssh 192.168.12.136 修改主配置文件内容如下，使得不可以通过root用户方式远程登陆服务器。 1vim /etc/ssh/sshd_config 在此远程连接服务器，发现依然可以通过root用户方式远程登陆服务器。 1ssh 192.168.12.64 重启服务 123#这里以sshd服务为例systemctl restart sshdsystemctl enable sshd 再次进行远程登陆 1ssh 192.168.12.64 每次输入密码后，会再次弹出界面框要求输入密码，进入死循环。。。。。。 （二）安全密钥验证 客户端主机中生成“密钥对” 1ssh-keygen 把客户端主机生成的公钥文件传送至远程主机 1ssh-copy-id 116.63.129.7 查看服务器端认证的文件 1cat /root/.ssh/authorized_keys 客户端登陆服务端 1ssh 116.63.129.7 此时就会发现，客户端远程访问服务器就不需要密码登陆了。因为密钥的优先级是高于密码的。这样是不是很方便呢？嘿嘿😁 而且，密钥方式登陆更安全哦！😊 修改服务器端配置文件，使其只支持密钥方式登陆 12#注意啦，一定要是服务器端的配置文件vim /etc/ssh/sshd_config 重启相应服务 1systemctl restart sshd Xshell远程登陆服务器（验证） 1ssh 116.63.129.7 登陆界面默认无法输入密码！ 注意啦，有对应私钥的客户端无需密码即可登陆拥有对应公钥的服务端 远程传输命令scp（secure copy）是一个基于SSH协议在网络之间进行安全传输的命令。 将客户端文件传给服务器端 123#一下操作在客户端进行echo &quot;this is a test about scp&quot;&gt;scptestscp /root/scptest 116.63.129.7:/home 进入服务器端进行查看 1cat /home/scptest 将服务器端文件传给客户端 12#以下操作在服务器端进行echo &quot;This is the message from server&quot;&gt;fromserver 进入客户端进行文件下载和查看 12scp 116.63.129.7:home/fromserver /rootcat /root/fromserver 不间断会话服务首先配置yum仓库 管理远程会话screen命令能做的事情非常多：可以用-S参数创建会话窗口；用-d参数将指定会话进行离线处理；用-r参数恢复指定会话；用-x参数一次性恢复所有的会话；用-ls参数显示当前已有的会话；以及用-wipe参数把目前无法使用的会话删除 screen服务介绍 12#创建一个会话框testscreen -S test 12#查看系统当前的会话screen -ls 12#退出当前会话exit screen实战一 12#创建TEST会话框,会发现屏幕闪跳了以下screen -S TEST 1234lscd /bincd /roo/biosoftls 当前界面如下 此时将终端kill掉，模拟远程连接过程中网络出错 12screen -lsscreen -r TEST #恢复离线的对话框 此时就会恢复到上一次发生事故时的界面 screen实战二 有时候在编辑文件时，文件内容还未及时保存，远程连接出现问题。这个时候screen服务就会🌟发光发热🔥 1screen vim test.txt 此时将终端kill掉，模拟远程连接过程中网络出错 12screen -lsscreen -r 10777 会话共享功能 终端A远程连接服务器+创建会话 ssh服务将终端A远程连接到服务器（这里假装连接了，其实就是连接到本地的虚拟机，为了方便展示） 1234#连接ssh 192.168.12.136#创建会话框screen -S test 终端B远程连接服务器+同步 12ssh 192.168.12.136screen -x 此后，终端A和终端B就同步画面啦 参考资料ssh配置文件ssh_config和sshd_config区别 远程控制服务_文字版 远程控制服务_视频版","link":"/SSH%E6%9C%8D%E5%8A%A1/"},{"title":"SUPPA2实战","text":"… SUPPA2实战（DLBCL）因为最近ascp无法使用了，只好使用preftch进行数据下载SRA数据，再自己转成fastq。 1nohup prefetch --option-file SRR_Acc_List1.txt &amp; 1234567scp -P 6001 ./test.csv gongyuqi@124.77.170.70:/home/gongyuqi#port: 6654gmb39@81.70.205.254:/home/data/gmb39/project/AS/SUPPA2/SRA #port: 6001gongyuqi@124.77.170.70:/home/gongyuqi/project/AS/SUPPA2/SRA conda安装参考：https://www.jianshu.com/p/edaa744ea47d 数据下载、转换12345678910#SRA格式转成fastq格式ls *.sra | while read iddo nohup fastq-dump --split-3 $id &amp;done#gzip压缩一下，节省空间ls *.fastq | while read iddo nohup pigz -p 8 $id &amp;done 质控1234567891011121314nohup fastqc -t 16 *gz -o ./fastqc &amp;nohup multiqc ./fastqc/*.zip -o ./multiqc &amp;out_dir=/home/gongyuqi/project/AS/SUPPA2/cleantrim_galore --paired --quality 20 -a AGATCGGAAGAGC -a2 AGATCGGAAGAGC --length 20 -o $out_dir R1.fq.gz R2.fq.gzout_dir=/home/gongyuqi/project/AS/SUPPA2/cleancat sample.txt|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}nohup trim_galore --paired --quality 25 -e 0.1 --length 36 -a AGATCGGAAGAGC -a2 AGATCGGAAGAGC -o $out_dir $fq1 $fq2 &amp;done (1)、运行salmon软件定量 12345678cat clean_sample.txt|while read iddoarr=(${id})fq1=${arr[0]}fq2=${arr[1]}dir=${fq1%%_*}nohup salmon quant -i /home/gongyuqi/ref/hg38/ensembl/SUPPA_ref/Ensembl_hg19_salmon_index -l ISF --gcBias -1 $fq1 -2 $fq2 -p 8 -o /home/gongyuqi/project/AS/SUPPA2/salmon_quant/$dir &amp;done (2)、提取所有样本的TPM值并合并为一个文件 12suppa_dir=/home/gongyuqi/miniconda3/envs/SUPPA2_3.9/bin/nohup python $suppa_dir/multipleFieldSelection.py -i /home/gongyuqi/project/AS/SUPPA2/salmon_quant/*/quant.sf -k 1 -f 4 -o /home/gongyuqi/project/AS/SUPPA2/output/iso_tpm.txt &amp; (3)、使iso_tpm.txt文件中的转录本id同下载的gtf文件id一致 运行此R脚本，会生成iso_tpm_formatted.txt文件 1Rscript format_Ensembl_ids.R iso_tpm.txt 三、PSI计算1、根据参考基因组注释文件生成可变剪切事件文件 -i GTF文件-o 输出文件前缀-e 输出文件中包含的可变剪切类型-f 输出的格式 12345678910#生成ioe文件suppa_dir=/home/gongyuqi/miniconda3/envs/SUPPA2_3.9/bin/gtf_dir=/home/gongyuqi/ref/hg38/ensembl/SUPPA_refpython $suppa_dir/suppa.py generateEvents -i $gtf_dir/Homo_sapiens.GRCh37.75.formatted.gtf -o ensembl_hg19.events -e SE SS MX RI FL -f ioe#合并所有的ioe文件cd $gtf_dirawk ' FNR==1 &amp;&amp; NR!=1 { while (/^&lt;header&gt;/) getline; } 1 {print}' *.ioe &gt; ensembl_hg19.events.ioe 结果如下 2、计算样本的PSI值 123cd /home/gongyuqi/project/AS/SUPPA2ioe_merge_file=~/ref/hg38/ensembl/SUPPA_ref/hg19_event/ensembl_hg19.events.ioenohup python $suppa_dir/suppa.py psiPerEvent -i $ioe_merge_file -e iso_tpm_formatted.txt -o TRA2_events &amp; 根据ensembl ID获取基因名参考资料：https://www.sohu.com/a/245475759_777125 123456789101112131415161718192021222324library(biomaRt)#Biomart目前提供了四种数据库，可以使用listMarts()函数查看listMarts()# biomart version#1 ENSEMBL_MART_ENSEMBL Ensembl Genes 103#2 ENSEMBL_MART_MOUSE Mouse strains 103#3 ENSEMBL_MART_SNP Ensembl Variation 103#4 ENSEMBL_MART_FUNCGEN Ensembl Regulation 103#当处理人类基因时，用useMart()函数选择“ENSEMBL_MART_ENSEMBL”human_mart&lt;-useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;)#ensembl数据库中包含了很多个数据集，可以使用这个代码查看datasets&lt;-listDatasets(human_mart)view(datasets)#从上一步的结果中，选择人类基因的ensembl数据集：hsapiens_gene_ensemblmy_datasets&lt;-useDataset(&quot;hsapiens_gene_ensembl&quot;,mart=human_nart)#输入需要转换的ensembl IDensembl_id&lt;-&quot;ENSG00000149554&quot;#getBM进行ID转换ensembl_symbol&lt;-getBM(attributes = c(&quot;ensembl_gene_id&quot;,&quot;chromosome_name&quot;,&quot;hgnc_symbol&quot;,&quot;hgnc_id&quot;), filters = &quot;ensembl_gene_id&quot;, values = ensembl_id, mart = my_datasets) ENSG00000000457","link":"/SUPPA2%E5%AE%9E%E6%88%98/"},{"title":"UEFI","text":"windows启动模式包括两种：UEFI+GPT、Legacy+MBR。在此介绍3中在windows上查看系统启动模式的方法。 （一）、diskpart判断UEFI方法win+r打开运行输入diskpart命令。 GPT一行下面带*表示使用的GPT分区表+UEFI启动模式。 （二）系统信息判断 也可以win+r打开运行，输入msinfo32命令来执行打开系统信息。 在系统摘要中可以看到bios的模式 （三）、磁盘管理判断UEFIwin+r打开，输入compmgmt.msc，打开管理。 在磁盘管理中将鼠标放到引导硬盘的分区上，是UEFI引导会有相应的提示，否则为传统模式。 参考资料https://jingyan.baidu.com/article/49ad8bce1bf31a1935d8fa09.html","link":"/UEFI/"},{"title":"MySQL简单介绍","text":"MySQL的下载安装简单命令介绍SQLyog的下载安装 MySQL简单介绍MySQL下载安装下载安装教程Choose Setup Type一栏，我选择的是Custom安装路径自定义到了非系统盘 MySQL服务的启动和停止 方式一：计算机—右击管理—服务 右击MySQL会显示启动、停止等选项 方式二：通过管理员身份运行cmd net restart 服务名称（启动服务） net stop 服务名称（停止服务） MySQL服务的登陆和退出 方式一：通过MySQL自带的客户端，只限于root用户 方式二：通过windows自带的客户端（cmd）mysql [-h 主机名 -P 端口号] -u 用户名 -p密码如果是本机的话，[]中的内容可以省去 退出：exit或者ctrl+C MySQL配置环境变量my.ini文件为MySQL的配置文件 MySQL常见命令介绍 显示数据库信息 1show databases; information_schema：保存源数据细信息。mysql：保存用户信息。performance_schema：搜集性能信息，性能参数。test：测试数据哭，为空。可以在次建表，修改库，删除库。 进入数据库mysql 1use mysql; 查看数据库中的表信息 1show tables; 查看当前属于哪个数据库 1select database(); 在数据库中新建一张表（test数据库） 12use test;select database()； 123creat table friends( id int, name varchar(20)); 查看当前数据库中的表 1show tables(); 查看friends表的结构 1desc friends; 查看friends表的信息 1select * from friends 为friends表添加数据信息 1234insert into friends(id,name) values(1,&quot;susu&quot;);insert into friends(id,name) values(2,&quot;zhengrong&quot;);insert into friends(id,name) values(3,&quot;yinyu&quot;);insert into friends(id,name) values(4,&quot;jiaru&quot;); 查看friends表中的信息 1select * from friends; 修改friends表中的信息 12update friends set name=&quot;SUSU&quot; where id=1select * from friends 删除friends表中某项信息 12delete from friends where id=4select * from friends 查看MySQL服务端版本 方法一： mysql里面查询 1select version(); 方法二： cmd里面查询 1mysql --version MySQL的语法规范 不区分大小写 每条命令最好用分号结尾 每条命令根据需要，可以进行换行或缩进 注释 单行注释：#注释文字 单行注释：– 注释文字 多行注释：/注释文字/ SQLyog的下载安装SQLyog下载安装教程注意：在进行连接的时候一定要保证MySQL服务是开启的。如果经常用到可以改成自动开启模式，如果不常用可以改成手动开启模式，手动开启模式下就要注意用之前要保证服务是开启的。 1——以root用户的方式进行登陆 2——SQL语句框，在此可以输入语句，自动将输入的语句规范化 3——执行完语句后会在此处收到相应的消息 4——当前处于mysql数据库，1中加粗字体mysql也表明当前处于mysql数据库，2中最后一条执行语句为USE mysql;","link":"/MySQL%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},{"title":"gawk 进阶","text":"使用变量、处理数组、处理模式 gawk进阶一、使用变量（一）、内建变量字段和记录分隔符变量 print命令会自动将OFS变量的值放置在输出中的每个字段间。通过设置OFS变量，可以在输 出中使用任意字符串来分隔字段。 FIELDWIDTHS变量允许你不依靠字段分隔符来读取记录。在一些应用程序中，数据并没有使 用字段分隔符，而是被放置在了记录中的特定列。这种情况下，必须设定FIELDWIDTHS变量来 匹配数据在记录中的位置。 如果你用默认的FS和RS变量值来读取这组数据，gawk就会把每行作为一条单独的记录来读 取，并将记录中的空格当作字段分隔符。要解决这个问题，只需把FS变量设置成换行符。这就表明数据流中的每行都是一个单独的字 段，每行上的所有数据都属于同一个字段。把RS变量设置成空字符串，然后在数据记录间留一个空白行。gawk会 把每个空白行当作一个记录分隔符。 数据变量 跟shell变量不同，在脚本中引用gawk变量时，变量名前不加美元符。 ARGC和ARGV变量允许从shell中 获得命令行参数的总数以及它们的值。但这可能有点麻烦，因为gawk并不会将程序脚本当成命令行参数的一部分。ARGV数组从索引0开始，代表的是命令。 ENVIRON变量使用关联数组来提取shell环境变量。关联数组用文本 作为数组的索引值，而不是数值。 ENVIRON[“HOME”]变量从shell中提取了HOME环境变量的值。类似地，ENVIRON[“PATH”]提 取了PATH环境变量的值 NF变量可以让你在不知道具体位置的情况下指定记录中的后一个数据字段。NF变量含有数据文件中后一个数据字段的数字值。可以在它前面加个美元符将其用作字段 变量。 FNR变量含有当前数据文件中已处理过的记录数，NR变量则含有已处理过的记录总数。 FNR变量的值在gawk处理第二个数据文件时被重置了，而NR变量则在处理第二个数据文件时 继续计数。结果就是：如果只使用一个数据文件作为输入，FNR和NR的值是相同的；如果使用多 个数据文件作为输入，FNR的值会在处理每个数据文件时被重置，而NR的值则会继续计数直到处 理完所有的数据文件。 （二）、自定义变量在脚本中给变量赋值 在命令行上给变量赋值 前两个例子可以让在不改变脚本代码的情况下改变脚本的行为。第一个例子显示了文件的第二个数据字段，第二个例子显示了第三个数据字段，只要在命令行上设置n变量的值就行。 使用命令行参数来定义变量值会有一个问题。在你设置了变量后，这个值在代码的BEGIN部分不可用。 可以用-v命令行参数来解决这个问题。它允许你在BEGIN代码之前设定变量。在命令行上，-v命令行参数必须放在脚本代码之前。变量紧挨-v参数，也要放在脚本之前。 二、处理数组为了在单个变量中存储多个值，许多编程语言都提供数组。gawk编程语言使用关联数组提供 数组功能。关联数组跟数字数组不同之处在于它的索引值可以是任意文本字符串。你不需要用连续的数字来标识数组中的数据元素。相反，关联数组用各种字符串来引用值。每个索引字符串都必须能 够唯一地标识出赋给它的数据元素。跟Python中的字典类似。 定义数组变量 var[index] = element 其中var是变量名，index是关联数组的索引值，element是数据元素值。下面是一些gawk中数组变量的例子。 ' 和&quot;在本例中效果不一样 遍历数组变量 关联数组变量的问题在于你可能无法知晓索引值是什么。跟使用连续数字作为索引值的数字数组不同，关联数组的索引可以是任何东西。 如果要在gawk中遍历一个关联数组，可以用for语句的一种特殊形式。 注意：(key in f)才能正确执行，key in f会报错。 删除数组变量 delete array[index] 删除命令会从数组中删除关联索引值和相关的数据元素值。 三、使用模式（一）、正则表达式gawk程序会用正则表达式对记录中所有的数据字段进行匹配，包括字段分隔符。 （二）、匹配操作符匹配操作符（matching operator）允许将正则表达式限定在记录中的特定数据字段。匹配操作符是波浪线（~）。可以指定匹配操作符、数据字段变量以及要匹配的正则表达式。 $2变量代表记录中的第一个数据字段。这个表达式会过滤出第一个字段以文本2开头的所有记录。data同理。 也可以用!符号来排除正则表达式的匹配。 $1 !~ /expression/ （三）、数学表达式第一个测试显示所有属于root用户组（组ID为0）的系统用户。 对文本数据使用表达式，必须小心。跟正则表达式不同，表达式必须完全匹配。数据必须跟模式严格匹配。第二个测试中没有匹配任何记录，因为第一个数据字段的值不在任何记录中。第三个测试用值data11匹配了一条记录。 四、结构化命令（一）、if语句gawk编程语言支持标准的if-then-else格式的if语句。你必须为if语句定义一个求值的条件，并将其用圆括号括起来。如果条件求值为TRUE，紧跟在if语句后的语句会执行。如果条 件求值为FALSE，这条语句就会被跳过。 格式： if (condition) statement1 也可以将它放在一行上，像这样： if (condition) statement1 可以在多行使用else子句,也可以在单行上使用else子句，但必须在if语句部分之后使用分号。 if (condition) statement1; else statement2 （二）、while语句while语句为gawk程序提供了一个基本的循环功能。下面是while语句的格式。 while (condition) { statements} while循环允许遍历一组数据，并检查迭代的结束条件。 第一个例子和第二个例子中的number=0多余了，忽略它。 （三）、do-while语句do-while语句类似于while语句，但会在检查条件语句之前执行命令。下面是do-while语句的格式。 do{ statements} while (condition) 这种格式保证了语句会在条件被求值之前至少执行一次。当需要在求值条件前执行语句时， 这个特性非常方便。 这个脚本会读取每条记录的数据字段并将它们加在一起，直到累加结果达到4。如果第一个数据字段大于4（就像在第二条和第三条记录中看到的那样），则脚本会保证在条件被求值前至少读取第一个数据字段的内容。 （四）、for语句for语句是许多编程语言执行循环的常见方法。gawk编程语言支持C风格的for循环。将多个功能合并到一个语句有助于简化循环。 五、格式化打印格式化打印命令：printf 如果你需要显示一个字符串变量，可以用格式化指定符%s。如果你需要显示一个整数 值，可以用%d或%i（%d是十进制数的C风格显示方式）。 你需要在printf命令的末尾手动添加换行符来生成新行。没添加的话，printf命令会继续在同一行打印后续输出。 每个printf的输出都会出现在同一行上。为了终止该行，END部分打印了一个换行符。 下面这个例子也可以再次体会RS的作用。FS指定输入的分隔符，RS指定输入记录分隔符。即对记录的数据从新定义的分隔符，所以$1为1，$2为2,$3为3,一次类推。 printf命令在处理浮点值时也非常方便。通过为变量指定一个格式，你可以让输出看起来更统一。可以使用%.1f格式指定符来强制printf命令将浮点值近似到小数点后一位。 可以用printf命令来帮助格式化输出，使得输出信息看起来更美观。 ◉ width：指定了输出字段小宽度的数字值。如果输出短于这个值，printf会将文本右 对齐，并用空格进行填充。如果输出比指定的宽度还要长，则按照实际的长度输出。 ◉ prec：这是一个数字值，指定了浮点数中小数点后面位数，或者文本字符串中显示的 大字符数。 ◉ -（减号）：指明在向格式化空间中放入数据时采用左对齐而不是右对齐。 首先，让我们将print 命令转换成printf命令，它会产生跟print命令相同的输出。printf命令用%s格式化指定符来作为这两个字符串值的占位符。printf需要手动换行。 通过添加一个值为16的修饰符，我们强制第一个字符串的输出宽度为16个字符。默认情况下， printf命令使用右对齐来将数据放到格式化空间中。要改成左对齐，只需给修饰符加一个减号 即可 六 内建函数（一）、数学函数gawk中内建的数学函数。 int() 函数会生成一个值的整数部分，但它并不会四舍五入取近似值。它的做法像R语言中的floor函数。它会生成该值和0之间接近该值的整数。 int()函数在值为5.6时返回5，在值为-5.6时则返回-5。 rand()函数非常适合创建随机数，但这个随机数只在0和1之间（不包括0或1）。要得到更大的数，就需要放大返 回值。 产生较大整数随机数的常见方法是用rand()函数和int()函数创建一个算法。 x = int(10 * rand()) gawk语言对于它能够处理的数值有一个限定区间。如果 超出了这个区间，就会得到一条错误消息。 （二）、字符串函数怎么感觉有点麻烦，先不看这个了。😏 （三）、时间函数 七、自定义函数（一）、定义函数要定义自己的函数，必须用function关键字。函数名必须能够唯一标识函数。 function name([variables]){ statements} （二）、使用自定义函数在定义函数时，它必须出现在所有代码块之前（包括BEGIN代码块）。乍一看可能有点怪异，但它有助于将函数代码与gawk程序的其他部分分开。 （三）、创建函数库gawk提供了一种途径来将多个函数放到一个库文件中，这样就能在所有的gawk程序中使用了。 首先，你需要创建一个存储所有gawk函数的文件。 需要使用-f命令行参数来使用它们。很遗憾，不能将-f命令 行参数和内联gawk脚本放到一起使用，不过可以在同一个命令行中使用多个-f参数。 因此，要使用库，只要创建一个含有你的gawk程序的文件，然后在命令行上同时指定库文件和程序文件就行了。","link":"/gawk-%E8%BF%9B%E9%98%B6/"},{"title":"WES","text":"whole exome-sequencing analysis pipeline 全外显子数据分析(一) 环境搭建1、GATK依赖Java 8/JDK 1.8 (Oracle or OpenJDK) 12#查看一下环境中是否有java，如果有，版本是否符合要求java --version 2、下载安装GATK4 12345wget https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zipunzip gatk-4.2.0.0.zip#将当前路径加到环境变量中echo 'export PATH=/home/gongyuqi/biosoft/GATK/gatk-4.2.0.0:$PATH' &gt;&gt; ~/.bashrcsource ~/.bashrc 3、下载其他需要的软件 123456789101112#首先创建WES的conda环境conda create -n WES#其次下载其他需要的软件conda install -y python=3.6.2conda install -y bwa sra-tools samtools bcftools snpEFF multiqc qualimap #激活环境，准备开始实战演练conda activate WES#创建存放各阶段数据的文件夹cd /home/gongyuqi/project/WESmkdir raw qc clean mutaioncd qc &amp;&amp; mkdir raw_qc clean_qc (二) WES测试数据下载1、数据来源GSE153707，我们这里从EBI直接下载fastq文件 12345678910111213#将下列数据下载的脚本保存到download.sh文件中dir=/home/gongyuqi/.aspera/connect/etc/asperaweb_id_dsa.opensshx=_1y=_2for id in {11,12}doascp -QT -l 300m -P33001 -i $dir era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR121/0$id/SRR121359$id/SRR121359$id$x.fastq.gz .ascp -QT -l 300m -P33001 -i $dir era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR121/0$id/SRR121359$id/SRR121359$id$y.fastq.gz . done#赋予执行权限chmod +x download.sh#放后台进行数据下载nohup ./download.sh &amp; 2、了解文章WES数据处理的相关步骤和参数 (三) GATK准备bam文件用于找变异1、 比对GATK官网提供的参考基因组1234#====step 1 首先质空，查看数据是否需要进行过滤#我们这里的质控结果显示reads的整体质量还不错（GC含量看着不太过分基本不影响后续的分析），可以直接用于比对ls ../raw/*.gz|xargs fastqc -t 4 -o ./multiqc *.zip 12345678910111213#====step 2 比对ls *1.fastq.gz &gt;fq1.txtls *2.fastq.gz &gt;fq2.txtpaste fq1.txt fq2.txt &gt; sample.txtindex=/home/gongyuqi/ref/GATK/hg38/v0/Homo_sapiens_assembly38.fasta.64cat sample.txt | while read iddo nohuparr=(${id})fq1=${arr[0]}fq2=${arr[1]}sample=${fq1%%_*}bwa mem -t 8 -R &quot;@RG\\tID:$sample\\tSM:$sample\\tLB:WGS\\tPL:Illumina&quot; /$index $fq1 $fq2 |samtools sort -@ 8 -o ../align/$sample.sort.bam - &amp;&gt;&gt;../align/$sample.log &amp;done 12# ====step 3 查看比对结果ls *.bam|while read id;do samtools flagstat $id;done 2、标记PCR重复123456789101112131415161718192021# ====step 1 MarkDuplicatesls *.bam|while read sampledo nohup gatk --java-options &quot;-Xmx20G -Djava.io.tmpdir=./&quot; \\ MarkDuplicates \\ --INPUT $sample \\ --OUTPUT ${sample%%.*}_marked.bam \\ -M ${sample%%.*}.metrics &amp;done# ====step 2 FixMateInformationls *_marked.bam|while read sampledo nohup gatk --java-options &quot;-Xmx20G -Djava.io.tmpdir=./&quot; \\ FixMateInformation \\ -I ${sample} \\ -O ${sample%%_*}_marked_fixed.bam \\ -SO coordinate &amp;done#建立*_fixed.bam文件索引ls *_fixed.bam|while read sampledo nohup samtools index $sample &amp;done 3、碱基矫正 BaseRecalibrator1234567891011121314151617181920212223242526272829303132333435ref=/home/gongyuqi/ref/GATK/hg38/v0/Homo_sapiens_assembly38.fastasnp=/home/gongyuqi/ref/GATK/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcfindel=/home/gongyuqi/ref/GATK/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gzls *_marked_fixed.bam|while read sampledo nohup gatk \\ --java-options &quot;-Xmx20G -Djava.io.tmpdir=./&quot; \\ BaseRecalibrator \\ -R $ref \\ -I $sample \\ --known-sites $snp \\ --known-sites $indel \\ -O ${sample%%_*}_recal.table \\ 1&gt;${sample%%_*}_log.recal 2&gt;&amp;1 &amp;done#BWA比对时设置@RG的重要性在这一步体现出来了，算法通过@RG中的ID来识别各个独立的测序过程ls *_marked_fixed.bam|while read sampledo nohup gatk \\ --java-options &quot;-Xmx20G -Djava.io.tmpdir=./&quot; ApplyBQSR \\ -R $ref \\ -I $sample \\ -bqsr ${sample%%_*}_recal.table \\ -O ${sample%%_*}_bqsr.bam \\ 1&gt;${sample%%_*}_log.ApplyBQSR 2&gt;&amp;1 &amp;done# 查看比对结果ls *bqsr.bam|while read iddo nohup samtools flagstat $id &amp;donegatk --java-options &quot;-Xmx20G&quot; AnalyzeCovariates \\ -before recal_data.table1 \\ -after recal_data.table2 \\ -plots AnalyzeCovariates.pdf 4、HaplotypeCaller 这一步是使用GATK的HaplotypeCaller找变异，但现在GATK官网推荐GATK4+Mutect2找变异，在这里我们还是简单运行一下HaplotypeCaller。 123456789ls *bqsr.bam|while read sampledo nohup gatk \\ --java-options &quot;-Xmx20G -Djava.io.tmpdir=./&quot; HaplotypeCaller \\ -R $ref \\ -I $sample \\ --dbsnp $snp \\ -O ../vcf/${sample%%_*}_raw.vcf \\ 1&gt;../vcf/${sample%%_*}_log.HC 2&gt;&amp;1 &amp;done (三) Mutect2找变异Mutect2 v4.1.0.0的教程目前已经不被GATK官方强烈推荐。官方更新推荐了Mutect2 v4.1.1.0 and later版本的使用教程，更新过的教程比较推荐用于正常对照样本&gt;40的WES测序数据。如果没有这么多的样本量，Mutect2 v4.1.0.0的教程也同样可以使用，只是有部分参数在新的Mutect2版本中被弃用或是将其功能整合到另外的参数去了。具体问题可以查看官网说明。 1、过滤种系突变 首先需要一个germline variant sites VCF文件，去官网下载af-only-gnomad.hg38.vcf.gz文件。记得一并下载对应的.tbi文件 1234567891011121314ref=/home/gongyuqi/ref/GATK/hg38/v0/Homo_sapiens_assembly38.fastagermine_vcf=/home/gongyuqi/ref/GATK/hg38/af-only-gnomad/af-only-gnomad.hg38.vcf.gznohup gatk --java-options &quot;-Xmx20g&quot; Mutect2 \\-R $ref \\-I SRR12135912_bqsr.bam \\-tumor SRR12135912 \\-I SRR12135911_bqsr.bam \\-normal SRR12135911 \\--germline-resource $germine_vcf \\--af-of-alleles-not-in-resource 0.0000025 \\--disable-read-filter MateOnSameContigOrNoMappedMateReadFilter \\--bam-output HQ461-untreated-Mutect2.bam \\-O ../Mutect2/HQ461-untreated.Mutect2.vcf &amp; 生成文件如下： 2、考虑样品纯度及假阳性 通常，病人的肿瘤样本是混合有正常组织的，这个时候我们可以选择GetPileupSummaries工具来计算肿瘤样本的污染程度。对于Mutect2流程而言，GATK只考虑了肿瘤样本中正常样本的污染情况，不考虑正常样本中肿瘤样本的污染情况。这里我们的处理组:HQ461,对照组untreated。，虽然我们的样本是肿瘤细胞的单克隆株，我们这里还是跑一下这个流程。 step 1、生成af-only-gnomad.hg38.SNP_biallelic.vcf.gz12345678ref=/home/gongyuqi/ref/GATK/hg38/v0/Homo_sapiens_assembly38.fastagermine_vcf=/home/gongyuqi/ref/GATK/hg38/af-only-gnomad/af-only-gnomad.hg38.vcf.gznohup gatk SelectVariants \\-R $ref \\-V $germine_vcf \\--select-type-to-include SNP \\--restrict-alleles-to BIALLELIC \\-O /home/gongyuqi/ref/GATK/hg38/af-only-gnomad/af-only-gnomad.hg38.SNP_biallelic.vcf.gz &amp; step 2、GetPileupSummaries1234567891011121314germine_biallelic_vcf=/home/gongyuqi/ref/GATK/hg38/af-only-gnomad/af-only-gnomad.hg38.SNP_biallelic.vcf.gzgenomic_intervals=/home/gongyuqi/ref/GATK/hg38/v0/wgs_calling_regions.hg38.interval_listnohup gatk GetPileupSummaries \\-I SRR12135911_bqsr.bam \\-L $genomic_intervals \\-V $germine_biallelic_vcf \\-O ../Mutect2/SRR12135911.pileups.table &amp;nohup gatk GetPileupSummaries \\-I SRR12135912_bqsr.bam \\-L $genomic_intervals \\-V $germine_biallelic_vcf \\-O ../Mutect2/SRR12135912.pileups.table &amp; step 3、 CalculateContamination 计算污染比例，用以过滤掉somatic variant中一些可能由污染导致的假阳性突变 1234nohup gatk CalculateContamination \\-I SRR12135912.pileups.table \\-matched SRR12135911.pileups.table \\-O HQ461-untreated.calculatecontamination.table &amp; step 3、 FilterMutectCalls 结合CalculateContamination的评估，进行最后的过滤 123456ref=/home/gongyuqi/ref/GATK/hg38/v0/Homo_sapiens_assembly38.fastanohup gatk FilterMutectCalls \\-R $ref \\-V HQ461-untreated.Mutect2.vcf \\--contamination-table HQ461-untreated.calculatecontamination.table \\-O HQ461-untreated.filtered.Mutect2.vcf &amp; 至此，我们找变异的步骤就差不多走完了。你会发现我们并没有使用到CollectSequencingArtifactMetrics和FilterByOrientationBias参数，那是因为我们当前版本的Mutect2已经将FilterByOrientationBias的功能整合进FilterMutectCalls中了，详情见GATK论坛中的一篇帖子。 另外，在做WES分析时，对照组是很重要的。拿肿瘤样本举例，tumor-only的模式会得到很多很多很多的假阳性，详情见GATK论坛中的另一篇帖子。对照组的存在会大大降低假阳性率。 (四) SnpEff &amp; SnpSift注释 以下流程参考SnpEff &amp; SnpSift官网文档 1、SnpEff软件及所需注释数据库的下载step 1、下载安装SnpEff软件下载SnpEff，现在SnpEff和SnpSift是绑定在一起的。 12345cd ~/biosoft/#下载wget https://snpeff.blob.core.windows.net/versions/snpEff_latest_core.zip &amp;#安装unzip snpEff_latest_core.zip step 2、下载SnpEff软件需要的数据库文件下载 SnpEff databases: 官网给的命令是java -jar snpEff.jar download GRCh38.76。但是运行这个命令会报错：找不到GRCh38.76 12345#运行下面的命令查看SnpEff目前支持的databasejava -jar snpEff.jar databases | less#运行下面的命令查看SnpEff支持的GRCh38 database,目前是GRCh38.99java -jar snpEff.jar databases | grep -i GRCh38#看来SnpEff网站更新不及时呀，在网站维护和更新这一点上，要重点表扬GATK团队！！！ 1234#于是下载GRCh38.99nohup java -jar snpEff.jar download GRCh38.99 &amp;#但是这个网络问题是无解的。我最后还是自己手动下载snpEff_v5_0_GRCh38.99.zip文件#解压完，最后GRCh38.99文件夹绝对路径/home/gongyuqi/biosoft/snpEff/data/GRCh38.99 GRCh38.99内容如下 2、SnpEff、SnpSift使用step 1、SnpEff注释12#the &quot;verbose&quot; mode (command line option -v), this makes SnpEff to show a lot of information which can be useful for debugging.java -Xmx16g -jar snpEff.jar -v GRCh38.99 /home/gongyuqi/project/WES/Mutect2/HQ461-untreated.filtered.Mutect2.vcf &gt; HQ461-untreated.filtered.ann.vcf step 2、SnpSift过滤该部分同时参考官网说明文档及某优秀博客用SnpSift过滤VCF文件 1234#保留Filter字段为'PASS'或缺失值的记录cat HQ461-untreated.filtered.ann.vcf | java -jar SnpSift.jar filter &quot;( na FILTER ) | (FILTER = 'PASS')&quot; &gt; HQ461-untreated.filtered.ann.snpsift.vcf#保留INFO中ANN字段的IMPACT为'HIGH'或'MODERATE'的记录cat HQ461-untreated.filtered.ann.snpsift.vcf | java -jar SnpSift.jar filter &quot;(ANN[0].IMPACT has 'HIGH') | (ANN[0].IMPACT has 'MODERATE')&quot; &gt; HQ461-untreated.filtered.ann.snpsift.second.vcf 123#简单看一下三个vcf文件的大小ls -lh *.vcfls *.vcf|while read id; do cat $id|wc -l;done step 3、sed命令复现SnpSift的功能 这里不难发现，SnpSift其实就是对文本文件的处理。要是shell脚本够扎实，也完全可以不依赖SnpSift。重要的是，在学习NGS组学分析过程中，但凡有锻炼自己shell脚本的地方，就一定要抓住机会动手写一写💪 这里我们可以用一个简单的sed命令解决 123456#第一个`sed`命令打印出所有标记为PASS的位点#第二个`sed`命令承接上一个管道输出的内容，保留标记为HIGH或者MODERATE的位点cat HQ461-untreated.filtered.ann.vcf | grep -v &quot;#&quot;| \\sed -n '/PASS/p' - | \\sed -n '/\\(HIGH\\|MODERATE\\)/p' - \\&gt;test_PASS_HIGH_MODERATE.third.vcf 1234#检验一下我们`sed`脚本的正确性#看看位点数目是否一致cat HQ461-untreated.filtered.ann.snpsift.second.vcf|grep -v &quot;#&quot;|wc -lcat test_PASS_HIGH_MODERATE.second.vcf|wc -l 123#再具体看看内容是否一致cat HQ461-untreated.filtered.ann.snpsift.second.vcf|grep -v &quot;#&quot;|less -SNcat test_PASS_HIGH_MODERATE.second.vcf|less -SN HQ461-untreated.filtered.ann.snpsift.second.vcftest_PASS_HIGH_MODERATE.second.vcf (五) 原文数据复现 真正接受检验的时候到了，看看我们的pipeline能不能复现文章的数据。这也是进一步检验我们pipeline的正确性的重要环节（当然也不能迷信文章的结果，但是7+文章的数据分析质量应该还是可以作为一个较好的参考）。 再看看我们过滤后的vcf文件中是否有检测到CDK12的G到A的突变 学习WES心得体会 1、学习某个软件的用法时，主要参考官方文档！！！可以选择性参考相关博客主博客，但切记拿来主义！！！深刻理解用到的每个参数。 2、WES的相关分析还有很多，我们这里只涉及到SNV。同样的，突变的注释也涉及到多种软件，我们这里只涉及到了SnpEff。学海无涯啊~~~从培养起自主学习的思维和习惯开始。 3、拿到最终的注释文件（.vcf），我们还可以用到maftools、ComplexHeatmap等R包将突变情况进行可视化。 4、不急不慢，不慌不忙，坚持下去❤","link":"/WES/"},{"title":"scRNA-TNBC","text":"… 8个TNBC患者的单细胞样本合并+分析 (一)数据合并step 1、组织文件格式12#下载好数据，不同病人样本的matrix.mtx.gz、features.tsv.gz、barcodes.tsv.gz文件放在各自的文件夹下面。cd /home/gongyuqi/project/scRNA-seq/TNBC 看一下文件结构 12345#记得解压gz文件，不然Read10X()会报错find ./ -name &quot;*.gz&quot;|xargs gunzip#Read10X()中，文件的命名一定要是&quot;barcodes.tsv&quot; &quot;genes.tsv&quot;&quot;matrix.mtx&quot;#所以我们将features.tsv改成genes.tsvfind ./ -name &quot;features*&quot; | while read id; do mv $id ${id%/*}/genes.tsv; done 再次看一下现在的文件结构 step 2、合并数据1234567891011121314151617181920212223setwd(&quot;/home/gongyuqi/project/scRNA-seq/TNBC&quot;)#Read10函数要对目录进行操作，先提取目录名folders&lt;-list.files(&quot;./&quot;)folderslibrary(Seurat)scelist&lt;-lapply(folders, function(folder){ CreateSeuratObject(counts = Read10X(folder), project = folder)})#scelist只是8个10X对象的集合scelist#现在开始真正的合并TNBC_scRNA&lt;-merge(scelist[[1]], y=c(scelist[[2]],scelist[[3]],scelist[[4]],scelist[[5]], scelist[[6]],scelist[[7]],scelist[[8]]), add.cell.ids=folders, project=&quot;TNBC&quot;)table(TNBC_scRNA$orig.ident)GetAssayData(TNBC_scRNA)[1:20,1:25]#感受一下矩阵的内容，是个稀疏矩阵save(TNBC_scRNA,file = &quot;TNBC_scRNA.Rdata&quot;) Suerat下游分析step 1、过滤1234567891011121314151617#去除线粒体基因grep(pattern = &quot;^MT-&quot;,rownames(TNBC_scRNA),value = T)TNBC_scRNA[[&quot;percent.mt&quot;]] &lt;- PercentageFeatureSet(TNBC_scRNA, pattern = &quot;^MT-&quot;)head(TNBC_scRNA@meta.data,5)summary(TNBC_scRNA@meta.data$nCount_RNA)#可视化单细胞数据质量VlnPlot(TNBC_scRNA, features = c(&quot;nFeature_RNA&quot;, &quot;nCount_RNA&quot;, &quot;percent.mt&quot;), ncol = 3)data&lt;-TNBC_scRNA@meta.datalibrary(ggplot2)p&lt;-ggplot(data = data,aes(x=nFeature_RNA))+geom_density()p#根据上一步质控的结果进行参数的设定#同时结合原文的参数设置，我们这里只保留RNA含量再500到6000之间，且线粒体含量小于20%的细胞TNBC_scRNA &lt;- subset(TNBC_scRNA, subset = nFeature_RNA &gt; 500 &amp; nFeature_RNA &lt; 6000 &amp; percent.mt &lt; 20)TNBC_scRNAVlnPlot(TNBC_scRNA, features = c(&quot;nFeature_RNA&quot;, &quot;nCount_RNA&quot;, &quot;percent.mt&quot;), ncol = 3) step 2、标准化&amp;&amp;降维1234567891011121314151617181920212223242526272829#标准化TNBC_scRNA[[&quot;RNA&quot;]]@data[1:20,1:10]TNBC_scRNA &lt;- NormalizeData(TNBC_scRNA, normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000)#标准化后的值保存在TNBC_scRNA[[&quot;RNA&quot;]]@datanormalized.data&lt;-TNBC_scRNA[[&quot;RNA&quot;]]@datanormalized.data[1:20,1:10]#鉴定高变基因TNBC_scRNA &lt;- FindVariableFeatures(TNBC_scRNA, selection.method = &quot;vst&quot;, nfeatures = 2000)#全部基因进行归一化all.genes &lt;- rownames(TNBC_scRNA)TNBC_scRNA &lt;- ScaleData(TNBC_scRNA, features = all.genes)#PCA降维TNBC_scRNA &lt;- RunPCA(TNBC_scRNA, features = VariableFeatures(object = TNBC_scRNA))# Examine and visualize PCA results a few different waysprint(TNBC_scRNA[[&quot;pca&quot;]], dims = 1:5, nfeatures = 5)#visualizationVizDimLoadings(TNBC_scRNA, dims = 1:2, reduction = &quot;pca&quot;)DimPlot(TNBC_scRNA, reduction = &quot;pca&quot;)DimHeatmap(TNBC_scRNA, dims = 1, cells = 500, balanced = TRUE)DimHeatmap(TNBC_scRNA, dims = 1:20, cells = 500, balanced = TRUE)#PCA个数的确定TNBC_scRNA &lt;- JackStraw(TNBC_scRNA, num.replicate = 100)TNBC_scRNA &lt;- ScoreJackStraw(TNBC_scRNA, dims = 1:20)ScoreJackStraw(TNBC_scRNA, dims = 1:20)ElbowPlot(TNBC_scRNA)#可视化不同PCA个数对分群的影响 step 3、细胞聚类12345678910111213141516171819202122#细胞聚类#resolution = 0.1, cluster=11(结合UMAP和tSNE,11个亚群是不能再合并了，但还可以再细分)#resolution = 0.2, cluster=13(同resolution = 0.25的结果几乎一致)######################################################################结合bulk RNA-seq的差异基因在不同亚群的分布，13个亚群的单细胞数据比较合适#resolution = 0.25, cluster=13######################################################################resolution = 0.35, cluster=16(同resolution = 0.4的结果几乎一致)#resolution = 0.4, cluster=16#resolution = 0.45, cluster=18#resolution = 0.5, cluster=20#resolution = 0.6, cluster=21（和resolution = 0.5时差异不大，tSNE结果显示有过度分群的趋势）TNBC_scRNA &lt;- FindNeighbors(TNBC_scRNA, dims = 1:10)TNBC_scRNA &lt;- FindClusters(TNBC_scRNA, resolution = 0.25)#UMAP可视化细胞亚群分布TNBC_scRNA &lt;- RunUMAP(TNBC_scRNA, dims = 1:10)DimPlot(TNBC_scRNA, reduction = &quot;umap&quot;,label = T,label.size = 5)#tSNE可视化细胞亚群分布set.seed(1918)TNBC_scRNA &lt;- RunTSNE(TNBC_scRNA, dims = 1:10)DimPlot(TNBC_scRNA, reduction = &quot;tsne&quot;,label = T,label.size = 5) step 4、探寻亚群特异性的差异基因1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556siggene&lt;-read.csv(&quot;TNBC_upregulation_gene.csv&quot;)#RNA-TNBC分析得到的文件siggene&lt;-siggene$x#一共177个基因#写了循环可视化177个在bulk RNA-seq数据集中共同上调的差异基因，挑选亚群特异性差异基因siggene&lt;-read.csv(&quot;TNBC_upregulation_gene.csv&quot;)siggene&lt;-siggene$xplot_list = list()for (i in 1:6) { front=i*30-29 back=i*30 plot_list[[i]] = DotPlot(TNBC_scRNA, features = siggene[front:back]) + coord_flip() print(plot_list[[i]])}#############################################################################################上面的代码提示没有在scRNA-seq数据中找到KIAA0101,EPT1,HN1基因，于是找到这三个基因的所有alias#但是，scRNA-seq数据中仍然没有这三个基因~~~所以就不要这三个基因咯KIAA0101&lt;-c(&quot;L5&quot;, &quot;NS5ATP9&quot;, &quot;OEATC&quot;, &quot;OEATC-1&quot;, &quot;OEATC1&quot;, &quot;PAF&quot;, &quot;PAF15&quot;, &quot;p15(PAF)&quot;, &quot;p15/PAF&quot;, &quot;p15PAF&quot;)EPT1&lt;-c(&quot;SELI&quot;, &quot;SEPI&quot;)HN1&lt;-c(&quot;ARM2&quot;, &quot;HN1A&quot;)DotPlot(TNBC_scRNA, features = HN1) + coord_flip()##############################################################################################第一步初筛选定了下列基因#69 candidate genesselect_siggene&lt;-read.csv(&quot;first_filter.csv&quot;)select_siggene&lt;-select_siggene$candidate#有了初筛后的基因，我们再次写循环，这次在tSNE中看看这些基因的分布情况，确定最终的亚群特异性差异基因plot_list = list()for (i in 1:12) { front=i*6-5 back=i*6 plot_list[[i]]=FeaturePlot(TNBC_scRNA, features = select_siggene[front:back], reduction=&quot;tsne&quot;, cols = c(&quot;grey&quot;, &quot;red&quot;)) print(plot_list[[i]])}#可视化上述基因后，进一步选定下列基因为最终的亚群特异性基因last_select_gene&lt;-c(&quot;CEP55&quot;,&quot;PRC1&quot;,&quot;PBK&quot;,&quot;CXCL10&quot;,&quot;SULF1&quot;,&quot;BCL2A1&quot;, &quot;PLA2G7&quot;,&quot;CXCL9&quot;,&quot;LYZ&quot;,&quot;COL11A1&quot;,&quot;MMP9&quot;,&quot;TDO2&quot;, &quot;TNFSF13B&quot;,&quot;ADAMDEC1&quot;,&quot;MMP12&quot;,&quot;SLAMF8&quot;, &quot;MMP1&quot;,&quot;COL10A1&quot;,&quot;NCF2&quot;)plot_list = list()for (i in 1:4) { front=i*6-5 back=i*6 plot_list[[i]]=FeaturePlot(TNBC_scRNA, features = last_select_gene[front:back], reduction=&quot;tsne&quot;, cols = c(&quot;grey&quot;, &quot;red&quot;)) print(plot_list[[i]])}p &lt;- DotPlot(TNBC_scRNA, features = last_select_gene) + coord_flip()p 这里简单看一下不同亚群都是些什么细胞 下一步计划 1、细胞亚群的注释（下周完成）2、亚群特异性基因的预后意义（下周完成）3、如果有多个预后意义显著的基因，准备进行预后模型的构建 疑惑 1、上述亚群特异性基因主要集中在免疫细胞群里面？要怎么去解释这个现象呢2、我们拿到这样的亚群特异性基因，如何更好的给它上价值呢？这一点我比较困惑。","link":"/scRNA-TNBC/"},{"title":"scRNA-seq_数据复现","text":"… 单细胞转录组数据降维聚类分群图表复现初尝试1. 这次我们要复现的单细胞数据来自International Journal of Cancer 2020年发表的一篇单细胞相关的纯生信文章。该文章数据链接：GSE150321 2. 简单浏览文章，抓取作者针对单细胞数据分析给出的一些参数信息，如下 Step 0、加载需要的R包12345library(Seurat)library(dplyr)library(patchwork)library(mindr)library(Matrix) Step 1、数据准备123456789101112131415setwd(&quot;//home//data//gongyuqi//R&quot;)#文章测序了两个病人的样本，正文figure展示的是其中一个病人的单细胞测序结果。我们下面的分析使用正文中展示的样本#读入数据并将表达矩阵转成稀疏矩阵，减少数据对空间资源的消耗以及运算资源的消耗GSM4546857&lt;-read.csv(&quot;GSM4546857_LSCC01_DBEC_UMI.csv&quot;,comment.char = &quot;#&quot;)dim(GSM4546857)GSM4546857[1:4,1:4]rownames(GSM4546857)&lt;-GSM4546857$Cell_IndexGSM4546857&lt;-GSM4546857[,-1]GSM4546857&lt;-t(GSM4546857)object.size(GSM4546857)#2331231144 bytesGSM4546857_sparse&lt;-as(as.matrix(GSM4546857),&quot;dgCMatrix&quot;)GSM4546857_sparse[1:4,1:4]object.size(GSM4546857_sparse)#166367952 bytessave(GSM4546857_sparse,file = &quot;GSM4546857_sparse.Rdata&quot;)dim(GSM4546857_sparse) 在此，可以直观的感受一下两种矩阵的大小 Step 2、创建Seurat对象123456789101112#下面这段代码中，最重要的就是创建Seurat对象以及去除线粒体基因，其他都是对Seurat对象的可视化，其目的在于提高初学者对该对象的了解## =============== 创建Seurat对象tissu1 &lt;- CreateSeuratObject(counts = GSM4546857_sparse, project = &quot;LSCC&quot;, min.cells = 3, min.features = 200)tissu1## =============== 去除线粒体基因# The [[ operator can add columns to object metadata. This is a great place to stash QC stats#此次数据检测到大量线粒体基因grep(pattern = &quot;^MT\\\\.&quot;,rownames(tissu1),value = T)tissu1[[&quot;percent.mt&quot;]] &lt;- PercentageFeatureSet(tissu1, pattern = &quot;^MT\\\\.&quot;)head(tissu1@meta.data,5)summary(tissu1@meta.data$nCount_RNA) 看一看过滤前数据情况 12345678910111213# Visualize QC metrics as a violin plotVlnPlot(tissu1, features = c(&quot;nFeature_RNA&quot;, &quot;nCount_RNA&quot;, &quot;percent.mt&quot;), ncol = 3)# densitydata&lt;-tissu1@meta.datalibrary(ggplot2)p&lt;-ggplot(data = data,aes(x=nFeature_RNA))+geom_density()p# FeatureScatter is typically used to visualize feature-feature relationships, but can be used# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.plot1 &lt;- FeatureScatter(tissu1, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;percent.mt&quot;)plot2 &lt;- FeatureScatter(tissu1, feature1 = &quot;nCount_RNA&quot;, feature2 = &quot;nFeature_RNA&quot;)plot1 + plot2 这里我们主要展示数据过滤前nFeature_RNA, nCount_RNA, percent.mt的情况 12345#Seurat官网给出的质控标准中，percent.mt &lt; 5，但不同组织不同细胞中线粒体含量存在差异,要根据自己的具体情况进行调整，#但是！！！太夸张啦！太夸张啦！我们这里线粒体基因阈值设置在80%，最终得到的细胞数为9796个，如果设置成95%，得到的细胞数也小于原文中的10699。太夸张啦！为了复现文章，我们先定成80。tissu1 &lt;- subset(tissu1, subset = nFeature_RNA &gt; 200 &amp; nFeature_RNA &lt; 2500 &amp; percent.mt &lt; 80)tissu1VlnPlot(tissu1, features = c(&quot;nFeature_RNA&quot;, &quot;nCount_RNA&quot;, &quot;percent.mt&quot;), ncol = 3) 这里我们主要展示数据过滤后nFeature_RNA, nCount_RNA, percent.mt的情况 Step 3、标准化12345## =============== Normalizationtissu1 &lt;- NormalizeData(tissu1, normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000)#标准化后的值保存在tissu1[[&quot;RNA&quot;]]@datanormalized.data&lt;-tissu1[[&quot;RNA&quot;]]@datanormalized.data[1:10,1:4] Step 4、鉴定高变基因123456789101112131415## =============== Identification of highly variable features#Seurat官方默认找前2000高变基因，用于下游的PCA降维分析#文章中作者是找的前2500，我们这里也就找前2500个高变基因#高变基因：在一些细胞中高表达，在另一些细胞中低表达#变异指标：mean-variance relationshiptissu1 &lt;- FindVariableFeatures(tissu1, selection.method = &quot;vst&quot;, nfeatures = 2500)# Identify the 10 most highly variable genestop10 &lt;- head(VariableFeatures(tissu1), 10)top10# plot variable features with and without labelsplot1 &lt;- VariableFeaturePlot(tissu1)plot2 &lt;- LabelPoints(plot = plot1, points = top10, repel = TRUE)plot1 + plot2 Step 5、降维12345678910111213141516171819202122232425## =============== 1. Scaling the data#Seurat默认是做前2000个基因的归一化#如何有热图展示需求的小伙伴，可以做全部基因的归一化，方便后续分析，我们这里就全部基因进行归一化all.genes &lt;- rownames(tissu1)tissu1 &lt;- ScaleData(tissu1, features = all.genes)## =============== 2. PCA降维tissu1 &lt;- RunPCA(tissu1, features = VariableFeatures(object = tissu1))# Examine and visualize PCA results a few different waysprint(tissu1[[&quot;pca&quot;]], dims = 1:5, nfeatures = 5)#visualizationVizDimLoadings(tissu1, dims = 1:2, reduction = &quot;pca&quot;)DimPlot(tissu1, reduction = &quot;pca&quot;)DimHeatmap(tissu1, dims = 1, cells = 500, balanced = TRUE)DimHeatmap(tissu1, dims = 1:20, cells = 500, balanced = TRUE)## =============== 3. PC个数的确定#这个非常非常具有挑战性，针对这个问题有很多教程,我们这里不着重讨论，因为我们主要是图表复现# NOTE: This process can take a long time for big datasets, comment out for expediency. More# approximate techniques such as those implemented in ElbowPlot() can be used to reduce# computation timetissu1 &lt;- JackStraw(tissu1, num.replicate = 100)tissu1 &lt;- ScoreJackStraw(tissu1, dims = 1:20)JackStrawPlot(tissu1, dims = 1:20)ElbowPlot(tissu1) Step 6、细胞聚类1234567891011121314151617181920## =============== 细胞聚类tissu1 &lt;- FindNeighbors(tissu1, dims = 1:10)#为了和原文聚类结果尽量一致，设置resolution = 0.02，这个数值是试出来的，resolution值越大，聚类数越多tissu1 &lt;- FindClusters(tissu1, resolution = 0.02)# Look at cluster IDs of the first 5 cellshead(Idents(tissu1), 5)#查看每个类别多少个细胞head(tissu1@meta.data)table(tissu1@meta.data$seurat_clusters)# If you haven't installed UMAP, you can do so via reticulate::py_install(packages =# 'umap-learn')tissu1 &lt;- RunUMAP(tissu1, dims = 1:10)# note that you can set `label = TRUE` or use the LabelClusters function to help label# individual clustersDimPlot(tissu1, reduction = &quot;umap&quot;,label = T,label.size = 5)#也可以tsne的方式展示细胞分区，但是umap给出的空间分布信息是有意义的，所以更倾向与使用umap#tissu1 &lt;- RunTSNE(tissu1, dims = 1:10)#DimPlot(tissu1, reduction = &quot;tsne&quot;,label = T,label.size = 5) 聚类结果，一共5个群 Step 7、差异表达123456## =============== 差异表达及细胞类型鉴定# find markers for every cluster compared to all remaining cells, report only the positive onestissu1.markers &lt;- FindAllMarkers(tissu1, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)write.csv(tissu1.markers,file = &quot;tissu1.markers.csv&quot;)#展示每一个cluster中top2显著的markertissu1.markers %&gt;% group_by(cluster) %&gt;% top_n(n = 2, wt = avg_log2FC) 可视化特异性marker基因 1、 首先看一下文章中给出的marker基因 1markergene&lt;-c(&quot;KRT5&quot;,&quot;PTPRC&quot;,&quot;CLU&quot;,&quot;AQP1&quot;,&quot;COL3A1&quot;,&quot;COL1A2&quot;) 2、 根据上表将marker基因映射到亚群上，以此确定不同亚群的具体细胞类型 123#umap可视化#根据文章supplementary table 1中提供的亚群的marker gene信息，在此绘制表达图谱FeaturePlot(tissu1, features = c(&quot;KRT5&quot;, &quot;PTPRC&quot;, &quot;CLU&quot;, &quot;AQP1&quot;, &quot;COL3A1&quot;, &quot;COL1A2&quot;)) 1234567#小提琴图可视化library(ggplot2)plot_list&lt;-list()for (i in 1:5) { p=VlnPlot(tissu1,features = markergene[i])+NoLegend() print(p)} 123#dotplot图可视化DotPlot(tissu1, features = markergene)+RotatedAxis()+ scale_x_discrete(&quot;&quot;)+scale_y_discrete(&quot;&quot;) 1234567891011121314151617#细胞亚群占比统计cluster&lt;-c(&quot;Cancer cell&quot;,&quot;Immune cell&quot;,&quot;Epithelial cells&quot;,&quot;Endothelial cells&quot;,&quot;Fibroblasts&quot;)subset_cell&lt;-c()for (i in 1:5) { subset_cell=c(subset_cell,dim(tissu1[,Idents(tissu1)%in%c(cluster[i])])[2])}subset_account&lt;-c()total_cells&lt;-dim(tissu1)[2]for (i in 1:5) { subset_account=c(subset_account,subset_cell[i]/total_cells)}subset_account&lt;-subset_account*100per.cell &lt;- paste(cluster,&quot;: &quot;,ceiling(subset_account),&quot;%&quot;,sep = &quot;&quot;)pie(subset_account,labels=per.cell) 文章中的各亚群占比 我们的计算结果如下，结果显示我们的计算结果和原文结果类似 Step 8、细胞亚群命名12345678910## =============== 细胞类型鉴定new.cluster.ids &lt;- c(&quot;Cancer cell&quot;, #&quot;KRT5&quot; &quot;Immune cell&quot;, #&quot;PTPRC&quot; &quot;Epithelial cells&quot;, #&quot;CLU&quot; &quot;Endothelial cells&quot;, #&quot;AQP1&quot; &quot;Fibroblasts&quot;) #&quot;COL3A1&quot;, &quot;COL1A2&quot;names(new.cluster.ids) &lt;- c(0,1,2,5,4)tissu1 &lt;- RenameIdents(tissu1, new.cluster.ids)DimPlot(tissu1, reduction = &quot;umap&quot;, label = TRUE, pt.size = 0.5) + NoLegend() 我们的聚类结果 原文的聚类结果 Step 9、细胞亚群提取及再分群12345678910111213141516171819202122## =============== #肿瘤细胞的提取，下面两种方式都可以实现特定亚群的提取Cancer_cell&lt;-tissu1[,tissu1@meta.data$seurat_clusters%in%c(0)]#Cancer_cell&lt;-tissu1[,Idents(tissu1)%in%c(&quot;Cancer cell&quot;)]#标准化之前Cancer_cell &lt;- NormalizeData(Cancer_cell, normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000) Cancer_cell &lt;- FindVariableFeatures(Cancer_cell, selection.method = 'vst', nfeatures = 2000)# Identify the 10 most highly variable genestop10 &lt;- head(VariableFeatures(Cancer_cell), 10)top10Cancer_cell &lt;- ScaleData(Cancer_cell, vars.to.regress = &quot;percent.mt&quot;)Cancer_cell &lt;- RunPCA(Cancer_cell, features = VariableFeatures(object = Cancer_cell)) DimHeatmap(Cancer_cell, dims = 1:20, cells = 500, balanced = TRUE)Cancer_cell &lt;- FindNeighbors(Cancer_cell, dims = 1:10)Cancer_cell &lt;- FindClusters(Cancer_cell, resolution = 0.3)Cancer_cell &lt;- RunUMAP(Cancer_cell, dims = 1:10)DimPlot(Cancer_cell, reduction = 'umap',label = T)#cluster Keratinocyte CancerFeaturePlot(Cancer_cell, features = c(&quot;SPRR3&quot;,&quot;SPRR1A&quot;,&quot;NCCRP1&quot;,&quot;TMPRSS11E&quot;,&quot;APOBEC3A&quot;)) 绞尽脑汁，一顿操作，最后还是没有办法很好的复现原文的数据，但是其中一个亚群有较好的聚类出来。 总结 1、单细胞数据复现可以提升自己对此类数据的理解和掌握。2、不同人处理单细胞数据时设置的参数会有所不同，作者并没有在文章中给出太多相关信息，导致最终的结果有所不同。3、单细胞亚群个数的确定以及类群注释需要非常强的生物学背景。4、 要学的东西太多了，一点点的来，不求快，但求稳，加油💪","link":"/scRNA-seq-%E6%95%B0%E6%8D%AE%E5%A4%8D%E7%8E%B0/"},{"title":"scRNA-seq数据分析流程","text":"··· scRNA-seq数据分析流程以下分析流程以GSE111229数据为例(为什么用它呢，因为它数据量小呀，嘿嘿🤭)在进行分析之前，首先了解一下该套数据的基本情况。 由此可基本了解，此数据为单端测序数据，物种为小鼠。768个样本即768个单细胞测序数据，总大小才10G。这在单细胞测序数据中真的是非常小！非常罕见啦！ 😄数据分析上游(Linux)😄因为是单细胞转录组测序数据，和转录组测序的上游分析没什么区别。所以以下分析流程都是在conda构建的rna环境下进行，这个环境也是很久之前搭建好的，在此不再赘述。 1conda activat rna 运行上述代码后，可以看到提示符(prompt)左边多了一个(rna)环境，此环境里面，预先装好了处理RNA-seq数据所需要的相关软件。 数据下载 如果样本量不多，网速又不理想，推荐EBI-ENA数据库，此数据库下载网速还可以，另外该数据库除了提供SRA文件外，还提供FASTQ文件，省去了从NCBI下载SRA文件转为FASTQ文件的麻烦(如何单个文件数据量大，此格式转换步骤非常耗时)。如果样本量特别多，网速又比较理想，推荐使用Aspera、prfetch、wget、curl。前两者大大优于后两者。Aspera Connect的下载速度是最快了，此方法也可以下载FASTQ和SRA文件，免去了SRA转至FASTQ的过程，该过程很耗时，十分耗时。 进入EBI官网，输入SRP号123以其中一个样本SRR6791132为例，点击SRR6791132进入下载界面，鼠标右键红色箭头处——“复制链接地址” 进入linux进行数据下载12345678910111213141516171819202122232425262728mkdir -p scRNA-seq/fastq &amp;&amp; cd scRNA-seq/fastq wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR679/002/SRR6791132/SRR6791132.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR679/005/SRR6791135/SRR6791135.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR679/005/SRR6791135/SRR6791135.fastq.gz``` &lt;font color=blue&gt;注意注意，特别注意，要下载整套数据最好使用Aspera或者prefetch，本宝宝没有办法呀，网不好，空间也不大，linux中的数据操作只能用三个样本做演示了，熟悉一下分析流程。如果是整套数据，记得写小循环，放后台，就不用一直守着它了，最好是睡觉的时候跑，不耽误事。&lt;/font&gt; ### 质控 ```bash #首先查看数据质量情况，进行如下操作 mkdir -p /scRNA-seq/html &amp;&amp; cd /scRNA-seq/htmlls /scRNA-seq/fastq/*gz|xargs fastqc -t 4 -o ./``` 上述操作会得到html文件和zip文件 &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/quality.JPG&quot;/&gt;网页打开其中一个html文件可查看测序数据的质量，该数据整体质量还不错（[质控报告解读教程](https://www.jianshu.com/p/835fd925d6ee)） &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/html.JPG&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt; 大部分测序数据都会出现如下情况，不过这个问题不大 &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/pbsc.JPG&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt; ```bash #接下来过滤掉质量不好的数据，使trim_galore软件 #mkdir /scRNA-seq/clean#ls /scRNA-seq/fastq/*gz|while read id; do trim_galore -q 25 --phred33 --length 35 -e 0.1 --stringency 5 -o . $id;done #在此会发现过滤后的数据质量反而没有过滤前的质量好，有时候数据分析就是会这样，过滤参数很难调整。。。所以在此后续步骤使用未过滤的数据进行处理。 比对1、小鼠参考基因组首先下载小鼠参考基因组，hisat2或者bowtie2构建小鼠参考基因组索引。也可以直接下载参考基因组索引文件。另外还需要下载小鼠参考基因组注释文件。(此步骤很早前处理RNA-seq数据时就已经完成了，就不再赘述)hisat2构建的mm10参考基因组索引文件mm10注释文件 2、进行参考基因组比对 123456#同参考基因组索引进行比对，生成bam文件。参考基因组索引一定是引用前缀&quot;genome&quot;。另外注意，此处数据为单端测序数据！ls *.gz | while read id; do hisat2 -p 4 -x /addfirst/reference/mouse/index/hisat2-build-index/mm10/genome -U $id -S ${id%%.*}.hisat2.sam;done mkdir /scRNA-seq/align &amp;&amp; cd /scRNA-seq/align cp /scRNA-seq/fastq/*.sam /scRNA-seq/fastq/*.bam ./ #使用samtools软件将sam文件转成bam文件 ls *.sam | while read id; do samtools sort -O bam -@ 4 -o ${id%%.*}.hisat2.bam $id; done 比对结果如下：82.25%的比对率，比较的凑合了，一般情况最好90%以上，后两个数据的比对率实在不能看。真实处理环境一定要仔细调节过滤参数！！！ 可以比较一下bam、fasatq、sam文件的大小。sam文件远远大于bam和fastq文件。生信分析上游一定要对文件大小保持高度敏感，避免不必要的错误。另外，数据量大的话，可以在生成sam文件时通过|将结果进一步生成bam文件，这样可以大大节省存储空间。 构建index文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566ls *.bam | xargs -i samtools index {}``` &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/bamindex.JPG&quot;/&gt;### 生成reads表达矩阵 ```bash gtf=/addfirst/reference/mouse/gencode.vM23.annotation.gtf featureCounts -T 4 -p -t exon -g gene_id -a $gtf -o all.id.txt *.bam 1&gt;counts.id.log 2&gt;&amp;1 查看counts矩阵 less -S all.id.txt``` &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/reads.JPG&quot;/&gt; 查看counts矩阵会发现绝大多数基因表达量都是0，因为对于大多数细胞来说，大多数基因都是测不到的，这个属于正常现象。在此，我们只用了3个样本做演示，总数据则有768个样本。 &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/countsmatrix.JPG&quot;/&gt; 理论上按照上面的分析流程生成的表达矩阵数据和下图中NCBI官网上的rawCounts.txt.gz的内容应该是差不多的，只是所用处理软件不同会有一点点区别。 &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/NCBImatrix.JPG&quot;/&gt;至此，scRNA-seq上游分析结束，这里比较重要的是**数据质量**，数据质量直接决定比对的效率，影响最终生成的表达矩阵的可靠性。所以质控步骤要严格把控，送样测序也要选靠谱的公司。 ## 😄数据分析下游(R)😄 ### 数据(data.frame)的构建如果是使用rawCounts数据进行下游分析，首先要对数据进行标准化，然后再做后续分析。该篇文章是使用rpkm的数据进行下游分析，这里也直接使用rpkm的数据进行下游分析。 ```R a=read.table('../GSE111229_Mammary_Tumor_fibroblasts_768samples_rpkmNormalized.txt.gz',header = T ,sep = '\\t') ##把表达矩阵文件载入R，header=T :保留文件头部信息，seq='\\t'以tap为分隔符# 记得检测数据a[1:6,1:4] #对于a矩阵取第1~6行，第1~4列## 读取RNA-seq的counts定量结果，表达矩阵需要进行简单的过滤dat=a[apply(a,1, function(x) sum(x&gt;0) &gt; floor(ncol(a)/50)),] #筛选表达量合格的行,列数不变 #上面的apply()指令代表对矩阵a进行行计算，判断每行表达量&gt;1的样本总个数，并筛选出细胞表达量合格的基因（行）#第一个参数是指要参与计算的矩阵——a#第二个参数是指按行计算还是按列计算，1——表示按行计算，2——按列计算；#第三个参数是指具体的运算参数,定义一个函数x（即表达量为x）dat[1:4,1:4] ``` &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/a.JPG&quot;/&gt; &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/dat.JPG&quot;/&gt; ```R #层次聚类hc=hclust(dist(t(log(dat+0.1)))) ##样本间层次聚类# 如果是基因聚类，可以选择 wgcna 等算法 plot(hc,labels = F) ##此图可以看出678个样本大致可以分为几类，在此大致可分为4类 clus = cutree(hc, 4) #对hclust()函数的聚类结果进行剪枝，即选择输出指定类别数的系谱聚类结果。group_list= as.factor(clus) ##转换为因子属性table(group_list) ##统计频数 ``` &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/hclust.JPG&quot; width=&quot;60%&quot; height=&quot;60%&quot;&gt; &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/grouplist.JPG&quot;/&gt;```R #提取批次信息colnames(dat) #取列名library(stringr)plate=str_split(colnames(dat),'_',simplify = T)[,3] #str_split()函数可以分割字符串。取列名，以'_'号分割，提取第三列。table(plate) #以为这768个样本是放在两个384孔检测的，后续要检测测序数据是否存在批次效应，所以要预先提取批次信息 n_g = apply(a,2,function(x) sum(x&gt;0)) #统计每个样本有表达的基因有多少行#reads数量大于1的那些基因为有表达，一般来说单细胞转录组过半数的基因是不会表达的。 df=data.frame(g=group_list,plate=plate,n_g=n_g) #新建数据框(细胞的属性信息) ##(样本为行名，列分别为：样本分类信息，样本批次，样本表达的基因数——不是表达量的和，而是种类数或者说个数) df$all='all' #添加列，列名为&quot;all&quot;，没事意思，就是后面有需要metadata=df df[1:4,1:4]save(dat,metadata,file = '../input_rpkm.Rdata') #保存a,dat,df这变量到上级目录的input.Rdata 检测是否存在批次效应因为该篇文章使用两块384孔进行测序，所以要先确定两块板是否存在批次效应，如果存在，则两块板的样本不能合并分析，如果不存在批次效应，则两块板可以进行批次分析。 PCA主成份分析 1234567891011121314151617181920212223242526272829303132333435363738394041424344rm(list = ls()) ## 魔幻操作，一键清空~options(stringsAsFactors = F)load(file = '../input_rpkm.Rdata')dat[1:4,1:4]head(metadata) plate=metadata$plate## 下面是画PCA的必须操作dat_back=datdat=t(dat) #做PCA之前一定要进行数据转置dat=as.data.frame(dat)dat=cbind(dat,plate ) #cbind根据列进行合并，即叠加所有列，矩阵添加批次信息dat[1:4,12688:12690] #检测是否添加上了批次信息table(dat$plate)library(&quot;FactoMineR&quot;)library(&quot;factoextra&quot;) dat.pca &lt;- PCA(dat[,-ncol(dat)], graph = FALSE)fviz_pca_ind(dat.pca,#repel =T, geom.ind = &quot;point&quot;, col.ind = dat$plate, # color by groups addEllipses = FALSE, # Concentration ellipses legend.title = &quot;Groups&quot;) ``` &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/PCA.jpeg&quot; width=&quot;60%&quot; height=&quot;60%&quot;&gt; **tSNE分析** ```R rm(list = ls()) ## 魔幻操作，一键清空~options(stringsAsFactors = F) library(Rtsne) load(file = '../input_rpkm.Rdata')dat[1:4,1:4]dat_matrix &lt;- t(dat) dat_matrix=log2(dat_matrix+0.01)set.seed(42) # 如果想得到可重复的结果，就种一颗种子吧😄tsne_out &lt;- Rtsne(dat_matrix,pca=FALSE,perplexity=30,theta=0.0) ##添加颜色tsnes=tsne_out$Ytsnes=as.data.frame(tsnes)group=c(rep('plate1',384),rep('plate2',384))colnames(tsnes)&lt;-c(&quot;tSNE1&quot;,&quot;tSNE2&quot;) library(ggfortify)ggplot(tsnes, aes(x = tSNE1, y = tSNE2))+ geom_point(aes(col=group))+ theme_classic() 细胞亚群######################### ##针对所有细胞做PCA分析## ######################### rm(list = ls()) ## 魔幻操作，一键清空 options(stringsAsFactors = F) load(file = '../input_rpkm.Rdata') dat[1:4,1:4] head(metadata) group&lt;-metadata$g #做好数据备份 dat_back=dat dat=t(dat) #记得转置数据 dat=as.data.frame(dat) dat=cbind(dat,group ) #cbind根据列进行合并，即叠加所有列，为矩阵添加分组信息 dat[1:4,12688:12690] table(dat$group) library(&quot;FactoMineR&quot;) library(&quot;factoextra&quot;) dat.pca &lt;- PCA(dat[,-ncol(dat)], graph = FALSE) fviz_pca_ind(dat.pca,#repel =T, geom.ind = &quot;point&quot;, # show points only (nbut not &quot;text&quot;) col.ind = dat$group, # color by groups addEllipses = FALSE, # Concentration ellipses legend.title = &quot;Groups&quot; ) ######################### ##针对所有细胞做tSNE分析## ######################### rm(list = ls()) ## 魔幻操作，一键清空~ options(stringsAsFactors = F) library(Rtsne) load(file = '../input_rpkm.Rdata') dat[1:4,1:4] dat_matrix &lt;- t(dat) dat_matrix=log2(dat_matrix+0.01) # Set a seed if you want reproducible results set.seed(42) tsne_out &lt;- Rtsne(dat_matrix,pca=FALSE,perplexity=30,theta=0.0) group=metadata$g ##添加颜色 tsnes=tsne_out$Y tsnes=as.data.frame(tsnes) colnames(tsnes)&lt;-c(&quot;tSNE1&quot;,&quot;tSNE2&quot;) ggplot(tsnes, aes(x = tSNE1, y = tSNE2))+ geom_point(aes(col=group))+ theme_classic() 单细胞测序分析旨在分出不同细胞亚群(展现细胞间的异质性)，力求寻找某类亚群与某特定生物学功能之间的联系。而要探讨细胞分群，就必须要知道PCA和tSNE。PCA(Principal Component Analysis)tSNE(t-Distributed Stochastic Neighbor Embedding)什么样的算法才是最理想的能够将细胞分群的算法呢？1）局部结构：属于同一个亚群的细胞，聚类尽可能近2）全局结构：属于不同亚群的细胞，聚类尽可能分来 PCA的方法侧重于去抓样本中隐含的主要效应，从而让差异大的样本在图上呈现较远的距离。常规RNA-seq项目中，处理效应、批次效应、离群效应等属于较大的效应，PCA的方法可以良好的去展示这些效应。 如果影响样本分组的不是主要效应，而是一些更小的效应，PCA则无法对样本进行准确的区分。scRNA-seq的可视化主要期望对各个细胞亚群有良好的区分。每次检测的上万甚至几十万个细胞中，几乎肯定可以区分出几十中细胞亚群，包括一些稀有的细胞。而区分这些细胞亚群（尤其是稀有细胞类型）的效应，往往不是主要效应（即大量基因的差异），而是一些微小效应（少量标记基因差异）。如果使用PCA进行分析的话，就会掩盖掉某些微小的但是有可能非常重要的细胞群。 tSNE算法就属于可以同时兼顾局部结构和全局结构的非线性降维可视化算法。tSNE不同于PCA（PCA主要目标是尽量去抓取群体中的主要效益），tSNE算法的主要目标是：降维后的数据依然保持最为相似的紧密成簇。这就保证了哪怕某些稀有细胞只是少量基因区别于其他细胞亚群，在tSNE中依然可以与其他细胞有良好的区分。 结论：PCA是常规转录组常用的数据降维和样本关系可视化的方法，但针对群体单细胞转录组数据，tSNE是明显胜过PCA的方法！ 参考资料http://www.sohu.com/a/320164491_278730","link":"/scRNA-seq/"},{"title":"scRNA-seq数据分析学习（一）","text":"用常规转录组的知识对scRNA-seq数据进行初步探索 scRNA-seq数据分析学习（一）用常规转录组的知识对scRNA-seq数据进行初步探索 bulk RNA-seq和scRNA-seq的区别http://www.bio-info-trainee.com/2548.html 样本量，基因数量导致统计学环境的变化通常情况下样本量都是成千上万的，检测的基因数量相对常规转录组较少 计算量迫使算法需要优化成千上万甚至几十万的样本量对计算要求非常高 待解决的生物问题有所不同 不仅仅是不同状态的不同，一个样本就是一个因素 细胞、组织间的异质性问题 探究变化的过程追踪和重现 相对常规转录组而言，scRNA-seq分析流程软件工具的成熟度有待提高，目前还没有对整个分析流程的金标准 单细胞差异基因表达统计学方法 归一化 聚类分群 重要基因挑选 差异基因 marker基因 变异基因 安装并加载scRNAseq这个R包R包scRNAseq内含数据集，下载安装加载相应的辅助R包来探索scRNAseq的内置数据集，对单细胞转录组分析进行初步探索（在此之前已经下载好了所需的R包）下载R包 123456789101112131415#如果没有所需R包，可以用以下方法进行下载(用两个R包为例)options()$repos #查看使用install.packages安装的默认镜像options(&quot;repos&quot;=c(CRAN=&quot;https://mirrors.tuna.tsinghua.edu.cn/CRAN/&quot;))#指定install.packages安装镜像为清华镜像options()$reposoptions()$BioC_mirror#查看使用bioconductor的默认镜像options(BioC_mirror=&quot;https://mirrors.ustc.edu.cn/bioc/&quot;)#指定Bioconductor安装默认镜像为中科大镜像options()$BioC_mirror(!requireNamespace(&quot;Rtsne&quot;))#查看是否存在Rtsne这个包install.packages(&quot;Rtsne&quot;)#如果Rtsne包不存在，就下载这个R包#CRAN是R默认使用的R包仓库，install.packages()只能用于安装发布在CRAN上的包#Bioconductor是基因组数据分析相关的软件仓库包(!requireNamespace(&quot;BiocManager&quot;))install.packages(&quot;BiocManager&quot;)#install.packages()是安装Bioconductor软件包的命令(!requireNamespace(&quot;scRNAseq&quot;))BiocManager::install(&quot;scRNAseq&quot;) 加载R包 1234567891011#suppressMessages(library(Rtsne)) suppressMessages()函数是加载R包的时候不显示说明信息library(Rtsne)library(FactoMineR)library(factoextra)library(scater)library(scRNAseq)library(M3Drop)library(ROCR)library(tidyr)library(cowplot)library(ggplot2) scRNA-seq包中的数据集这个内置的是Pollen et al.2014年数据集，人类单细胞数据，分为4类：NPC、GW16、GW21、GW21+3。此R包只提供了4种细胞类型，完整的数据是23730 features，301 samples。这里面的表达矩阵是有RSEM(Li and Dewey 2011)软件根据hg38 RefSeq transcription得到的，总共130个文库。每个细胞测了2次。测序深度不一样。 载入R包scRNA-seq 12345library(scRNAseq)data(fluidigm)ct&lt;-floor(assays(fluidigm)$rsem_counts)#assays函数拿到表达矩阵，rsem_counts有小数，所以floorct[1:4,1:4]dim(ct) #[1] 26255 130 接下来的代码初步探索scRNA-seq内置数据集的各个属性 12345678sample_ann&lt;-as.data.frame(colData(fluidigm))#得到130个样本的28个临床信息#利用lapply针对sample_ann中的前19个属性（数值型的）进行批量处理，一次性展示box&lt;-lapply(colnames(sample_ann[,1:19]), function(i){ dat&lt;-sample_ann[,i,drop=F] dat$sample=rownames(dat) ggplot(dat,aes('all cells',get(i))) +geom_boxplot() +xlab(NULL) +ylab(i) +theme_classic()})plot_grid(plotlist = box,ncol = 5) 123456##基因表达情况的初步探索counts&lt;-cttable((apply(counts, 1, function(x) sum(x&gt;0))))#查看基因的详细表达情况table((apply(counts, 1, function(x) sum(x&gt;0)&gt;0)))#查看至少有一个样本表达某个基因的总体情况boxplot(apply(counts, 1, function(x) sum(x&gt;0)))hist(apply(counts,2,function(x) sum(x&gt;0)))#可视化每个样本的基因表达情况 过滤（基因+细胞） 1234567891011121314151617181920212223242526272829#过滤不满足某个条件的基因#choosed_genes&lt;-apply(counts, 1, function(x) sum(x&gt;0))&gt;0#等价于choosed_genes&lt;-apply(counts, 1, function(x) sum(x&gt;0)&gt;0)#table(choosed_genes)#结果显示#choosed_genes#FALSE TRUE #9159 17096#counts&lt;-counts[choosed_genes,]#过滤掉了在所有样本中都不表达的基因 #同理也可以过滤掉不满足某个条件的细胞，apply函数中的1改为2，在设定特定的条件即可 #在此，我们根据上述样本的boxplot图中展示的各个属性来进行简单过滤#过滤没有统一的标准，根据自己对数据的要求，细节会很多，据不同情况而定#这里进行简单的批量处理，不考虑太多filter&lt;-colnames(sample_ann[,c(1:9,11:16,18,19)])tf&lt;-lapply(filter,function(i){ #i=filter[1] dat&lt;-sample_ann[,i] dat&lt;-abs(log10(dat)) fivenum(dat) (up&lt;-mean(dat)+2*sd(dat)) (down&lt;-mean(dat)-2*sd(dat)) valid&lt;-ifelse(dat&gt;down &amp; dat&lt;up,1,0)})tf&lt;-do.call(cbind,tf)choosed_cell&lt;-apply(tf, 1, function(x) all(x==1))table(sample_ann$Biological_Condition)sample_ann=sample_ann[choosed_cell,]#rownames(sample_ann)中的样本和rownames(tf)数字一一对应table(sample_ann$Biological_Condition)ct&lt;-ct[,choosed_cell]#colnames(ct)和rownames(tf)的数字一一对应dim(ct)#26255个基因，99个样本 探索基因的表达情况12345ct[1:4,1:4]counts&lt;-cttable(apply(counts, 1, function(x) sum(x&gt;0)))fivenum(apply(counts, 1, function(x) sum(x&gt;0)))boxplot(apply(counts, 1, function(x) sum(x&gt;0))) 123#针对基因fivenum(apply(counts, 2, function(x) sum(x&gt;0)))hist(apply(counts, 2, function(x) sum(x&gt;0))) R语言中fivenum函数介绍：https://blog.csdn.net/mr_muli/article/details/79616124R语言中do.call函数介绍：https://blog.csdn.net/zdx1996/article/details/87899029R语言all函数介绍：https://blog.csdn.net/scong123/article/details/70184038 利用常规转录组分析知识查看细胞间所有基因表达量的相关性123456##下面的计算都是基于log后的表达矩阵dat&lt;-log2(edgeR::cpm(counts)+1)dat[1:4,1:4]dat_back&lt;-dat#备份表达矩阵exprSet&lt;-dat_backcolnames(exprSet) 相关性可视化 1pheatmap::pheatmap(cor(exprSet))#查看矩阵中个样本间的相关性 12345#加上了分组信息注释,组内样本相似性应该高于组间样本相似性group_list&lt;-sample_ann$Biological_Conditiontmp&lt;-data.frame(group=group_list)rownames(tmp)&lt;-rownames(sample_ann)pheatmap::pheatmap(cor(exprSet),annotation_col = tmp)#加上了分组信息注释 12345#去除掉小于5个样本表达的基因dim(exprSet)exprSet=exprSet[apply(exprSet, 1, function(x) sum(x&gt;1)&gt;5),]dim(exprSet)pheatmap::pheatmap(cor(exprSet),annotation_col = tmp) 12345678#选取前500个差异最大的基因exprSet&lt;-exprSet[names(sort(apply(exprSet, 1, mad),decreasing = T)[1:500]),]dim(exprSet)exprSet[1:4,1:4]M&lt;-cor(log2(exprSet+1))tmp&lt;-data.frame(group=group_list)rownames(tmp)&lt;-colnames(M)pheatmap::pheatmap(M,annotation_col = tmp) 有热图分析可知，从细胞的相关性角度来看，NPC跟另外的GW细胞群可以区分得很好，但是GW本身得3个小群并没有那么好得区分度 简单的选取top的sd的基因来计算相关性，并没有明显的改善 首先对表达矩阵进行简单的层次聚类12hc&lt;-hclust(dist(t(dat))) #次dat是上述经过过滤的，剩下99个细胞样本的数据plot(hc,labels = FALSE) 1234clus&lt;-cutree(hc,4)#对hclust()函数的聚类结果进行剪枝，即选择输出指定类别的系谱聚类结果group_list&lt;-as.factor(clus)#转换为因子table(group_list)#统计频数table(group_list,sample_ann$Biological_Condition) 可以看到如果是普通的层次聚类的话，GW16、GW21、GW21+3是很难区分开的 看看常规PCA的结果12345678910dat&lt;-dat_backdat&lt;-t(dat)dat&lt;-as.data.frame(dat)anno&lt;-sample_ann$Biological_Conditiondat&lt;-cbind(dat,anno)dat[1:4,26254:26256]table(dat$anno)dat.pca&lt;-PCA(dat[,-ncol(dat)],graph = FALSE)#PCA分析之前先去掉变量即新增加的分组信息annohead(dat.pca$var$coord)#每个组成份的基因重要性占比head(dat.pca$ind$coord)#每个细胞的前5个主成分取值 12345678fviz_pca_ind( dat.pca, geom.ind = &quot;point&quot;, col.ind = dat$anno, #color by groups addEllipses = TRUE, legend.title=&quot;groups&quot;) **同样的，NPC和其他类型细胞区分得很好，但GW本身得3个小群体并没有很好得区分度** tSNE降维结果12345678##tSNE降维#dat_matrix&lt;-dat_back 原始数据会比较耗时，结果就不展示了dat_matrix&lt;-dat.pca$ind$coordlibrary(Rtsne)set.seed(42)tsne_out&lt;-Rtsne(dat_matrix,perplexity = 10,check_duplicates = FALSE)plate&lt;-sample_ann$Biological_Conditionplot(tsne_out$Y,col=rainbow(4)[as.numeric(as.factor(plate))]) 参考资料生信菜鸟团：http://www.bio-info-trainee.com/生信技能树：http://www.biotrainee.com/R包的下载方法：https://www.jianshu.com/p/8e0dece51757","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"scRNA-seq数据分析学习（三）","text":"学习R包seurat对scRNA-seq数据进行分析 scRNA-seq数据分析（三）😊seurat不是一个R包，seurat是一个优秀的R包😊seurat不是提供服务的，seurat是提供一条龙服务的 seurat版本有2.×.×和3.×.×。同一般的R包升级不太一样的是：2.×和3.×之间区别还是蛮多的，各种函数也都有变化，虽然升级带来了更多的优点，但是函数名称的变化就会给学习者带来不小的麻烦呀！ 这里先学习2.×。为什么呢？因为我还不会3.×呀😵😏😜 载入R包 123rm(list = ls())#clear the environmentoptions(warn=-1)#turn off warning message globallylibrary(Seurat) 同样使用scRNAseq内置数据集 123456library(scRNAseq)data(fluidigm)#加载测试数据assay(fluidigm)&lt;-assays(fluidigm)$rsem_countsct&lt;-floor(assays(fluidigm)$rsem_counts)ct[1:4,1:4]counts&lt;-ct 创建Seurat要求的对象123names(metadata(fluidigm))meta&lt;-as.data.frame(colData(fluidigm))identical(rownames(meta),colnames(counts))#检测meta和counts这两个对象，后面有需要 123456seu&lt;-CreateSeuratObject(raw.data = counts, meta.data = meta, min.cells = 3, min.genes = 200, project = &quot;seu&quot;)seu 增加相关属性信息 12345#增加线粒体基因信息，如果线粒体所占基因比例过高，意味着可能是死细胞mito.gene&lt;-grep(pattern = &quot;^MT-&quot;,x=rownames(x=seu@data),value = TRUE)#但是我们知道这个数据集里面并没有线粒体基因percent.mito&lt;-Matrix::colSums(seu@raw.data[mito.gene,])/Matrix::colSums(seu@raw.data)seu&lt;-AddMetaData(object = seu,metadata = percent.mito,col.name = &quot;percent.mito&quot;)#加入了线粒体基因的信息#这里也可以加入ERCC等其他属性 可视化（初步）12#可视化，meta信息里面存在分组变量，可以指定分组，scRNAseq数据集里的分组信息是&quot;Biological_Condition&quot;VlnPlot(object = seu,features.plot = c(&quot;nGene&quot;,&quot;nUMI&quot;,&quot;percent.mito&quot;),group.by = 'Biological_Condition',nCol = 3) 12#查看一下某两个属性间的相关性GenePlot(object=seu,gene1=&quot;nUMI&quot;,gene2=&quot;nGene&quot;) 123#查看基因间的相关性tail(sort(Matrix::rowSums(seu@raw.data)))GenePlot(object = seu,gene1 = &quot;SOX11&quot;,gene2 = &quot;EEF1A1&quot;) 12#细胞间的相关性，更好的方式是用cor()+heatmap展示CellPlot(seu,seu@cell.names[3],seu@cell.names[5],do.identify=FALSE) 表达矩阵归一化只有进行归一化后，样本之间的比较才更能说明问题 123456identical(seu@raw.data,seu@data)seu&lt;-NormalizeData(object = seu,normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000,display.progress = F)#将每个细胞中总UMI设定为10000，计算方法为loge（每个细胞中基因的nUMI/该细胞内总UMI*10000+1）#经过归一化后，seu对象里面的data被改变了identical(seu@raw.data,seu@data) 寻找波动比较明显的基因，后续使用这些差异基因进行分析，主要为了降低计算量123456seu&lt;-FindVariableGenes(object = seu,mean.function = ExpMean,dispersion.function = LogVMR, x.low.cutoff = 0.0125, y.high.cutoff = 3, y.cutoff = 0.5)#选择不同的阈值，得到的基因取决于实际情况length(seu@var.genes) 对归一化后的矩阵进行分群 对矩阵进行回归建模，以及scale center=T：每个细胞中基因表达量-该基因在所有细胞中的表达量 scale=T：每个细胞中基因中心化后的表达值/该基因所在所有细胞中表达值的标准差 注意：执行ScaleData之前需要先执行NormalizeData 1seu&lt;-ScaleData(object = seu,vars.to.regress = c(&quot;nUMI&quot;),display.progress = F)#在这里只去除了文库大小的影响 PCA降维1234567#采用上述的差异基因进行降维seu&lt;-RunPCA(object = seu, pc.genes = seu@var.genes, do.print = 1:5, genes.print = 5)seu@dr#这样就能拿到PC的基因的重要性占比情况 1tmp&lt;-seu@dr$pca@gene.loadings 12#看一下前两个主成分的情况VizPCA(seu,pcs.use = 1:2) 1PCAPlot(seu,dim.1=1,dim.2=2,group.by='Biological_Condition') 12345#热图查看PC1的情况PCHeatmap(object = seu,pc.use = 1,cells.use = ncol(seu@data),do.balanced = TRUE,label.columns = FALSE)#一次性展示前10个主成分在各样本间的体现情况#PCHeatmap(object = seu,pc.use = 1:10,cells.use = ncol(seu@data),do.balanced = TRUE,label.columns = FALSE) 基于PCA情况看看细胞如何分群重点：需要搞清楚resolution参数 1234567seu&lt;-FindClusters(object = seu, reduction.type = &quot;pca&quot;, dims.use = 1:10,force.recalc = T, resolution = 0.9,print.output = 0, save.SNN = TRUE)PrintFindClustersParams(seu)table(seu@meta.data$res.0.9) resolution的值调得不一样，最后table出来得细胞亚群数也会不一样 细胞分群后的tSNE图12345seu&lt;-RunTSNE(object = seu, dims.use = 1:10, do.fast=TRUE, perplexity=10)TSNEPlot(object = seu)#由图看出，tSNE分出了3群，为什么呢？因为上面PCA降维出3各亚群呀！ 同样是PCA降维算法得到得3各细胞亚群，tSNE明显展示出更好的试图效果 1234567table(meta$Biological_Condition)table(meta$Biological_Condition,seu@meta.data$res.0.9)#这里就有一个问题了：这里明明在探讨tSNR降维，为什么这里又用到了上面pca降维中所生成的参数res.0.9呢？#有一个办法，就是先不进行pca降维，tSNE降维看看是否依然得到当前的数据#问题来了，我验证了上一条猜想，结果呢：Error in GetDimReduction(object = object, reduction.type = reduction.use, : #pca dimensional reduction has not been computed#所以呢，在这里的tSNE降维居然要用到PCA降维，嗯......表示不是很理解呀！两者不是独立的吗！？！？ 对每个亚群寻找marker基因下面的代码需要适时修改，因为每次分组都不一样，本次是3组，因为pca降维成3组以第一群细胞为例ident.1 = 1 12345678markers_df&lt;-FindMarkers(object = seu,ident.1 = 1,min.pct = 0.25)print(x=head(markers_df))markers_genes=rownames(head(x=markers_df,n=5))vlnPlot(object=seu,features.plot=markers_genes,use.raw=TRUE,y.log=TRUE)FeaturePlot(object = seu, features.plot = markers_genes, cols.use = c(&quot;grey&quot;,&quot;blue&quot;), reduction.use = &quot;tsne&quot;) 展示各个分类的marker基因的表达情况12seu.markers&lt;-FindAllMarkers(object = seu,only.pos = TRUE,min.pct = 0.25,thresh.use=0.25)DT::datatable(seu.markers)#这个操作有点优秀呀，可以在Rstudio里面试一试 热图展示各个亚群的marker基因1234567library(dplyr)seu.markers%&gt;%group_by(cluster)%&gt;%top_n(2,avg_logFC)#每个亚群挑10个marker基因进行展示top10&lt;-seu.markers%&gt;%group_by(cluster)%&gt;%top_n(10,avg_logFC)DoHeatmap(object = seu,genes.use = top10$gene,slim.col.label = TRUE,remove.key = TRUE)#FeaturePlot批量产图#FeaturePlot(object = seu,features.plot = top10$gene,cols.use = c(&quot;grey&quot;,&quot;blue&quot;),reduction.use = &quot;tsne&quot;)","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"scRNA-seq数据分析学习（二）","text":"学习R包scater对scRNA-seq数据进行分析 scRNA-seq数据分析（二）12rm(list = ls())#clear the environmentoptions(warn=-1)#turn off warning message globally 认识scater创建scater要求的对象123456789101112library(scRNAseq)data(fluidigm)assay(fluidigm)&lt;-assays(fluidigm)$rsem_countsct&lt;-floor(assays(fluidigm)$rsem_counts)ct[1:4,1:4]table(rownames(ct)==0)sample_ann&lt;-as.data.frame(colData(fluidigm))#数据集的临床信息#这里需要将表达矩阵做成我们的scater要求的对象sce&lt;-SingleCellExperiment( assays=list(counts=ct), colData=sample_ann) 过滤基因层面的过滤 使用calculateQCMetrics函数作用于sce这个单细胞数据对象后，就可以用rowData(object)查看各个基因各项统计指标： mean_counts：平均表达量 log10_mean_counts：归一化 log10-scale pct_dropout_by_counts：该基因丢失率 n_cell_by_counts：多少个细胞表达了该基因 上面的指标可以用来过滤，也可以自己计算这些统计学指标主要是过滤掉低表达量的基因，还有线粒体基因和ERCC spike-ins的控制 123456789exprs(sce)&lt;-log2(calculateCPM(sce)+1)genes&lt;-rownames(rowData(sce))#rowData(object)基因相关统计情况genes[grepl('^MT-',genes)]genes[grepl('^ERCC-',genes)]#比较不幸，这个测试数据里面没有线粒体基因，也没有ERCC序列sce&lt;-calculateQCMetrics(sce, feature_controls = list(ERCC=grep('^ERCC',genes)))#后面的分析都是基于sce这个变量，这个是一个SingleCellExperiment对象#sce是一个list(S4),但这个list可以当成data.frame来使用，非常方便 1234#查看信息tmp&lt;-as.data.frame(rowData(sce))colnames(tmp)head(tmp) 123456#目前只过滤掉那些在所有细胞都没有表达的基因#这个过滤条件可以自行调整，可以看到基因数量大幅减少keep_feature&lt;-rowSums(exprs(sce)&gt;0)&gt;0table(keep_feature)sce&lt;-sce[keep_feature,]sce 细胞层面的过滤用colData(object)可以查看各个样本统计情况 1234567891011tmp&lt;-as.data.frame(colData(sce)) colnames(tmp)#尝试查看一些信息tf&lt;-sce$total_features_by_countsboxplot(tf)fivenum(tf)#还是那句话，过滤没有统一的标准，视具体情况而定，下面举一个例子进行简单的过滤尝试tmp$pct_counts_in_top_100_features_endogenous&lt;50table(tmp$pct_counts_in_top_100_features_endogenous&lt;50)sce&lt;-sce[,tmp$pct_counts_in_top_100_features_endogenous&lt;50]sce 这个sce真的是非常方便了，虽然是个list,但是却可以像dataframe一样对其进行处理，至此，我们过滤掉了9000多个基因，4个样本 数据可视化1234#展示一些基因在不同细胞分类的表达，这里只做一下演示，一般不这么用，不过筛出来的marker基因可以用这种方式展现其在各组间的差异plotExpression(sce,rownames(sce)[1:6], x=&quot;Biological_Condition&quot;, exprs_values = &quot;logcounts&quot;) PCA 123456#还可以可视化细胞距离分布sce&lt;-runPCA(sce)#这里没有进行任何基因的挑选，就直接进行PCA了，与seurat包不一样reducedDimNames(sce)#PCA分布图上添加临床信息，同样发现不同GW细胞之间不能很好的分群#plotPCA(sce)该数据最原始的PCA图plotReducedDim(sce,use_dimred = &quot;PCA&quot;,colour_by = &quot;Biological_Condition&quot;) 12#PCA分布图上面添加表达量信息，一般标marker gene，次方法不常用，这里就以一个基因为例示范一下plotReducedDim(sce,use_dimred = &quot;PCA&quot;,colour_by = rownames(sce)[1],size_by = rownames(sce)[1]) 123##仅仅是选取前20个PC，分群时细胞分布并没有太大区别sce&lt;-runPCA(sce,ncomponents = 20)plotPCA(sce,colour_by=&quot;Biological_Condition&quot;) 12##可以挑选指定的PC来可视化，这里选5个PCplotPCA(sce,ncomponents=5,colour_by=&quot;Biological_Condition&quot;) tSNE 1234#tSNE可视化set.seed(1234)sce&lt;-runTSNE(sce,perplexity = 30)plotTSNE(sce,colour_by=&quot;Biological_Condition&quot;) 1234567#perplexity的设定值不一样，散点的分布也会有所不同，但趋势都是一致的#perplexity设置为10sce&lt;-runTSNE(sce,perplexity = 10,use_dimred = &quot;PCA&quot;,n_dimred = 10)plotTSNE(sce,colour_by=&quot;Biological_Condition&quot;)#perplexity设置为20sce&lt;-runTSNE(sce,perplexity = 20,use_dimred = &quot;PCA&quot;,n_dimred = 10)plotTSNE(sce,colour_by=&quot;Biological_Condition&quot;) 认识SC3R包SC3处理scRNAseq内置的数据1234567891011#R包SC3library(SC3)sce&lt;-sce_backsce&lt;-sc3_estimate_k(sce)metadata(sce)$sc3$k_estimationrowData(sce)$feature_symbol&lt;-rownames(rowData(sce))#一步运行sc3的所有分析，相当耗时，数据量大的话就真的相当耗时#这里kn表示的预估聚类数，数据集是已知的，我们这里就设定为4组，具体数据要具体考虑kn&lt;-4sc3_cluster&lt;-&quot;sc3_4_clusters&quot;sce&lt;-sc3(sce,ks=kn,biology = TRUE) 可视化展示kn就是聚类数 12#热图：比较先验分类和SC3的聚类的一致性sc3_plot_consensus(sce,k=kn,show_pdata = c(&quot;Biological_Condition&quot;,sc3_cluster)) 12#热图：展示表达量信息sc3_plot_expression(sce,k=kn,show_pdata = c(&quot;Biological_Condition&quot;,sc3_cluster)) SC3包找marker基因 12#热图，展示可能的标记基因sc3_plot_markers(sce,k=kn,show_pdata =c(&quot;Biological_Condition&quot;,sc3_cluster) )","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"scRNA-seq数据分析学习（四）","text":"学习R包monocle对scRNA-seq数据进行分析 scRNA-seq数据分析（四）创建数据集后续分析的前提就是将数据构建成monocle需要的对象因此这里先介绍一下monocle需要的用来构建CellDataSet对象的三个数据集 表达量矩阵exprs：数据矩阵，行名是基因，列明是细胞编号 细胞的表型信息phenoData：第一列是细胞编号，其他列是细胞的相关信息 基因注释featureData：第一列是基因编号，其他列是基因对应的信息这三个数据集要满足如下要求表达量矩阵 保证它的列数等于phenoData的行数 保证它的行数等于featureData的行数 phenoData的行名需要和表达矩阵的列明匹配 featureData和表达矩阵的行名要匹配 featureData至少要有一列”gene_short_name”，就是基因的symbol 123rm(list = ls())#clear the environmentoptions(warn=-1)#turn off warning message globallysuppressMessages(library(monocle)) 这里同样使用scRNAseq R包中的数据集，构建monocle对象 1234567#首先需要一个表达矩阵，还需要临床信息library(scRNAseq)data(fluidigm)#load examaple dataassay(fluidigm)&lt;-assays(fluidigm)$rsem_counts#set assay to RSEM estimated countsct&lt;-floor(assays(fluidigm)$rsem_counts)ct[1:4,1:4]#表达矩阵sample_ann&lt;-as.data.frame(colData(fluidigm))#临床信息 1234567#准备monocle对象需要的phenotype和feature data以及表达矩阵，从scRNA-seq这个R包里面提取这三个数据gene_ann&lt;-data.frame( gene_short_name=row.names(ct), row.names = row.names(ct))pd&lt;-new(&quot;AnnotatedDataFrame&quot;,data=sample_ann)fd&lt;-new(&quot;AnnotatedDataFrame&quot;,data=gene_ann) #构建monocle后续分析的所有对象，主要是根据包的说明书，仔细探索其需要的构建对象的必备元素#因为表达矩阵是counts值，所以注意expressionFamily参数,其他类型的值用其他参数 12345678sc_cds&lt;-newCellDataSet( ct, phenoData = pd, featureData = fd, expressionFamily = negbinomial.size(), lowerDetectionLimit = 1)sc_cds 在此示范如何加载加载RPKM数据，在此并不运行该部分代码从本地读入RPKM值文件，构造CellDataSet对象 123456789#读入所需要的数据：表达矩阵，样本信息，基因信息expression_matrix&lt;-read.table(&quot;fpkm_matrix.txt&quot;)sample_sheet&lt;-read.delin(&quot;cell_sample_sheet.txt&quot;)gene_annotation&lt;-read.delin(&quot;gene_annotation.txt&quot;)#创建CellDataSet对象pd&lt;-new(&quot;AnnotatedDataFrame&quot;,data=sample_sheet)fd&lt;-new(&quot;AnnotatedDataFrame&quot;,data=gene_annotation)sc_cds&lt;-newCellDataSet(as.matrix(expression_matrix), phenoData=pd,featureData=fd) monocle和scater、seurat他们基于的对象不一样，所以monocle还提供了转换函数 12345lymphomadata&lt;-sc_cds#转换成seurat对象lymphomadata_seurat&lt;-exportCDS(sc_cds,'Seurat')#转换成SCESet对象lymphomadata_scater&lt;-exportCDS(sc_cds,'Scater') 下面是monocle对新构建的CellDataSet对象的标准操作 123456library(dplyr)colnames(phenoData(sc_cds)@data)#estimateDispersions这步的时间和电脑配置密切相关,所以我们这套分析不选用monocle包自带的数据集，因为太大了，仍然采用scRNAseq内置数据集sc_cds&lt;-estimateSizeFactors(sc_cds)#一定要运行sc_cds&lt;-estimateDispersions(sc_cds)#一定要运行 质控对基因和细胞进行质量控制，指控指标根据课题进行具体探索，没有统一标准，这里只是简单演示 过滤基因 12345678cds&lt;-sc_cdscdscds&lt;-detectGenes(cds,min_expr = 0.1)#设置基因最小表达量为0.1print(head(fData(cds)))expressed_gene&lt;-row.names(subset(fData(cds),num_cells_expressed&gt;=5))length(expressed_gene)cds&lt;-cds[expressed_gene,]cds 过滤细胞 123456789#过滤细胞print(head(pData(cds)))#如果没有运行上述步骤的estimateSizeFactors、estimateDispersions，这一步就会发现某个属性出现NA值tmp&lt;-pData(cds)fivenum(tmp[,1])fivenum(tmp[,30])#这里展示就先不过滤细胞了，有需要就自行过滤valid_cell&lt;-row.names(pData(cds))#其实这里没有过滤细胞，valid_cell全为TRUEcds&lt;-cds[,valid_cell]cds 聚类单细胞转录组最重要的就是把细胞分群，相关的算法非常多，这里选用最常用的tSNE 1plot_pc_variance_explained(cds,return_all = F) 123456#由上图决定用前6个组成份进行降维,降维采用tSNE的方法cds&lt;-reduceDimension(cds,max_components = 2,num_dim=6, reduction_method = 'tSNE',verbose = T)cds&lt;-clusterCells(cds,num_clusters = 4)plot_cell_clusters(cds,1,2,color=&quot;Biological_Condition&quot;)table(pData(cds)$Biological_Condition) 可以看出，用monocle的方法进行单细胞转录组数据分析，GW的三群细胞依然无法很好的区分开，NPC倒是聚类得非常完美 寻找差异基因1234#这一步需要计算量，如果数据量大，这一步很耗时Sys.time()diff_test_res&lt;-differentialGeneTest(cds,fullModelFormulaStr = &quot;~Biological_Condition&quot;)Sys.time() 选出差异基因 12sig_gene&lt;-subset(diff_test_res,qval&lt;0.1)#筛选条件qval&lt;0.1head(sig_gene[,c(&quot;gene_short_name&quot;,&quot;pval&quot;,&quot;qval&quot;)]) 1234567choosed_gene&lt;-as.character(head(sig_gene$gene_short_name))plot_genes_jitter(cds[choosed_gene,],grouping = &quot;Biological_Condition&quot;,ncol = 2)plot_genes_jitter(cds[choosed_gene,],grouping = &quot;Biological_Condition&quot;, color_by = &quot;Biological_Condition&quot;,nrow = 3,ncol = NULL )#自己画图也可以 #boxplot(log10(exprs(cds)['A1BG',]+1)~pData(cds)$Biological_Condition) 推断发育轨迹 这是monocle包最大的亮点第一步：挑选合适的基因有多种方法，例如可以提供已经的基因集，这里选取统计学上有差异的基因 123ordering_gene&lt;-row.names(subset(diff_test_res,qval&lt; 0.01))cds&lt;-setOrderingFilter(cds,ordering_gene)plot_ordering_genes(cds) 第二部：降维降维的目的是更好的展示数据。函数里提供很多方法，不同方法最后展示的图会有所不同，DDRTree是Monocle2使用的默认方法 1cds&lt;-reduceDimension(cds,max_components = 2,method='DDRTree') 第三步：对细胞进行排序 12cds&lt;-orderCells(cds)plot_cell_trajectory(cds,color_by = &quot;Biological_Condition&quot;) 第四步：可视化marker gene随发育阶段变化的表达情况 12#plot_genes_in_pseudotime可以展示marker基因，本例子随便选取了6个差异表达基因plot_genes_in_pseudotime(cds[choosed_gene,],color_by = &quot;Biological_Condition&quot;) 参考资料生信技能数（B站、公众号）","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/"},{"title":"scRNA-seq数据分析实战（三）","text":"single cell RNA-seq &amp;&amp; 10×利用Seurate 2.3.3完成单细胞转录组下游分析，重现文章《Acquired cancer resistance to combination immunotherapy from transcriptional loss of class I HLA》的部分图片。 1234rm(list = ls()) # clear the environment#load all the necessary librariesoptions(warn=-1) # turn off warning message globallysuppressMessages(library(Seurat)) 读入文章关于第一个病人的PBMC表达矩阵123raw_dataPBMC &lt;- read.csv('path/to/GSE117988_raw.expMatrix_PBMC.csv.gz', header = TRUE, row.names = 1)raw_dataPBMC[1:4,1:4]dim(raw_dataPBMC) 标准化1dataPBMC &lt;- log2(1 + sweep(raw_dataPBMC, 2, median(colSums(raw_dataPBMC))/colSums(raw_dataPBMC), '*')) # Normalization，矩阵大也会比较耗时 按照治疗阶段分组（后续会用到）123456timePoints &lt;- sapply(colnames(dataPBMC), function(x) ExtractField(x, 2, '[.]'))timePoints &lt;-ifelse(timePoints == '1', 'PBMC_Pre', ifelse(timePoints == '2', 'PBMC_EarlyD27', ifelse(timePoints == '3', 'PBMC_RespD376', 'PBMC_ARD614')))table(timePoints) 表达矩阵的基本信息可视化（可用于知道质控）12345## 表达矩阵的质量控制fivenum(apply(dataPBMC,1,function(x) sum(x&gt;0) ))boxplot(apply(dataPBMC,1,function(x) sum(x&gt;0) ))fivenum(apply(dataPBMC,2,function(x) sum(x&gt;0) ))hist(apply(dataPBMC,2,function(x) sum(x&gt;0) )) 然后创建Seurat的对象12345# Create Seurat object# already normalized (above)PBMC &lt;- CreateSeuratObject(raw.data = dataPBMC, min.cells = 1, min.genes = 0, project = '10x_PBMC') #没有进行过滤PBMC 根据需要添加metadata信息12345# Add meta.data (nUMI and timePoints)PBMC &lt;- AddMetaData(object = PBMC, metadata = apply(raw_dataPBMC, 2, sum), col.name = 'nUMI_raw')PBMC &lt;- AddMetaData(object = PBMC, metadata = timePoints, col.name = 'TimePoints') 将对象进行基本的可视化1234sce=PBMCVlnPlot(object = sce, features.plot = c(&quot;nGene&quot;, &quot;nUMI&quot;), group.by = 'TimePoints', nCol = 2) 1GenePlot(object = sce, gene1 = &quot;nUMI&quot;, gene2 = &quot;nGene&quot;) 123456#可以看看高表达量基因是哪些tail(sort(Matrix::rowSums(sce@raw.data)))## 散点图可视化任意两个基因的一些属性（通常是细胞的度量）# 这里选取两个基因。tmp=names(sort(Matrix::rowSums(sce@raw.data),decreasing = T))GenePlot(object = sce, gene1 = tmp[1], gene2 = tmp[2]) 123# 散点图可视化任意两个细胞的一些属性（通常是基因的度量）# 这里选取两个细胞CellPlot(sce,sce@cell.names[3],sce@cell.names[4],do.ident = FALSE) 聚类可视化（重点）1234# Cluster PBMCPBMC &lt;- ScaleData(object = PBMC, vars.to.regress = c('nUMI_raw'), model.use = 'linear', use.umi = FALSE)#因为矩阵比较大，scale这一步非常耗内存，计算机内存不够的话这一步会报错PBMC &lt;- FindVariableGenes(object = PBMC, mean.function = ExpMean, dispersion.function = LogVMR, x.low.cutoff = 0.0125, x.high.cutoff = 3, y.cutoff = 0.5) 1234567891011121314PBMC &lt;- RunPCA(object = PBMC, pc.genes = PBMC@var.genes)#### 避免太多log日志被打印出来。PBMC &lt;- FindClusters(object = PBMC, reduction.type = &quot;pca&quot;, dims.use = 1:10, resolution = 1, print.output = 0, k.param = 35, save.SNN = TRUE)#在此，resolution、k.param的值设定不同，分群效果也会不一样，参数的调整也是一个比较麻烦的事情table(PBMC@ident)# 分出了13个clustersPBMC &lt;- RunTSNE(object = PBMC, dims.use = 1:10)TSNEPlot(PBMC, colors.use = c('green4', 'pink', '#FF7F00', 'orchid', '#99c9fb', 'dodgerblue2', 'grey30', 'yellow', 'grey60', 'grey', 'red', '#FB9A99', 'black'),do.label=T) 1table(PBMC@meta.data$TimePoints,PBMC@ident) 添加亚群信息12345current.cluster.ids &lt;- c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)#具体是哪一种cell type,根据每个亚群的maker基因，再结合生物学背景确定亚群的具体类型，这个也有网站预测。new.cluster.ids &lt;- c(&quot;B cells&quot;, &quot;CD14+ T cells&quot;, &quot;Classical monicytes&quot;,&quot;Naive memory T cells&quot;, &quot;CD8 effector T cells&quot;, &quot;NK cells&quot;, &quot;NULL1&quot;, &quot;Non-classical monocytes&quot;,&quot;Dendritic cells&quot;, &quot;NULL2&quot;,&quot;CD8+ cytotoxic T cells&quot;,&quot;Myeloid cells&quot;,&quot;NULL3&quot;)PBMC@ident &lt;- plyr::mapvalues(x = PBMC@ident, from = current.cluster.ids, to = new.cluster.ids)TSNEPlot(object = PBMC, colors.use = c('green4', 'pink', '#FF7F00', 'orchid', '#99c9fb', 'dodgerblue2', 'grey30', 'yellow', 'grey60', 'grey', 'red', '#FB9A99', 'black'),do.label=T) QUESTION 代码都跟原文的代码一样，虽然主要想体现的东西是有所体现的，聚类出来的细胞亚群还是有小小的偏差。 FindClusters的参数真的是我至今为止最最纠结的问题啦，参数不一样，亚群就会不一样，难不成为了得到自己想要的分群效果，不停的调整参数吗？ 对象问题也是个巨大的问题，对象里面包裹了好多东西，有些不太会调用。 该数据按照TimePoint来体现，目前自创的方法无法做到与原文类似的图，这也是一个待解决的问题。","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"scRNA-seq数据分析实战（一）","text":"single cell RNA-seq &amp;&amp; SMART-seq2上游分析：Linux下游分析：用转录组思想初步了解探究单细胞转录组数据（注意：处理单细胞转录组和转录组是有很大区别的，完全将转录组思想应用于单细胞转录组数据分析是万万不可取的！！！） scRNA-seq数据分析流程以下分析流程以GSE111229数据为例(为什么用它呢，因为它数据量小呀，嘿嘿🤭)在进行分析之前，首先了解一下该套数据的基本情况。 由此可基本了解，此数据为单端测序数据，物种为小鼠。768个样本即768个单细胞测序数据，总大小才10G。这在单细胞测序数据中真的是非常小！非常罕见啦！ 😄数据分析上游(Linux)😄因为是单细胞转录组测序数据，和转录组测序的上游分析没什么区别。所以以下分析流程都是在conda构建的rna环境下进行，这个环境也是很久之前搭建好的，在此不再赘述。 1conda activat rna 运行上述代码后，可以看到提示符(prompt)左边多了一个(rna)环境，此环境里面，预先装好了处理RNA-seq数据所需要的相关软件。 数据下载 如果样本量不多，网速又不理想，推荐EBI-ENA数据库，此数据库下载网速还可以，另外该数据库除了提供SRA文件外，还提供FASTQ文件，省去了从NCBI下载SRA文件转为FASTQ文件的麻烦(如何单个文件数据量大，此格式转换步骤非常耗时)。如果样本量特别多，网速又比较理想，推荐使用Aspera、prfetch、wget、curl。前两者大大优于后两者。Aspera Connect的下载速度是最快了，此方法也可以下载FASTQ和SRA文件，免去了SRA转至FASTQ的过程，该过程很耗时，十分耗时。 进入EBI官网，输入SRP号123以其中一个样本SRR6791132为例，点击SRR6791132进入下载界面，鼠标右键红色箭头处——“复制链接地址” 进入linux进行数据下载1234mkdir -p scRNA-seq/fastq &amp;&amp; cd scRNA-seq/fastq wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR679/002/SRR6791132/SRR6791132.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR679/005/SRR6791135/SRR6791135.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR679/005/SRR6791135/SRR6791135.fastq.gz 注意注意，特别注意，要下载整套数据最好使用Aspera或者prefetch，本宝宝没有办法呀，网不好，空间也不大，linux中的数据操作只能用三个样本做演示了，熟悉一下分析流程。如果是整套数据，记得写小循环，放后台，就不用一直守着它了，最好是睡觉的时候跑，不耽误事。 质控123#首先查看数据质量情况，进行如下操作 mkdir -p /scRNA-seq/html &amp;&amp; cd /scRNA-seq/htmlls /scRNA-seq/fastq/*gz|xargs fastqc -t 4 -o ./ 上述操作会得到html文件和zip文件 网页打开其中一个html文件可查看测序数据的质量，该数据整体质量还不错（质控报告解读教程） 大部分测序数据都会出现如下情况，不过这个问题不大 1234#接下来过滤掉质量不好的数据，使trim_galore软件 #mkdir /scRNA-seq/clean#ls /scRNA-seq/fastq/*gz|while read id; do trim_galore -q 25 --phred33 --length 35 -e 0.1 --stringency 5 -o . $id;done #在此会发现过滤后的数据质量反而没有过滤前的质量好，有时候数据分析就是会这样，过滤参数很难调整。。。所以在此后续步骤使用未过滤的数据进行处理。 比对1、小鼠参考基因组首先下载小鼠参考基因组，hisat2或者bowtie2构建小鼠参考基因组索引。也可以直接下载参考基因组索引文件。另外还需要下载小鼠参考基因组注释文件。(此步骤很早前处理RNA-seq数据时就已经完成了，就不再赘述)hisat2构建的mm10参考基因组索引文件mm10注释文件 2、进行参考基因组比对 123456#同参考基因组索引进行比对，生成bam文件。参考基因组索引一定是引用前缀&quot;genome&quot;。另外注意，此处数据为单端测序数据！ls *.gz | while read id; do hisat2 -p 4 -x /addfirst/reference/mouse/index/hisat2-build-index/mm10/genome -U $id -S ${id%%.*}.hisat2.sam;done mkdir /scRNA-seq/align &amp;&amp; cd /scRNA-seq/align cp /scRNA-seq/fastq/*.sam /scRNA-seq/fastq/*.bam ./ #使用samtools软件将sam文件转成bam文件 ls *.sam | while read id; do samtools sort -O bam -@ 4 -o ${id%%.*}.hisat2.bam $id; done 比对结果如下：82.25%的比对率，比较的凑合了，一般情况最好90%以上，后两个数据的比对率实在不能看。真实处理环境一定要仔细调节过滤参数！！！ 可以比较一下bam、fasatq、sam文件的大小。sam文件远远大于bam和fastq文件。生信分析上游一定要对文件大小保持高度敏感，避免不必要的错误。另外，数据量大的话，可以在生成sam文件时通过|将结果进一步生成bam文件，这样可以大大节省存储空间。 构建index文件1ls *.bam | xargs -i samtools index {} 生成reads表达矩阵1234gtf=/addfirst/reference/mouse/gencode.vM23.annotation.gtf featureCounts -T 4 -p -t exon -g gene_id -a $gtf -o all.id.txt *.bam 1&gt;counts.id.log 2&gt;&amp;1 查看counts矩阵 less -S all.id.txt 查看counts矩阵会发现绝大多数基因表达量都是0，因为对于大多数细胞来说，大多数基因都是测不到的，这个属于正常现象。在此，我们只用了3个样本做演示，总数据则有768个样本。 理论上按照上面的分析流程生成的表达矩阵数据和下图中NCBI官网上的rawCounts.txt.gz的内容应该是差不多的，只是所用处理软件不同会有一点点区别。 至此，scRNA-seq上游分析结束，这里比较重要的是数据质量，数据质量直接决定比对的效率，影响最终生成的表达矩阵的可靠性。所以质控步骤要严格把控，送样测序也要选靠谱的公司。 😄数据分析下游(R)😄数据(data.frame)的构建如果是使用rawCounts数据进行下游分析，首先要对数据进行标准化，然后再做后续分析。该篇文章是使用rpkm的数据进行下游分析，这里也直接使用rpkm的数据进行下游分析。 1234567891011a=read.table('../GSE111229_Mammary_Tumor_fibroblasts_768samples_rpkmNormalized.txt.gz',header = T ,sep = '\\t') ##把表达矩阵文件载入R，header=T :保留文件头部信息，seq='\\t'以tap为分隔符# 记得检测数据a[1:6,1:4] #对于a矩阵取第1~6行，第1~4列## 读取RNA-seq的counts定量结果，表达矩阵需要进行简单的过滤dat=a[apply(a,1, function(x) sum(x&gt;0) &gt; floor(ncol(a)/50)),] #筛选表达量合格的行,列数不变 #上面的apply()指令代表对矩阵a进行行计算，判断每行表达量&gt;1的样本总个数，并筛选出细胞表达量合格的基因（行）#第一个参数是指要参与计算的矩阵——a#第二个参数是指按行计算还是按列计算，1——表示按行计算，2——按列计算；#第三个参数是指具体的运算参数,定义一个函数x（即表达量为x）dat[1:4,1:4] 1234567#层次聚类hc=hclust(dist(t(log(dat+0.1)))) ##样本间层次聚类# 如果是基因聚类，可以选择 wgcna 等算法 plot(hc,labels = F) ##此图可以看出678个样本大致可以分为几类，在此大致可分为4类 clus = cutree(hc, 4) #对hclust()函数的聚类结果进行剪枝，即选择输出指定类别数的系谱聚类结果。group_list= as.factor(clus) ##转换为因子属性table(group_list) ##统计频数 12345678910111213#提取批次信息colnames(dat) #取列名library(stringr)plate=str_split(colnames(dat),'_',simplify = T)[,3] #str_split()函数可以分割字符串。取列名，以'_'号分割，提取第三列。table(plate) #以为这768个样本是放在两个384孔检测的，后续要检测测序数据是否存在批次效应，所以要预先提取批次信息 n_g = apply(a,2,function(x) sum(x&gt;0)) #统计每个样本有表达的基因有多少行#reads数量大于1的那些基因为有表达，一般来说单细胞转录组过半数的基因是不会表达的。 df=data.frame(g=group_list,plate=plate,n_g=n_g) #新建数据框(细胞的属性信息) ##(样本为行名，列分别为：样本分类信息，样本批次，样本表达的基因数——不是表达量的和，而是种类数或者说个数) df$all='all' #添加列，列名为&quot;all&quot;，没事意思，就是后面有需要metadata=df df[1:4,1:4]save(dat,metadata,file = '../input_rpkm.Rdata') #保存a,dat,df这变量到上级目录的input.Rdata 检测是否存在批次效应因为该篇文章使用两块384孔进行测序，所以要先确定两块板是否存在批次效应，如果存在，则两块板的样本不能合并分析，如果不存在批次效应，则两块板可以进行批次分析。 PCA主成份分析 123456789101112131415161718192021rm(list = ls()) ## 魔幻操作，一键清空~options(stringsAsFactors = F)load(file = '../input_rpkm.Rdata')dat[1:4,1:4]head(metadata) plate=metadata$plate## 下面是画PCA的必须操作dat_back=datdat=t(dat) #做PCA之前一定要进行数据转置dat=as.data.frame(dat)dat=cbind(dat,plate ) #cbind根据列进行合并，即叠加所有列，矩阵添加批次信息dat[1:4,12688:12690] #检测是否添加上了批次信息table(dat$plate)library(&quot;FactoMineR&quot;)library(&quot;factoextra&quot;) dat.pca &lt;- PCA(dat[,-ncol(dat)], graph = FALSE)fviz_pca_ind(dat.pca,#repel =T, geom.ind = &quot;point&quot;, col.ind = dat$plate, # color by groups addEllipses = FALSE, # Concentration ellipses legend.title = &quot;Groups&quot;) tSNE分析 12345678910111213141516rm(list = ls()) ## 魔幻操作，一键清空~options(stringsAsFactors = F) library(Rtsne) load(file = '../input_rpkm.Rdata')dat[1:4,1:4]dat_matrix &lt;- t(dat) dat_matrix=log2(dat_matrix+0.01)set.seed(42) # 如果想得到可重复的结果，就种一颗种子吧😄tsne_out &lt;- Rtsne(dat_matrix,pca=FALSE,perplexity=30,theta=0.0) ##添加颜色tsnes=tsne_out$Ytsnes=as.data.frame(tsnes)group=c(rep('plate1',384),rep('plate2',384))colnames(tsnes)&lt;-c(&quot;tSNE1&quot;,&quot;tSNE2&quot;) library(ggfortify)ggplot(tsnes, aes(x = tSNE1, y = tSNE2))+ geom_point(aes(col=group))+ theme_classic() 细胞亚群123456789101112131415161718192021222324252627282930313233343536373839404142434445###########################针对所有细胞做PCA分析## #########################rm(list = ls()) ## 魔幻操作，一键清空options(stringsAsFactors = F)load(file = '../input_rpkm.Rdata')dat[1:4,1:4]head(metadata) group&lt;-metadata$g#做好数据备份dat_back=datdat=t(dat) #记得转置数据dat=as.data.frame(dat)dat=cbind(dat,group ) #cbind根据列进行合并，即叠加所有列，为矩阵添加分组信息dat[1:4,12688:12690]table(dat$group)library(&quot;FactoMineR&quot;)library(&quot;factoextra&quot;) dat.pca &lt;- PCA(dat[,-ncol(dat)], graph = FALSE)fviz_pca_ind(dat.pca,#repel =T, geom.ind = &quot;point&quot;, # show points only (nbut not &quot;text&quot;) col.ind = dat$group, # color by groups addEllipses = FALSE, # Concentration ellipses legend.title = &quot;Groups&quot; ) ###########################针对所有细胞做tSNE分析## #########################rm(list = ls()) ## 魔幻操作，一键清空~options(stringsAsFactors = F)library(Rtsne) load(file = '../input_rpkm.Rdata')dat[1:4,1:4]dat_matrix &lt;- t(dat)dat_matrix=log2(dat_matrix+0.01)# Set a seed if you want reproducible resultsset.seed(42)tsne_out &lt;- Rtsne(dat_matrix,pca=FALSE,perplexity=30,theta=0.0) group=metadata$g##添加颜色tsnes=tsne_out$Ytsnes=as.data.frame(tsnes)colnames(tsnes)&lt;-c(&quot;tSNE1&quot;,&quot;tSNE2&quot;)ggplot(tsnes, aes(x = tSNE1, y = tSNE2))+ geom_point(aes(col=group))+ theme_classic() 单细胞测序分析旨在分出不同细胞亚群(展现细胞间的异质性)，力求寻找某类亚群与某特定生物学功能之间的联系。而要探讨细胞分群，就必须要知道PCA和tSNE。PCA(Principal Component Analysis)tSNE(t-Distributed Stochastic Neighbor Embedding)什么样的算法才是最理想的能够将细胞分群的算法呢？1）局部结构：属于同一个亚群的细胞，聚类尽可能近2）全局结构：属于不同亚群的细胞，聚类尽可能分来 PCA的方法侧重于去抓样本中隐含的主要效应，从而让差异大的样本在图上呈现较远的距离。常规RNA-seq项目中，处理效应、批次效应、离群效应等属于较大的效应，PCA的方法可以良好的去展示这些效应。 如果影响样本分组的不是主要效应，而是一些更小的效应，PCA则无法对样本进行准确的区分。scRNA-seq的可视化主要期望对各个细胞亚群有良好的区分。每次检测的上万甚至几十万个细胞中，几乎肯定可以区分出几十中细胞亚群，包括一些稀有的细胞。而区分这些细胞亚群（尤其是稀有细胞类型）的效应，往往不是主要效应（即大量基因的差异），而是一些微小效应（少量标记基因差异）。如果使用PCA进行分析的话，就会掩盖掉某些微小的但是有可能非常重要的细胞群。 tSNE算法就属于可以同时兼顾局部结构和全局结构的非线性降维可视化算法。tSNE不同于PCA（PCA主要目标是尽量去抓取群体中的主要效益），tSNE算法的主要目标是：降维后的数据依然保持最为相似的紧密成簇。这就保证了哪怕某些稀有细胞只是少量基因区别于其他细胞亚群，在tSNE中依然可以与其他细胞有良好的区分。 结论：PCA是常规转录组常用的数据降维和样本关系可视化的方法，但针对群体单细胞转录组数据，tSNE是明显胜过PCA的方法！ 参考资料http://www.sohu.com/a/320164491_278730","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"scRNA-seq数据分析实战（二）","text":"体验Seraut包处理GSE111229的scRNA-seq数据 12rm(list = ls()) Sys.setenv(R_MAX_NUM_DLLS=999) 载入数据12345678910load(file='../input.Rdata')counts=a# using raw counts is the easiest way to process data through Seurat.counts[1:4,1:4];dim(counts)library(stringr) meta=dfhead(meta) # 下面的基因是文章作者给出的gs=read.table('top18-genes-in-4-subgroup.txt')[,1]gs 12345#简单了解一下表达矩阵fivenum(apply(counts,1,function(x) sum(x&gt;0) ))boxplot(apply(counts,1,function(x) sum(x&gt;0) ))fivenum(apply(counts,2,function(x) sum(x&gt;0) ))hist(apply(counts,2,function(x) sum(x&gt;0) )) 构建Seraute对象123456789library(Seurat)# 其中 min.cells 和 min.genes 两个参数是经验值#Seurat真的很优秀呀，给定参数，自动进行质控seu &lt;- CreateSeuratObject(raw.data = counts, meta.data =meta, min.cells = 5, min.genes = 2000, project = &quot;seu&quot;)seu 123#可以尝试以下手动质控table(apply(counts,2,function(x) sum(x&gt;0) )&gt;2000)table(apply(counts,1,function(x) sum(x&gt;0) )&gt;4) 可以看到，两种质控方法得到的数据非常接近 查看对象的基本情况 12head(seu@meta.data) dim(seu@data) 可视化基本信息 1234VlnPlot(object = seu, features.plot = c(&quot;nGene&quot;, &quot;nUMI&quot; ), group.by = 'plate', nCol = 2) 可以看出：不管是各个样本检测到的基因数量还是文库大小，都没有批次效应。 1234VlnPlot(object = seu, features.plot = c(&quot;nGene&quot;, &quot;nUMI&quot; ), group.by = 'g', nCol = 2) 可以看出：层次聚类得到的分组主要是因为样本间检测的基因数量的差异造成的。 可以给seu对象增加一个属性，供QC使用 12345678910#构建新增的属性ercc.genes &lt;- grep(pattern = &quot;^ERCC-&quot;, x = rownames(x = seu@raw.data), value = TRUE)percent.ercc &lt;- Matrix::colSums(seu@raw.data[ercc.genes, ]) / Matrix::colSums(seu@raw.data)# 添加新增的属性seu &lt;- AddMetaData(object = seu, metadata = percent.ercc, col.name = &quot;percent.ercc&quot;)VlnPlot(object = seu, features.plot = c(&quot;nGene&quot;, &quot;nUMI&quot;, &quot;percent.ercc&quot; ), group.by = 'g', nCol = 3) 可以看出：细胞能检测到的基因数量与其含有的ERCC序列反相关。当组间的文库大小差异可忽略时，外源RNA多，计算后的内源RNA的比例自然是会下降的。 进一步可视化基本信息 123456#VlnPlot(seu,group.by = 'plate',c(&quot;nGene&quot;,&quot;Gapdh&quot;,&quot;Bmp3&quot;))#上面这段代码和下面的代码时同样的效果VlnPlot(object = seu, features.plot = c(&quot;nGene&quot;, &quot;Gapdh&quot;, &quot;Bmp3&quot; ), group.by = 'plate', nCol = 3) 再次证明没有批次效应 1GenePlot(object = seu, gene1 = &quot;nUMI&quot;, gene2 = &quot;nGene&quot;) 1GenePlot(object = seu, gene1 = &quot;Brca1&quot;, gene2 = &quot;Brca2&quot;) 123CellPlot(seu,seu@cell.names[3], seu@cell.names[4], do.ident = FALSE) 表达矩阵标准化1234seu &lt;- NormalizeData(object = seu, normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000)seu@data[1:4,1:4] 挑选变化的基因1234567# 这里需要理解 dispersion 值seu &lt;- FindVariableGenes(object = seu, mean.function = ExpMean, dispersion.function = LogVMR )## 默认值是：x.low.cutoff = 0.1, x.high.cutoff = 8, y.cutoff = 1## 根据经验阈值挑选的变化基因个数。length( seu@var.genes) 去除一些技术误差，比如 nUMI或者ERCC123456head(seu@meta.data) seu &lt;- ScaleData(object = seu, vars.to.regress = c(&quot;nUMI&quot;,'nGene',&quot;percent.ercc&quot; ))# 后面就不需要考虑ERCC序列了。seu@scale.data[1:4,1:4]pheatmap(as.matrix(seu@scale.data[gs,])) 普通PCA降维用前面挑选的有变化的基因进行降维 12345seu &lt;- RunPCA(object = seu, pc.genes = seu@var.genes, do.print = TRUE, pcs.print = 1:5, genes.print = 5)tmp &lt;- seu@dr$pca@gene.loadingshead(tmp) 1VizPCA( seu, pcs.use = 1:2) 1PCAPlot(seu, dim.1 = 1, dim.2 = 2,group.by = 'plate') 1PCAPlot(seu, dim.1 = 1, dim.2 = 2,group.by = 'g') 根据不同组成份进行热图的展示 12345seu &lt;- ProjectPCA(seu, do.print = FALSE)#根据PC1进行可视化PCHeatmap(object = seu, pc.use = 1, cells.use = 100, do.balanced = TRUE, label.columns = FALSE)#使用pc.use = 1:10即批量展示前10各组成份的热图信息 根据参数来调整最后的分组 123456789#根据参数来调整最后的分组个数#因为文章分为4群细胞，所以这里调整resolution的值为0.4，最后会得到4个组#resolution很重要很重要很重要seu1 &lt;- FindClusters(object = seu, reduction.type = &quot;pca&quot;, dims.use = 1:20, force.recalc = T, resolution = 0.4, print.output = 0, save.SNN = TRUE)PrintFindClustersParams(seu1)table(seu1@meta.data$res.0.4) tSNE展示分组信息12345## resolution 是最关键的参数seu=seu1seu &lt;- RunTSNE(object = seu, dims.use = 1:10, do.fast = TRUE)# note that you can set do.label=T to help label individual clustersTSNEPlot(object = seu, do.label=T) 对每个类别的细胞寻找marker基因123seu.markers &lt;- FindAllMarkers(object = seu, only.pos = TRUE, min.pct = 0.25, thresh.use = 0.25) 123456library(dplyr)top20 &lt;- seu.markers %&gt;% group_by(cluster) %&gt;% top_n(20, avg_logFC)#读入作者的4个亚群的top级的差异基因gs=read.table('top18-genes-in-4-subgroup.txt')[,1]gsintersect(top10$gene,gs) 这时，你会发现用Seurat方法计算得到的差异基因和作者计算（自己写函数，并没有用到任何R包，有点搞不懂他为什么要这么做，还有一点小小的敬佩）得到的差异基因部分重合（37个共同基因）。 用不同的方法计算得到的差异基因都会有一定的差异，就转录组差异分析而言，limma、DESeq2、EdgeR三大差异分析包计算的差异基因也是部分重合的。 生信分析在一定程度上给研究者提供了方向，具体验证还是要落实到实验上。但是，干数据也很重要，没有方向怎么能行！所以呀，干数据和湿数据都要要，一个都不能少！ 1234top20 &lt;- seu.markers %&gt;% group_by(cluster) %&gt;% top_n(20, avg_logFC)intersect(top20$gene,gs)DoHeatmap(object = seu, genes.use = top20$gene, slim.col.label = TRUE, remove.key = TRUE) 123456FeaturePlot(object = seu, features.plot =&quot;Rgs5&quot;, cols.use = c(&quot;yellow&quot;, &quot;red&quot;), reduction.use = &quot;tsne&quot;, no.legend=FALSE ) 123456FeaturePlot(object = seu, features.plot =&quot;Dcn&quot;, cols.use = c(&quot;yellow&quot;, &quot;red&quot;), reduction.use = &quot;tsne&quot;, no.legend=FALSE ) 123456FeaturePlot(object = seu, features.plot =&quot;Pbk&quot;, cols.use = c(&quot;yellow&quot;, &quot;red&quot;), reduction.use = &quot;tsne&quot;, no.legend=FALSE ) 123456FeaturePlot(object = seu, features.plot =&quot;Trf&quot;, cols.use = c(&quot;yellow&quot;, &quot;red&quot;), reduction.use = &quot;tsne&quot;, no.legend=FALSE )","link":"/scRNA-seq%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"sed编辑器基础","text":"sed编辑器更多的命令和格式：更多的替换选项、使用地址、删除行、插入和附加文本、修改行、转换命令、打印、使用sed处理文件 sed编辑器基础更多的替换选项1、替换标记替换命令在替换多行中的文本时能正常工作，但默认情况下它只替换每行中出现的第一处。要让替换命令能够替换一行中不同地方出现的文本必须使用替换标记（substitution flag）。 s/pattern/replacement/flags 有4种可用的替换标记： 数字，表明新文本将替换第几处模式匹配的地方 g，表明新文本将会替换所有匹配的文本 p，表明原先行的内容要打印出来 w file，将替换的结果写到文件中 12345678910111213141516#默认情况下sed只替换每行中出现的第一处。sed 's/test/trial/' sed1.txt#sed替换每行中第二初匹配的地方。sed 's/test/trial/2' sed1.txt#替换所有匹配的文本。sed 's/test/trial/g' sed1.txt#p替换标记会输出修改过的行。#sed命令的正常输出就是STANDOUT。sed 's/test/trial/p' sed1.txt#p替换标记会输出修改过的行。sed 's/test/trial/p' sed2.txt#-n选项将禁止sed编辑器输出。但p替换标记会输出修改过的行。将二者配合使用的效果就是只输出被替换命令修改过的行。sed -n 's/test/trial/p' sed2.txt#sed编辑器的正常输出是在STDOUT中，只有那些包含匹配模式的行才会保存在指定的输出文件中。sed 's/test/trial/w test.txt' sed2.txtcat test.txt 2、替换字符有时你会在文本字符串中遇到一些不太方便在替换模式中使用的字符。Linux中一个常见的 例子就是正斜线（/），由于正斜线通常用作字符串分隔符，因而如果它出现在了模式文本中的话，必须用反斜线来转义。 要解决这个问题，sed编辑器允许选择其他字符来作为替换命令中的字符串分隔符（与需要替换的转义字符不一致的一般都可以）。 使用地址默认情况下，在sed编辑器中使用的命令会作用于文本数据的所有行。如果只想将命令作用 于特定行或某些行，则必须用行寻址（line addressing）。 在sed编辑器中有两种形式的行寻址： 以数字形式表示行区间 用文本模式来过滤出行 1、数字方式的行寻址12345cat test1.txtsed 's/dog/cat/' test1.txtsed '2s/dog/cat/' test1.txtsed '2,4s/dog/cat/' test1.txtsed '2,$/dog/cat/' test1.txt 2、使用文本模式过滤器/pattern/command必须用正斜线将要指定的pattern封起来。 12345#方法一cat /etc/passwd|tail | sed '/apache/s/nologin/modification/' #方法二cat /etc/passwd|tail | sed '/apache/s$nologin$modification$'sed '/root/s/bash/modification/' /etc/passwd 3、命令组合如果需要在单行上执行多条命令，可以用花括号将多条命令组合在一起。sed编辑器会处理 地址行处列出的每条命令。 123456789sed '2{&gt; s/fox/elephant/&gt; s/dog/cat/&gt; }' test1.txtsed '3,${&gt; s/fox/elephant/&gt; s/dog/cat/&gt; }' test1.txt 3、删除行 指定地址 123456#不加入寻址模式，流中的所有文本行都会被删除sed 'd' test2.txt#加入寻址模式sed '3d' test2.txtsed '3,5d' test2.txtsed '3,$d' test2.txt 模式匹配 12345678sed '/line 2/d' test2.txt#只要符合匹配模式的行都会被删除，所以下面的命令会删除2行sed '/line 1/d' test2.txt#指定的第一个模式 会“打开”行删除功能，第二个模式会“关闭”行删除功能sed '/2/,/4/d' test2.txt#因为有两行都匹配到1了，而第二个匹配到的1匹配不到“关闭的模式”，所以后面的内容统统删掉了sed '/1/,/3/d' test2.txtsed '/1/,/6/d' test2.txt 插入和附加文本 插入（insert）命令（i）会在指定行前增加一个新行 附加（append）命令（a）会在指定行后增加一个新行 要向数据流行内部插入或附加数据，必须用寻址来告诉sed编辑器你想让数据出现在什么位置。可以在用这些命令时只指定一个行地址。可以匹配一个数字行号或文本模式，但不能用地址区间。 修改行修改（change）命令允许修改数据流中整行文本的内容。 1234567#将第一句修改为I like teacher Li.#方法一：行定位需要修改的行sed '1c\\I like teacher Li.' test2.txt#方法二：模式匹配定位需要修改的行sed '/line 1/c\\I like teacher Li.' test2.txt#将1至6行都修改为I like teacher Li. ♥因为1至6行都跟teacher Li无关，所以要强行修改一下 ♥ 1sed '1,6c\\I like teacher Li.' test2.txt 转换命令转换（transform）命令（y）是唯一可以处理单个字符的sed编辑器命令。 [address]y/inchars/outchars/ 转换命令会对inchars和outchars值进行一对一的映射。inchars中的第一个字符会被转 换为outchars中的第一个字符，第二个字符会被转换成outchars中的第二个字符。这个映射过 程会一直持续到处理完指定字符。如果inchars和outchars的长度不同，则sed编辑器会产生一条错误消息。 12345#会发现转换命令是全局的sed 'y/123/456' test2.txt#匹配行进行修改sed '1y/1/6' test2.txtsed '6y/1/6' test2.txt 回顾打印 p命令用来打印文本行 等号（=）命令用来打印行号 l（小写的L）命令用来列出行 1、打印行123456789#模式匹配到的行打印出来sed -n '/li/p' test2.txt#行数匹配到的行打印出来sed -n '7,8p' tets2.txt#打印出修改之前的行和修改之后的行sed -n '/qinqin/{&gt; p&gt; s/meimei/baobei/p&gt; }' test2.txt 2、打印行号等号命令会打印行在数据流中的当前行号。行号由数据流中的换行符决定。每次数据流中出 现一个换行符，sed编辑器会认为一行文本结束了。 在数据流中查找特定文本模式的行，并将其与行号一起打印出来。 1234sed -n '/Li/{&gt; =&gt; p}' test2.txt 使用sed处理文件1、写入文件w命令用来向文件写入行。该命令的格式如下： [address]w filename 1234#支持行匹配和模式匹配sed '7,8w li.txt' test2.txtsed '/Li/w Li.txt' test2.txtsed -n '/qinqin/w qinqin.txt' test2.txt 2、从文件读取数据已经了解了如何在sed命令行上向数据流中插入或附加文本。读取（read）命令（r）允许将一个独立文件中的数据插入到数据流中。 读取命令的格式如下： [address]r filename 支持行寻址和模式匹配 123sed '4r yinyu.txt' friends.txtsed '4r yinyu.txt' friends.txtsed '/yuqi/r yinyu.txt' friends.txt 1234567sed '/friends/{r friends.txt}' beautiful-people.txtsed '/friends/{r friends.txtd}' beautiful-people.txt","link":"/sed%E7%BC%96%E8%BE%91%E5%99%A8%E5%9F%BA%E7%A1%80/"},{"title":"sed 进阶","text":"多行命令、保持空间、排除命令、改变流、模式替代、在脚本中使用sed、创建sed实用程序 sed进阶一、多行命令sed编辑器包含了三个可用来处理多行文本的特殊命令。 N：将数据流中的下一行加进来创建一个多行组（multiline group）来处理 D：删除多行组中的一行 P：打印多行组中的一行 （一）next命令1、单行的next命令12345#sed命令默认匹配所有符合条件的行sed '/^\\s*$/d' data1.txt#模式匹配到含有header的那一行，n为跳到下一行，d为删掉n跳到的那行sed '/header/{n;d}' data.txtsed '/data/{n;d}' data.txt 2、合并文本行单行next命令会将数据流中的下一文本 行移动到sed编辑器的工作空间（称为模式空间）。多行版本的next命令（用大写N）会将下一文本行添加到模式空间中已有的文本后。 这样的作用是将数据流中的两个文本行合并到同一个模式空间中。文本行仍然用换行符分隔，但sed编辑器现在会将两行文本当成一行来处理。 123456789101112#sed编辑器脚本查找含有单词first的那行文本。找到该行后，它会用N命令将下一行合并到那 行，然后用替换命令s将换行符替换成空格sed '/first/{N; s/\\n/ /}' data2.txt#一下两行代码效果一样。因为data3.txt中的System和Administrator间为换行符，所以使用N也没有用sed 's/System Administrator/Desktop User/' data3.txt sed 'N; s/System Administrator/Desktop User/' data3.txt #替换命令在System和Administrator之间用了通配符模式（.）来匹配空格和换行符,当它匹配了换行符时，它就从字符串中删掉了换行符，导致两行合并成一行sed 'N; s/System.Administrator/Desktop User/' data3.txt#可以用一下方式避免两行匹配sed 'N&gt; s/System\\nAdministrator/Desktop\\nUser/&gt; s/System Administrator/Desktop User/&gt; ' data3.txt 1234567891011#这个脚本总是在执行sed编辑器命令前将下一行文本读入到模式空间。当它到了最后一行文本时，就没有下一行可读了，所以N命令会叫sed编辑器停止。如果要匹配的文本正好在数据流的最后一行上，命令就不会发现要匹配的数据。 sed 'Ns/System\\nAdministrator/Desktop\\nUser/s/System Administrator/Desktop User/' data4.txt#由于System Administrator文本出现在了数据流中的后一行，N命令会错过它，因为没有其他行可读入到模式空间跟这行合并。将单行命令放到N命令前面，并将多行命令放到N命令后面就可以得到理想的结果。sed '&gt; s/System Administrator/Desktop User/&gt; N&gt; s/System\\nAdministrator/Desktop User/&gt; ' data4.txt （二）多行删除命令12345678910#很奇怪，我的centos7环境下 sed '/^$/d' data5.txt似乎失灵了。#删除所有空白行sed '/^\\s*$/d' data5.txt#sed编辑器脚本会查找空白行，然后用N命令来将下一文本行添加到模式空间。如果新的模式空间内容含有单词header，则D命令会删除模式空间中的第一行。sed '/^\\s*$/{N; /header/D}' data5.txtsed '/^\\s*$/{N; /last/D}' data5.txt#删除命令会在不同的行中查找单词System和Administrator，然后在模式空间中将两行都删掉。 sed 'N; /System\\nAdministrator/d' data4.txt#sed编辑器提供了多行删除命令D，它只删除模式空间中的第一行。该命令会删除到换行符（含换行符）为止的所有字符。sed 'N; /System\\nAdministrator/D' data4.txt 如果不结合使用N命令和D命令， 就不可能在不删除其他空白行的情况下只删除第一个空白行。 （三）多行打印命令当多行匹配出现时，P命令只会打印模式空间中的第一行。-n选项禁止标准输出，值输出P的内容。 二、保存空间 模式空间（pattern space）是一块活跃的缓冲区，在sed编辑器执行命令时它会保存待检查的文本。但它并不是sed编辑器保存文本的唯一空间。 sed编辑器有另一块称作保持空间（hold space）的缓冲区域。在处理模式空间中的某些行时，可以用保持空间来临时保存一些行。 p命令是打印当前模式空间的内容 1sed -n '/first/{h;p;n;p;g;p}' data2.txt sed脚本在地址中用正则表达式来过滤出含有单词first的行 当含有单词first的行出现时，h命令将该行放到保持空间 p命令打印模式空间也就是第一个数据行的内容 n命令提取数据流中的下一行（This is the second data line），并将它放到模式空间 p命令打印模式空间的内容，现在是第二个数据行 g命令将保持空间的内容（This is the first data line）放回模式空间，替换当前文本 p命令打印模式空间的当前内容，现在变回第一个数据行了 三、排除命令感叹号命令（!）用来排除（negate）命令，也就是让原本会起作用的命令不起作用。 1234567891011121314#打印模式匹配的行sed -n '/header/p' data2.txt#打印除了模式匹配的行sed -n '/header/!p' data2.txt#N命令会在最后一行结束，因为后面没有更多的行与最后一行合并了sed 'Ns/System\\nAdministrator/Desktop\\nUser/s/System Administrator/Desktop User/' data4.txt#N命令可以作用于除了最后一行的所有行sed '$!Ns/System\\nAdministrator/Desktop\\nUser/s/System Administrator/Desktop User/' data4.txt 1234567cat data2.txt#倒序输出文本内容tac data2.txt#也可以用sed命令实现倒序输出，既打印最后一次的模式空间的内容sed -n '{1!G;h;$p}' data2.txt#可以查看每次模式空间的内容sed -n '{1!G;h;p}' data2.txt 四、改变流（一）、分支将模式匹配的行定义一个标签，满足模式匹配的行执行标签后的命令，不满足模式匹配的行执行非标签后的命令 1234567891011#此处没有定义标签，既满足模式匹配的行不进行任何操作，其他行执行给定的替换操作sed '{2,3b;s/This is/Is this/;s/line./test?/}' data2.txt #以下三种方法效果一样#①sed '{/header/b lable; s/This is/Is this/:lable s/This is/THIS IS/}' data2.txt#②sed '{/header/b lable;s/This is/Is this/;s/line./test?/;:lable s/This is/THIS IS/}' data2.txt #③sed '/header/b lable;s/This is/Is this/;s/line./test?/;:lable s/This is/THIS IS/' data2.txt 上面的例子演示了跳转到sed脚本后面的标签上。也可以跳转到脚本中靠前面的标签上，这样就达到了循环的效果。 12345678910#此方法会形成了一个无穷循环，不停地查找逗号，直到使用Ctrl+C组合键发送一个信号， 手动停止这个脚本$echo &quot;I, like, my, teacher, Li.&quot;|sed -n '{:lables/,//1pb lable}'#加上模式匹配，就会在模式匹配是执行替换命令，无法匹配时停止脚本$echo &quot;I, like, my, teacher, Li.&quot;|sed -n '{:lables/,//1p/,/b lable}' （二）测试测试命令会根据替换命令的结果跳转到某个标签，而不是根据地址进行跳转 测试命令使用与分支命令相同的格式： [address]t [label] 123456789101112#第一个替换命令会查找模式文本first。如果匹配了行中的模式，它就会替换文本，而且测 试命令会跳过后面的替换命令。如果第一个替换命令未能匹配模式，第二个替换命令就会被执行sed '{s/first/matched/ts/This is the/No match on/}' data2.txt#如果替换命令成功匹配并替换了一个模式，测试命令就会跳转到指定的标签。如果替换命令 未能匹配指定的模式，测试命令就不会跳转。#有了测试命令，就能结束之前用分支命令形成的无限循环。 $echo &quot;I, like, my, teacher, Li.&quot;|sed -n '{&gt; :lable&gt; s/,//p&gt; t lable}' 五、模式替代（一）&amp;符号匹配模式中的一个单 词，那就非常简单。 1echo &quot;The cat sleeps in his hat.&quot; | sed 's/cat/&quot;cat&quot;/' 但如果你在模式中用通配符（.）来匹配多个单词呢? 12345#默认替换匹配的第一个 echo &quot;The cat sleeps in his hat.&quot; | sed 's/.at/&quot;.at&quot;/' #加上g就可以全局匹配 echo &quot;The cat sleeps in his hat.&quot; | sed 's/.at/&quot;.at&quot;/g' #但是，只是想对符合.at模式的单词添加引号，而上述方法不能得到理想的结果 &amp;符号可以用来代表替换命令中的匹配的模式。不管模式匹 配的是什么样的文本，你都可以在替代模式中使用&amp;符号来使用这段文本 1echo &quot;The cat sleeps in his hat.&quot; | sed 's/.at/&quot;&amp;&quot;/g' （二）、代替单独的单词当在替换命令中使用圆括号时，必须用转义字符将它们标示为分组字符而不是普通的圆 括号。这跟转义其他特殊字符正好相反。 sed编辑器用圆括号来定义替换模式中的子模式。你可以在替代模式中使用特殊字符来引用 每个子模式。替代字符由反斜线和数字组成。数字表明子模式的位置。sed编辑器会给第一个子 模式分配字符\\1，给第二个子模式分配字符\\2，依此类推。 第一个例子中直接将sleeps替换为空，但其后面的空格依然保留了，用子模式的方法就可以很好的解决。 第二个例子中想要在teacher和Li之间加上，号 六、在脚本中使用sed（一）、使用包装脚本命令行的方法倒序文本内容。 脚本的方法倒序文本内容。 （二）、重定向sed的输出 七、创建sed实用工具（一）、加倍行间距向文本文件的行间插入空白行的简单sed脚本。 G命令会简单地将保持空 间内容附加到模式空间内容后。当启动sed编辑器时，保持空间只有一个空行。将它附加到已有 行后面，你就在已有行后面创建了一个空白行。 如果不想要这个空白行，可以用排除符号（!）和尾行符号（$）来确 保脚本不会将空白行加到数据流的后一行后面。 （二）、对含有空白行的文件加行间距直接用G命令会发现：在原来空白行的位置有了三个空白行。这个问题的解决办法是，首先删除数据流中的所有空白行，然后用G命令在所有行后插入新的空白行。要删除已有的空白行，需要将d命令和一个匹配空白行的模式一起使用。 （三）、给文件中的行编号有些bash shell命令也可以添加行号，但它们会另外加入一些东西（有可能是不需要的间隔）。 可以用等号=来显示数据流中行的行号，但阅读上不友好。 另一种办法是：在获得了等号命令的输出之后，你可以通过管道将输出传给另一个sed编辑器脚本，它会使用N命令来合并这两行。还需要用替换命令将换行符更换成空格或制表符。 （四）、打印末尾行循环N命令和D命令，你在向模式空间的文本行块增加新行的同时也删除了旧行。分支命令非常适合这个循环。要结束循环，只要识别出后一行并用q命令退出就可以了。 第一个sed脚本会首先检查这行是不是数据流中后一行。如果是，退出（quit）命令会停止循环。N命令会将下一行附加到模式空间中当前行之后。如果当前行在第2行后面，3,$D命令会删除模式空间中的第一行。这就会在模式空间中创建出滑动窗口效果。第二个脚本，如果当前行在第3行后面，4,$D命令会删除模式空间中的第一行。第三个脚本，如果当前行在第4行后面，5,$D命令会删除模式空间中的第一行。 （五）、删除行1、删除连续的空白行 区间是/./到/^$/。区间的开始地址会匹配任何含有至少一个字符的行。区间的结束地址会 匹配一个空行。在这个区间内的行不会被删除。 如果开头有空白行的话，开头的空白行也会被删除 2、删除开头的空白行 /./,$!d 这个则表示从第一个字符到最后的内容统统不删除，即删除开头的空白行。 3、删除结尾的空白行 1sed '{ :start /^\\n*$/{$d; N; b start } }' 在正常脚本的花括号里还有花括号。这允许你在整个命令脚本 中将一些命令分组。该命令组会被应用在指定的地址模式上。地址模式能够匹配只含有一个换行 符的行。如果找到了这样的行，而且还是后一行，删除命令会删掉它。如果不是后一行，N 命令会将下一行附加到它后面，分支命令会跳到循环起始位置重新开始。 (六)、删除HTML标签s/&lt;.*&gt;//g 这种匹配模式会删掉This is the page title等。 s/&lt;[^&gt;]*&gt;//g 这种模式才能解决问题。","link":"/sed-%E8%BF%9B%E9%98%B6/"},{"title":"初步认识sed和gwak","text":"想在shell脚本中处理任何类型的数据，就要熟悉Linux中的sed和gawk工具。Linux世界中广泛使用的两个命令行编辑器：sed和gawk sed编辑器 sed编辑器被称作流编辑器（stream editor），和普通的交互式文本编辑器恰好相反。在交互式文本编辑器中（比如vim），你可以用键盘命令来交互式地插入、删除或替换数据中的文本。流编辑器则会在编辑器处理数据之前基于预先提供的一组规则来编辑数据流。 1.再命令行定义编辑器命令 可以直接讲数据通过管道得形式传给sed编辑器。 1echo &quot;This is a test&quot;|sed 's/test/final test' sed编辑器并不会修改文本文件的数据。它只会将修改后的数据发送到 STDOUT。如果你查看原来的文本文件，它仍然保留着原始数据。 2、在命令行使用多个编辑器命令 要在sed命令行上执行多个命令时，只要用-e选项就可以了。1234567#多条命令间用分号隔开，分号和命令间不能有空格sed -e 's/brown/green/;s/dog/cat/' test1.txt#也可以用此提示符来分隔命令sed -e '&gt; s/brown/green/&gt; s/fox/algea/&gt; s/dog/cat/' test1.txt 3、从文件中读取编辑器命令 如果有大量要处理的sed命令，那么将它们放进一个单独的文件中通常会更方便一些。 可以在sed命令中用-f选项来指定文件。 1sed -f script1.sed test1.txt 用.sed作为sed脚本文件的扩展名，以避免与bash shell脚本文件搞混。 gawk程序1、gawk命令格式1gawk options program file 2、从命令行读取程序脚本1gawk 'print &quot;I like teacher Li&quot;' 由于程序脚本被设为显示一行固定的文 本字符串，因此不管你在数据流中输入什么文本，都会得到同样的文本输出。 bash shell提供了一个组合键来生成 EOF（End-of-File）字符。Ctrl+D组合键会在bash中产生一个EOF字符。这个组合键能够终止该gawk 程序并返回到命令行界面提示符下。 3、使用数据字段变量 $0代表整个文本行 $1代表文本行中的第1个数据字段 $2代表文本行中的第2个数据字段 $n代表文本行中的第n个数据字段 12gawk '{print $1}' test1.txtgawk '{print $2}' test1.txt gawk中默认的字段分隔符是任意的空白字符（例如空格或制表符）。 如果你要读取采用了其他字段分隔符的文件，可以用-F选项指定。 /etc/passwd文件用冒号来分隔数字字段，因而如果要划分开每个数据元素，则必须在gawk选项中将冒号指定为字段分隔符。 12cat /etc/passwdgawk -F: '{print $1}' /etc/passwd 4、在程序脚本中使用多个命令123456789#在命令行上的程序脚本中使用多条命令echo &quot;I like my teacher Li&quot;|gawk '{$4=&quot;qinqin&quot;;$5=&quot;meimei&quot;;print $0}'#也可以用次提示符一次一行地输入程序脚本命令gawk '{&gt; $4=&quot;qinqin&quot;&gt; $5=&quot;meimei&quot;&gt; print $0}'I like my teacher Li 因为没有在命令行中指定文件名，gawk程序会 从STDIN中获得数据。当运行这个程序的时候，它会等着读取来自STDIN的文本。要退出程序， 只需按下Ctrl+D组合键来表明数据结束。 5、从文本中读取程序 gawk程序在引用变量值时并未像shell脚本一样使用美元符。 6、在处理数据前运行脚本有时可能需要在处理数据前运行脚本，比如为报告创建标题。BEGIN就可以做到。BEGIN强制gawk在读取数据前执行BEGIN关键字后指定的程序脚本。 这次print命令会在读取数据前显示文本。但在它显示了文本后，它会快速退出，不等待任 何数据。 1gawk 'BEGIN {print &quot;the content of gawk.txt&quot;}' 如果想使用正常的程序脚本中处理数据，必须用另一个脚本区域来定义程序。 12gawk 'BEGIN {print &quot;the content of gawk.txt&quot;}&gt; {print $0}' gawk.txt 7、在处理数据后运行脚本 与BEGIN关键字类似，END关键字允许你指定一个程序脚本，gawk会在读完数据后执行它。 也可以将一组命令组合起来形成一个小脚本文件，同样使用-f选项执行脚本中的命令。","link":"/%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86sed%E5%92%8Cgwak/"},{"title":"可变剪切之SUPPA2","text":"RNA-seq高级分析之可变剪切 可变剪切之SUPPA2SUPPA2参考资料：https://github.com/comprna/SUPPA/wiki/SUPPA2-tutorial#differential-splicing-with-local-events分析流程中所需要得参考基因组、注释文件、R脚本都可以在该链接中找到并下载 下面这段话引自官网，简单了解一下SUPPA2及其功能 SUPPA2 obtains the inclusion levels of AS events exploiting transcript quantification. There are different measures for isoform quantification (counts, TMM, FPKM…). We strongly recommend the use of TPM (Transcripts Per Million), because is normalized for gene length first, and then for sequencing depth. This allows direct comparison between samples (check the following video if you want to know more). There are several tools for transcript quantification (this is an interesting review² on this matter). Each one has different pros and cons and the user is free to use the one that fits better to his requirements. We recommend to use Salmon³, because is one of the fastest methods for quantification without compromising accuracy and it also offers some other interesting options like GC bias correction or bootstrapping among others. The commands indicated here are oriented for using this tools. (一)环境搭建123conda create -n SUPPA2_3.9 python=3.9.1conda activate SUPPA2_3.9conda install -c bioconda salmon suppa matplotlib (二)转录本定量1、建立索引1salmon index -t hg19_EnsenmblGenes_sequence_ensenmbl.fasta -i Ensembl_hg19_salmon_index 2、定量获得TPM值(1)、运行salmon软件定量 123456x=_1y=_2for id in {SRR1513329,SRR1513330,SRR1513331,SRR1513332,SRR1513333,SRR1513334}donohup salmon quant -i /home/gongyuqi/ref/hg38/ensembl/SUPPA_ref/Ensembl_hg19_salmon_index -l ISF --gcBias -1 $id$x.fastq.gz -2 $id$y.fastq.gz -p 20 -o /home/gongyuqi/project/AS/SUPPA2/$id &amp;done 展示一个样本的输出结果结构其中quant.sf文件很重要，要用于后续的分析，展示该文件部分内容 (2)、提取所有样本的TPM值并合并为一个文件 -k indicates the row used as index-f TPM值在第四列，在这里要提取每个样本的TPM值 12suppa_dir=/home/gongyuqi/miniconda3/envs/SUPPA2_3.9/bin/python $suppa_dir/multipleFieldSelection.py -i /home/gongyuqi/project/AS/SUPPA2/*/quant.sf -k 1 -f 4 -o /home/gongyuqi/project/AS/SUPPA2/iso_tpm.txt 查看iso_tpm.txt的内容 (3)、使iso_tpm.txt文件中的转录本id同下载的gtf文件id一致 运行此R脚本，会生成iso_tpm_formatted.txt文件 1Rscript format_Ensembl_ids.R iso_tpm.txt 查看iso_tpm_formatted.txt的内容 三、PSI计算1、根据参考基因组注释文件生成可变剪切事件文件 -i GTF文件-o 输出文件前缀-e 输出文件中包含的可变剪切类型-f 输出的格式 123456789#生成ioe文件gtf_dir=/home/gongyuqi/ref/hg38/ensembl/SUPPA_refpython $suppa_dir/suppa.py generateEvents -i $gtf_dir/Homo_sapiens.GRCh37.75.formatted.gtf -o ensembl_hg19.events -e SE SS MX RI FL -f ioe#合并所有的ioe文件cd $gtf_dirawk ' FNR==1 &amp;&amp; NR!=1 { while (/^&lt;header&gt;/) getline; } 1 {print}' *.ioe &gt; ensembl_hg19.events.ioe 结果如下 2、计算样本的PSI值 123cd /home/gongyuqi/project/AS/SUPPA2ioe_merge_file=~/ref/hg38/ensembl/SUPPA_ref/hg19_event/ensembl_hg19.events.ioenohup python $suppa_dir/suppa.py psiPerEvent -i $ioe_merge_file -e iso_tpm_formatted.txt -o TRA2_events &amp; 查看结果TRA2_events.psi文件 3、boxplot图可视化 -i 输入的PSI矩阵-e 需要可视化的某基因的某种可变剪切类型-g 样品按顺序分组-c 分组名称，1-3好样本为NC组，4-6号样本为KD组-o 结果输出地址 以ENSG00000149554为例进行可视化 12scripts_dir=/home/gongyuqi/project/AS/SUPPA2/scriptspython $scripts_dir/generate_boxplot_event.py -i TRA2_events.psi -e &quot;ENSG00000149554;SE:chr11:125496728-125497502:125497725-125499127:+&quot; -g 1-3,4-6 -c NC,KD -o ./ 四、差异可变剪切分析分组情况negative control siRNA ：SRR1513329,SRR1513330,SRR1513331TRA2A/B siRNA ：SRR1513332,SRR1513333,SRR1513334 1、分别构建两组的TPM和PSI文件 12$scripts_dir/split_file.R iso_tpm_formatted.txt SRR1513329,SRR1513330,SRR1513331 SRR1513332,SRR1513333,SRR1513334 TRA2_NC_iso.tpm TRA2_KD_iso.tpm -i$scripts_dir/split_file.R TRA2_events.psi SRR1513329,SRR1513330,SRR1513331 SRR1513332,SRR1513333,SRR1513334 TRA2_NC_events.psi TRA2_KD_events.psi -e 2、差异分析 -m empirical 选择empirical方法-gc 基因修正--save_tpm_events 参数的添加，会多生成一个TRA2_diffSplice_avglogtpm.tab文件，是后续火山图可视化的输入文件之一 1python $suppa_dir/suppa.py diffSplice -m empirical -gc -i $ioe_merge_file --save_tpm_events -p TRA2_KD_events.psi TRA2_NC_events.psi -e TRA2_KD_iso.tpm TRA2_NC_iso.tpm -o ./test/TRA2_diffSplice 差异分析生成下列3个文件 1cat TRA2_diffSplice.dpsi|grep &quot;ENSG00000149554&quot;|less -N 查看dpsi文件，以ENSG00000149554为例，分析结果中会显示这个基因不同的可变剪切类型在两种处理情况下的psi值及其显著性，红色箭头标记的可变剪切事件在两组中的差异是显著的(p_value&lt;0.05) 当然,上述可变剪切差异分析的结果还可以导入R中进行volcanoplot可视化分析，官网提供了具体的R脚本可视化结果如下 总结：1、技术路线：salmon定量获得各个样本的TPM值——&gt; 合并样本——&gt; 计算PSI值——&gt; 可变剪切差异分析2、拿到最终的差异分析结果，提取出显著的差异可变剪切事件，写脚本将可变剪切事件的id转换成基因名，就得视觉上更友好的文件了。拿到这个文件开展后续的个性化分析。","link":"/%E5%8F%AF%E5%8F%98%E5%89%AA%E5%88%87%E4%B9%8BSUPPA2/"},{"title":"可变剪切分析之MISO","text":"RNA-seq高级分析之可变剪切 可变剪切分析之MISOMISO软件原文档：https://miso.readthedocs.io/en/fastmiso/index.html#installing-fastmiso 数据下载 写一个bash脚本下载数据一次性下载所有的.fastq.gz文件，本次实验数据是单端测序数据123456#！/bin/bashdir=/home/gongyuqi/.aspera/connect/etc/asperaweb_id_dsa.opensshfor id in {84,78,76,72,82,80,74,68} do nohup ascp -QT -l 300m -P33001 -i $dsa era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/SRR974/SRR9749$id/SRR9749$id.fastq.gz . &amp;done 执行脚本 12chmod +x download.shnohup ./download.sh &amp; 比对参考资料：http://www.360doc.com/content/18/0714/20/19913717_770401548.shtml tophat进行转录组的比对时，输出的是bam（二进制的sam）文件。 tophat依赖bowtie2（bowtie1也可以），参考基因组索引用bowtie2-build进行构建。 如果FASTA格式的基因组与索引文件不在同一个文件夹下面，tophat会在运行中自动生成，所以记得把参考基因组和索引文件放在同一个文件夹下面，以减少系统运行时的高消耗。 如果GTF或者GFF格式的基因组注释文件存在， tophat会优先比对到注释文件的转录组上。最好事先准备好转录组索引以节约后续的比对时间。 如果没有事先准备好转录组索引，tophat会在运行过程中生成转录组索引，这样就比较耗时耗力了。 参考基因组索引构建、转录组索引构建 hg19参考基因组gtf文件下载链接：https://www.gencodegenes.org/human/release_36lift37.html 12345#建立hg19参考基因组索引文件nohup time bowtie2-build path/to/hg19.fa ./hg19 &gt; hg19.bowtie_index.log 2&gt;&amp;1 &amp;#建立hg19转录组索引文件#../index/bowtie/hg19为上一步构建的hg19参考基因组索引文件的相对路径及前缀nohup tophat -G ../gencode.v36lift37.annotation.gtf --transcriptome-index=hg19.tr ../index/bowtie/hg19 &amp; 参考基因组的比对 比对方法一123456#以其中一个样本为例，走一波比对转录组流程。#理论上可以节约时间，但是实际跑起来非常耗时！#1G左右得样本跑了4个小时都没有跑完！！！不知道问题出在那儿，但是还是果断kill掉tr_index=/home/gongyuqi/ref/hg19/transcriptome-index/hg19.trgene_index=~/ref/hg19/index/bowtie/hg19tophat2 -o ../aligndata/SRR974980.out --transcriptome-index=$tr -p 30 --phred64-quals $gene_index SRR974980.fastq.gz 比对方法二 写一个比对的bash脚本文件（align.txt），如下12345#!/bin/bashls *.gz|while read iddonohup tophat -G ~/ref/hg19/gencode.v36lift37.annotation.gtf -p 30 -o ../aligndata/$id.out ~/ref/hg19/index/bowtie/hg19 $id &amp;done 执行这个脚本，然后去睡觉，早上醒来看看比对得结果如何~1nohup bash align.txt &amp; 运行结果 每个文件夹打开结果如下 大概了解一下样本的运行日志 注意reads的长度——36。miso后续分析中需要知道这个值注意其中有一个error,至于为什么有这个报错，先不追究注意比对结果的那个统计文件align_summary.txt,查看这个文件发现各个样本比对情况都挺好的。这就是为什么这里暂时不追究那个报错最后会统计样本比对时长 3、bam文件排序及索引文件建立 12ls *.bam|while read id; do samtools sort -@ 30 -m 8G -O bam $id -o ${id%%.*}.accepted_hits.sorted.bam;donels *sorted.bam|while read id; do nohup samtools index -@ 30 $id &amp; done 运行MISO下载hg19的GFF文件并构建索引 具体下载网页及详情：https://miso.readthedocs.io/en/fastmiso/annotation.html 123cd /home/gongyuqi/project/AS/annotation/human/hg19wget hollywood.mit.edu/burgelab/miso/annotations/ver2/miso_annotations_hg19_v2.zipunzip miso_annotations_hg19_v2.zip 这里我们以外显子的gff3为例构建索引 1index_gff --index SE.hg19.gff3 ./index 结果如下 以单端测序样本为例运行miso 这一步生成的结果是很关键的，会计算alternative splicing events的PSI值。而这个在最后可视化的阶段会以posterior distributions over Ψ(PSI)的形式呈现——即sashimi_plot图最右边一列的柱状图。 写一个miso_run的脚本，内容如下 12345678#!/bin/bashcd /home/gongyuqi/project/AS/aligndata/accpted_bamindex_db=/home/gongyuqi/project/AS/annotation/human/hg19/index/out_dir=/home/gongyuqi/project/AS/miso_step1ls *.sorted.bam|while read iddonohup miso --run $index_db $id --output-dir $out_dir/${id%%.*} --read-len 36 &amp;done 运行脚本 1nohup ./miso_run.sh &amp; 举例查看其中一个样本的运行结果 **样本间比较** 这个软件最大的问题在于只能进行两个样本的比较，这一点就很气！！！气归气，还是先运行一下，至少先把流程走完，先学起来。 1compare_miso --compare-samples SRR974978/ SRR974982/ comparison/ 过滤 设置各种参数的阈值，目的是筛选差异显著的alternative splicing events。 12345678filter_events \\--filter SRR974978_vs_SRR974982.miso_bf \\--num-inc 1 \\--num-exc 1 \\--num-sum-inc-exc 10 \\--delta-psi 0.20 \\--bayes-factor 10 \\--output-dir ./filter_out 过滤前后.misoz_bf文件中的alternative splicing events有明显减少 可视化 1234567index_db=/home/gongyuqi/project/AS/annotation/human/hg19/index/#下面做演示的剪切事件是从上一步过滤后的.miso_bf.filtered文件中提取的一个sashimi_plot \\--plot-event &quot;chr20:1093906:1094052:+@chr20:1099396:1099545:+@chr20:1106141:1106293:+&quot; \\$index_db \\sashimi_plot_settings.txt \\--output-dir ./sashimi_plot 其中sashimi_plot_settings.txt是配置文件，内容如下。当然，也可以根据自己的作图需要修改相应的参数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[data]# directory where BAM files arebam_prefix = /home/gongyuqi/project/AS/aligndata/accpted_bam/# directory where MISO output ismiso_prefix = /home/gongyuqi/project/AS/miso_step1bam_files = [ &quot;SRR974978.accepted_hits.sorted.bam&quot;, &quot;SRR974984.accepted_hits.sorted.bam&quot;, &quot;SRR974968.accepted_hits.sorted.bam&quot;, &quot;SRR974972.accepted_hits.sorted.bam&quot;, &quot;SRR974982.accepted_hits.sorted.bam&quot;, &quot;SRR974980.accepted_hits.sorted.bam&quot;, &quot;SRR974974.accepted_hits.sorted.bam&quot;, &quot;SRR974976.accepted_hits.sorted.bam&quot;]miso_files = [ &quot;SRR974978&quot;, &quot;SRR974984&quot;, &quot;SRR974968&quot;, &quot;SRR974972&quot;, &quot;SRR974982&quot;, &quot;SRR974980&quot;, &quot;SRR974974&quot;, &quot;SRR974976&quot;][plotting]# Dimensions of figure to be plotted (in inches)fig_width = 4fig_height = 10# Factor to scale down introns and exons byintron_scale = 30exon_scale = 4# Whether to use a log scale or not when plottinglogged = Falsefont_size = 6# Max y-axisymax = 150# Whether to plot posterior distributions inferred by MISOshow_posteriors = True# Whether to show posterior distributions as bar summariesbar_posteriors = False# Whether to plot the number of reads in each junctionnumber_junctions = Trueresolution = .5posterior_bins = 40gene_posterior_ratio = 5# List of colors for read denisites of each samplecolors = [&quot;#CC0011&quot;, &quot;#CC0011&quot;, &quot;#CC0011&quot;, &quot;#CC0011&quot;, &quot;#FF8800&quot;, &quot;#FF8800&quot;, &quot;#FF8800&quot;, &quot;#FF8800&quot;]# Number of mapped reads in each sample# (Used to normalize the read density for RPKM calculation)coverages = [27232306, 16799279, 31298963, 26308550, 27067722, 17372484, 26095030, 26980460]# Bar color for Bayes factor distribution# plots (--plot-bf-dist)# Paint them bluebar_color = &quot;b&quot;# Bayes factors thresholds to use for --plot-bf-distbf_thresholds = [0, 1, 2, 5, 10, 20] 可视化结果如下因为我们选取的剪切事件是SRR974978和SRR974982之间的差异剪切事件，所以在多个样本间趋势不是很明显。但这里我们先学个方法。","link":"/%E5%8F%AF%E5%8F%98%E5%89%AA%E5%88%87%E4%B9%8BMISO/"},{"title":"特定版本R包下载及其报错","text":"Error: Failed to install ‘unknown package’ from URL:(converted from warning) installation of package ‘dtw’ had non-zero exit status 特定版本R包下载及其报错下载Seurat 2.×.×首先查看版本信息：https://github.com/satijalab/seurat/releases 12require(devtools)install_version(&quot;Seurat&quot;,version = &quot;2.3.3&quot;) 报错如下 12Error: Failed to install 'unknown package' from URL:(converted from warning) installation of package ‘dtw’ had non-zero exit status 显示R包dtw退出码非零，即不正常退出，思考再三，是不是因为这个Seurat依赖的R包没有存在并且下载不成功导致的呢？尝试单独下载R包dtw 1install.packages(&quot;dtw&quot;) 12package ‘dtw’ successfully unpacked and MD5 sums checkedThe downloaded binary packages are in ... 再次下载Seurat 2.×.×12require(devtools)install_version(&quot;Seurat&quot;,version = &quot;2.3.3&quot;) 再次得到报错信息 12Error: Failed to install 'unknown package' from URL:(converted from warning) installation of package ‘doSNOW’ had non-zero exit status 此次报错和上次类似，但变成另一个R包退出码非零，说明上次的排错思路可能是对的。 于是后面每次出现类似的报错就单独下载相应的R包，在尝试下载Seurat 2.3.3。 第N次得到报错信息 用以下方法试图下载Seurat 2.3.3依赖的R包SDMTool 1234567#方法1install.packages(&quot;SDMTools&quot;)#方法2BiocManager::install(&quot;SDMTools&quot;)#方法3source(&quot;https://bioconductor.org/biocLite.R&quot;)biocLite(&quot;SDMTools&quot;) 遗憾的是，报错了，报错信息都是以下内容 😭你时常感概，国内的网怎么老是不能通向世界呢😭 1package ‘SDMTools’ is not available (for R version 3.5.2) 所以只好手动下载SDMTools,手动安装 首先登陆CRAN官网，点击进入Download R for Windows（这个视不同系统而定） 点击contrib 点击相应的R版本，进入R包界面，ctrl+F搜索所需要的R包，下载到本地 进入Rstudio,点击Tools,点击install packages…或者直接点击environment下的install 然后点击install即可 当你再次下载Seurat 2.3.3时，遇上第N+1次报错当你解决了上诉报错问题后，你还会收到各种各样的报错，比如缺少这个包，缺少那个包。你就当体验一次手动解决R包之间的依赖关系好了。直到不再提示缺少什么R包。 再再再次次次下载Seurat 2.3.312require(devtools)install_version(&quot;Seurat&quot;,version = &quot;2.3.3&quot;) 成功！！！ 加载Seurat并查看相关信息12library(Seurat)sessionInfo()","link":"/%E7%89%B9%E5%AE%9A%E7%89%88%E6%9C%ACR%E5%8C%85%E4%B8%8B%E8%BD%BD%E5%8F%8A%E5%85%B6%E6%8A%A5%E9%94%99/"},{"title":"磁盘管理 （分区、格式化、挂载、卸载）","text":"磁盘管理（基础篇） 磁盘管理 （分区、格式化、挂载、卸载）磁盘结构 磁盘的0磁道0柱面1扇区存储的是MBR（主引导记录）信息，位于最外圈。 MBR共512个字节，446的主引导程序+64的分区表+2的魔数（分区结束符）组成。一个分区占16个字节，64个字节的分区表，所以主分区和扩展分区加起来不能超过4个。 磁盘表示 /dev/sda1/dev 设备文件目录sd：sata,sas,usb,scsi接口硬盘 hd：代表IDE接口的硬盘a：linux用字母表示第几块磁盘，a代表第一块1：linux用数字表示某块磁盘的第几个分区 磁盘使用概述 磁盘初始化MBR OR GPT 磁盘分区一个磁盘只能分4个分区：主分区+扩展分区&lt;=4 格式化文件系统主要负责数据如何存储在硬盘上 磁盘挂载设备必须经过挂在才可以使用 分区介绍计算机中存放信息的主要的存储设备就是硬盘，但是硬盘不能直接使用，必须对硬盘进行分割，分割成一块一块的硬盘区域就是磁盘分区。传统的磁盘管理中，将一个硬盘分为两大类分区：主分区和扩展分区。主分区是能够安装操作系统，能够进行计算机启动的分区，这样的分区可以直接格式化，然后安装操作系统，直接存放文件。扩展分区无法直接使用，必须在扩展分区中再次划分逻辑驱动器，才可以经过格式化后存放数据。 fdisk分区命令linux fdisk是一个创建和维护分区表的程序，它兼容DOS类型的分区表、BSD或者SUN类型的磁盘列表。语法：fdisk [命令选项][参数]必要选项：-l 列出所有分区表参数：可以是指定的磁盘，如果不指定，默认列出所有磁盘 一共3块磁盘：/dev/sda、/dev/sdb、/dev/sdc /dev/sda这块磁盘进行了分区，带*的分区/dev/sda1表示系统启动盘。Id：83表示linux的基本分区，Id：8e表示linux的逻辑卷分区。System:Extended表示扩展分区。System:Linux LVM表示逻辑卷分区。 /dev/sdb、/dev/sdc这两块盘并没有进行磁盘分区。 /dev/mapper/centos-root为卷组。 /dev/mapper/centos-swap为交换分区。 主分区可以直接格式化存数据，扩展分区要划逻辑驱动器，对逻辑驱动器进行格式化后才能存数据。 磁盘分区练习 分一个主分区 分一个扩展分区，并分两个逻辑驱动器 分一个交换分区 添加一个主分区（500M）。 添加一个扩展分区（剩下所有的空间）。此时再想添加一个主分区，就会发现扇区不够了。 SWAP（交换）分区是一种通过在硬盘中预先划分一定的空间，然后将把内存中暂时不常用的数据临时存放到硬盘中，以便腾出物理内存空间让更活跃的程序服务来使用的技术，其设计目的是为了解决真实物理内存不足的问题。但由于交换分区毕竟是通过硬盘设备读写数据的，速度肯定要比物理内存慢，所以只有当真实的物理内存耗尽后才会调用交换分区的资源。 1fdisk /dev/sdb 添加两个逻辑驱动器 /dev/sdb5 /dev/sdb6 各分500M空间 添加一个交换分区 查看/dev/sdb磁盘分区结果 磁盘格式化格式化 格式化是指对磁盘或磁盘中的分区进行初始化的一种操作，这种操作通常会导致现有的磁盘或分区中所有的文件被清楚。格式化就相当于给磁盘装一个大管家，这位大管家负责数据怎么在硬盘中读入读出。但是这位大管家在接管这块硬盘的时候会一股脑清除里面原来的东西，接着再按照自己的方式管理这块磁盘。 格式化通常分为低级格式化和高级格式化。如果没有特别指明，对硬盘的格式化通常是指高级格式化。 文件系统 文件系统：负责管理和存储数据的系统数据是以什么方式存在于硬盘，又是以什么方式读出的。 文件系统类型ext2、ext3、ext4、xfs mkfsmkfs [options] device命令选项-t：指定文件系统类型 练习 主分区格式化为ext4 两个逻辑分区分别格式化为xfs、ext3 交换分区的格式化 12#将/dev/sdb1主分区格式化为ext4文件系统mkfs -t ext4 /dev/sdb1 12#将/dev/sdb5逻辑分区格式化为xfs文件系统 mkfs -t xfs /dev/sdb5 12#将/dev/sdb6逻辑分区格式化为ext3文件系统mkfs -t ext3 /dev/sdb6 12#将/dev/sdb7交换分区格式化mkswap /dev/sdb7 磁盘挂载挂载 linux中的所有设备必须经过挂载才可以被用户使用 挂载的方式其实是将某个设备挂到文件系统的某个文件夹 mount 命令语法mount device directory 命令选项-a：挂载所有文件系统，参考文件/etc/fstab-l：显示当前挂载-t：文件系统类型-o：指定挂载权限 12#查看当前系统的挂载信息，发现并没有/dev/sdb相关的挂载信息mount -l 手动挂载/dev/sdb1、/dev/sdb5、/dev/sdb6系统重启后，手动挂载方式挂载的分区不会生效 123456789mkdir /opt/sdb1mkdir /opt/sdb5mkdir /opt/sdb6mount /dev/sdb1 /opt/sdb1 #mount -t ext4 /dev/sdb1 /opt/sdb1mount /dev/sdb5 /opt/sdb5 #mount -t xfs /dev/sdb5 /opt/sdb5mount /dev/sdb6 /opt/sdb6 #mount -t ext3 /dev/sdb6 /opt/sdb6#不使用-t参数指定文件系统类型，系统也会自动识别文件系统类型#查看当前系统挂载信息，发现多出了/dev/sdb相关信息mount -l umount umount：卸载文件系统 umount 设备挂载点|设备源-l 加上-l参数进行卸载即——在该设备处于使用中是不进行卸载，在该设备处于空闲时进行卸载 123456#卸载3个挂载点umount /dev/sdb1 #umount /opt/sdb1umount /dev/sdb5 #umount /opt/sdb5umount /dev/sdb6 #umount /opt/sdb6#查看系统当前挂载信息,发现/dev/sdb相关信息消失mount -l | tail 1234#尝试对/dev/sdb5进行只读挂载mount -o ro /dev/sdb5 /opt/sdb#查看当前挂载信息mount -l | tail 123#检测/dev/sdb5的权限cd /opt/sdb5touch test.txt 自动挂载 /etc/fstab文件设备 挂载点 文件系统 权限 备份 检测权限 auto：系统自动挂载，fstab默认就是这个选项 defaults：rw、suid、dev、exec、auto、nouser、async noauto：开机不自动挂载 nouser：只有超级用户可以挂载 ro：按只读权限挂载 rw：按可读可写权限挂载 user：任何用户都可以挂载备份 1：允许dump备份程序备份 0：忽略备份操作fsck磁盘检测设置 0：永远不检测 /目录分区永远都是1 2：其他分区从2开始，数字越小越先检查，如果两个分区的数字相同，则同时检查 练习 自动挂载一下分区 主分区 格式化ext4 /opt/sdb1 扩展分区中的两个逻辑驱动器 格式化为xfs ext3 /opt/sdb[5-6] 交换分区 swap 12#查看以下当前交换分区的大小free -m 想要设备文件的挂载永久生效，就需要把挂载的信息写入配置文件/etv/fstab。 1234567891011vim /etc/fstab#添加以下内容到/etc/fstab文件中#/dev/sdb/dev/sdb1 /opt/sdb1 ext4 defaults 0 0/dev/sdb5 /opt/sdb5 xfs ro 0 0/dev/sdb6 /opt/sdb6 ext3 rw 0 0/dev/sdb7 swap swap defaults 0 0#挂载mount -a#查看挂载系统当前挂载信息,主分区和两个逻辑分区都挂载成功mount -l 123456#使用设备名（/dev/sdb[number]）的方式挂载可能会出现找不到设备而加载失败的问题。uuid作为系统中存储设备提供的唯一标志字符串，使用UUID则不会出现这样的情况 #查看uuid的3种方法 blkidlsblkll /dev/disk/by-uuid#也就是说，将上述/dev/sdb1换成其对应的uuid，其他的不变 123456789101112#查看当前交换分区大小，Swap分区大小没有变化，mount -a不能挂载swap分区free -m``` &lt;img src=&quot;https://blog-image-host.oss-cn-shanghai.aliyuncs.com/gyqblog/freem1.JPG&quot;/&gt; ```bash#swap分区的挂载方式swapon -a #会自动读取/etc/fstab文件中的分区挂载信息#也可以通过查看交换分区的大小,相比之前，Swap大小增加了1Gfree -m 另外两个命令df、du12#df命令用于显示目前在Linux系统上的文件系统的磁盘使用情况统计df -Th 12#du查看文件大小du -h /addfirst/reference/mouse/index/bowtie2-build-index/genome.1.bt2 12#du -sh查看文件夹大小du -sh /addfirst/reference/ 卸载分区（以/dev/sdb为例）1234567#查看当前分区情况df -lh#卸载/dev/sdb5分区，但是重启后又会恢复到挂载状态umount /dev/sdb5#查看当前分区情况，可以看到/dev/sdb5分区消失df -lh#卸载了还可以再挂载，数据都还在 删除分区1fdisk /dev/sdb 12#查看/dev/sdb的分区信息fdisk -l /dev/sdb 磁盘卸载12345#删除掉配置文件/etc/fstab中/dev/sdb的相关信息vim /etc/fstabreboot#重启后查看是否卸载df -lh 😄然后关机，就可以删掉相应的硬盘，重新开机后一切正常。 参考资料https://www.linuxprobe.com/chapter-06.htmlhttps://www.bilibili.com/video/av76344853?p=36https://www.cnblogs.com/maohai-kdg/p/12067101.html","link":"/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"},{"title":"可变剪切之rMART","text":"RNA-seq高级分析之可变剪切 可变剪切之rMART官方参考资料http://rnaseq-mats.sourceforge.net/rMATS:https://github.com/Xinglab/rmats-turbo/blob/v4.1.0/README.mdrmats2sashimiplot:https://github.com/Xinglab/rmats2sashimiplot (一)rMATS软件及相关依赖包的下载12345678910111213141516#先用conda下载rMATs软件需要的依赖包#有时候python版本问题搞得我很烦，有些生信软件依赖特定的python版本，所以在软件安装过程中多加注意这个问题conda create rMATs2.7 python=2.7conda activate rMATs2.7conda install -y Cython=0.29.14 numpy=1.16.5 blas lapack gsl=2.6 gcc=5.4.0 CMake=3.15.4conda install libgfortran==1deactivate rMATs2.7#下载软件并解压rMATs软件#rMATs不同版本下载网址：https://github.com/Xinglab/rmats-turbo/releases/tag/v4.1.0cd /home/gongyuqi/project/AS/rMATSwget https://github.com/Xinglab/rmats-turbo/releases/download/v4.1.0/rmats_turbo_v4_1_0_python_2_7.tar.gztar -zxvf rmats_turbo_v4_1_0_python_2_7.tar.gz#永久添加路径到环境变量中export PATH=/home/gongyuqi/project/AS/rMATS/rmats-turbo/:$PATHsource ~/.bashrc (二)STAR比对软件的使用这么多比对软件，为什么选STAR？因为官网用的STAR！据说其他可以做可变剪切的比对软件生成的bam文件，rMATs运行不了~~~ 1、构建参考基因组索引 123456#激活比对的环境,这个环境里面我下载了各种比对软件，python版本为2.7.15.conda activate alignment2.7 nohup STAR --runThreadN 30 --runMode genomeGenerate \\--genomeDir /home/gongyuqi/ref/hg38/index/star \\--genomeFastaFiles /home/gongyuqi/ref/hg38/Homo_sapiens.GRCh38.dna.primary_assembly.fa \\--sjdbGTFfile /home/gongyuqi/ref/hg38/Homo_sapiens.GRCh38.102.gtf &amp; 2、用STAR软件进行比对 STAR的使用以及输出文件解读参考资料：https://blog.csdn.net/yssxswl/article/details/105703869https://www.jianshu.com/p/eca16bf2824e 123456789101112#进入fastq文件坐在目录，这里是原始的fastq数据，质控结果挺好的，不用再过滤了，所以我直接比对了cd /home/gongyuqi/project/AS/rawdata#写循环进行比对ls *.gz|while read iddonohup STAR --runThreadN 20 \\--genomeDir /home/gongyuqi/ref/hg38/index/star \\--readFilesCommand gunzip -c \\--readFilesIn ./$id \\--outSAMtype BAM SortedByCoordinate \\--outFileNamePrefix /home/gongyuqi/project/AS/aligndata_star/${id%%.*}. &amp;done 3、比对情况统计并查看 (1)直接查看 1ls *.Log.progress.out|while read id;do sed -n '1,3p' $id;done 结果如下，比对情况还是不错的，可以进行后续的分析。 (2)flagstat统计比对情况查看比对情况 12345ls *.bam|while read iddonohup samtools flagstat -@ 20 $id &gt; ${id%%.*}.flagstat.txt &amp;donels *.txt|while read id;do cat $id | grep &quot;mapped (&quot;;done 这个比对结果实在是难以置信！！但是结合上面的比对结果，还是可以相信比对这一步是没有问题的。 (三)运行rMATs123456789101112131415#首先我们要失活掉比对的环境（alignment2.7），激活rMATs环境conda deactivate alignment2.7conda activate rMATs2.7#虽然rMATS路径被永久添加到环境变量中，但是PATH只认可执行文件，所以这一步还是要切换到rMATs路径下cd /home/gongyuqi/project/AS/rMATS/rmats-turboinput_dir=/home/gongyuqi/project/AS/aligndata_starpython rmats.py \\--b1 $input_dir/b1.txt \\--b2 $input_dir/b2.txt \\--gtf /home/gongyuqi/ref/hg38/ensembl/Homo_sapiens.GRCh38.102.gtf \\-t single --readLength 36 --nthread 20 \\--od /home/gongyuqi/project/AS/rMATS/rmats_output \\--tmp /home/gongyuqi/project/AS/rMATS/rmats_tmp#运行完记得退出当前环境，因为后面我们要用另外一个环境conda deactivate rMATs2.7 运行上面的代码，会报如下错误： 我查看了一下，/home/gongyuqi/miniconda3/envs/rMATs2.7/lib路径下有这个文件呢！为什么电脑找不到？触及到了我的知识盲区了~~~查了一下午的资料，发现这个报错涉及到查找共享库（动态链接库）问题，根据查到的东西操作了一波居然没有解决，旁边的零食都不香了！😫算了~~~我还是先去吃晚饭吧，已经饿得很瘦了！😫。。。。。。好啦，我吃完饭回来了。我继续倒腾，不放弃。。。。。。。额，我知道了，这段代码运行过程中还涉及到gcc编译，因为它依赖包中有GCC。这两个问题实在是太专业了，今天又学习到了。现在有一点小小的开心！😊解决方法参考资料：https://www.jianshu.com/p/a62e1d327023 12345#Linux默认只会在指定的几个目录找共享库（动态链接库），其他目录的要自己加在LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/home/gongyuqi/miniconda3/envs/rMATs2.7/lib#因为上述命令是依赖gcc的，需要gcc进行编译，所以还需要下面的操作export LIBRARY_PATH=/home/gongyuqi/miniconda3/envs/rMATs2.7/lib#注意，上述都是临时的，治标不治本，下次执行运行rMATs时，依然要重复上述操作。当然你也可以将其永久性添加。 运行结果如下，关于这些结果文件的解释说明，可以参考rMATS官网，上面有很详细的阐述。 (四)rmats2sashimiplot可视化终于到了激动人心的可视化阶段了，一顿操作之后，产生一张漂亮的图放PPT上还是很欣慰的。 1、环境搭建 123conda create -n rmats2sashimiplot python=2.7 conda activate rmats2sashimiplotconda install -y numpy scipy matplotlib pysam samtools bedtools 2、准备可视化的txt文件 以Exon Skipping为例演示,这里我随便找一个基因进行可视化，注意了，这个基因不一定是差异可变剪切的基因哟，因为是我随便找的一个。嘻嘻🤭 12sed -n '2p' SE.MATS.JC.txt &gt; SE.plot.PRMT2.txtcat SE.plot.PRMT2.txt 文件内容如下： 3、可视化的txt文件 一开始，我非常自信的运行了下面的代码~12345678910gff3=/home/gongyuqi/ref/hg38/ensembl/Homo_sapiens.GRCh38.102.gff3output_dir=/home/gongyuqi/project/AS/rMATS/rmats2sashimiplot/rmats2sashimiplot \\--b1 SRR974968.Aligned.sortedByCoord.out.bam,SRR974972.Aligned.sortedByCoord.out.bam,SRR974978.Aligned.sortedByCoord.out.bam,SRR974984.Aligned.sortedByCoord.out.bam \\--b2 SRR974974.Aligned.sortedByCoord.out.bam,SRR974976.Aligned.sortedByCoord.out.bam,SRR974980.Aligned.sortedByCoord.out.bam,SRR974982.Aligned.sortedByCoord.out.bam \\-c chr21:+:46636438:46636547:$gff3 \\-e /home/gongyuqi/project/AS/rMATS/rmats_output/SE.plot.PRMT2.txt \\--l1 treatment --l2 control \\--exon_s 1 --intron_s 5 -t SE \\-o $output_dir/test 但是，我遇到了如下报错 我回头看了一下我的bam文件，染色体是以1,2…X,Y形式体现而不是chr1,chr2…chrX,chrY形式体现。我第一次遇到染色体号的问题呢。我要怎么让bam文件中有chr存在且让其存在在正确的位置呢。我绞尽脑汁的想了很久很久😫，我还考虑到是不是我参考基因组的问题，不同数据库参考基因组是不是有差异，我要不要换一个数据库的参考基因组。然后我一顿操作，无果~~~算了，我打一局游戏睡觉吧，明天再想。起床吃早饭骑车去实验室思考查资料有一点小想法了我实践一下看看可不可参考资料：https://www.jianshu.com/p/5ceda0350d0d 第一步：给bam文件添加chr12345#添加chrls *.bam|while read iddonohup samtools view -h $id | sed -e '/^@SQ/s/SN\\:/SN\\:chr/' -e '/^[^@]/s/\\t/\\tchr/2'|awk -F ' ' '$7=($7==&quot;=&quot; || $7==&quot;*&quot;?$7:sprintf(&quot;chr%s&quot;,$7))' |tr &quot; &quot; &quot;\\t&quot; | samtools view -h -b -@ 10 -S - &gt; ${id%%.*}.Aligned.sortedByCoord.out.chr.bam &amp;done 第二步：使用添加chr的bam文件执行程序12345678910gff3=/home/gongyuqi/ref/hg38/ensembl/Homo_sapiens.GRCh38.102.gff3output_dir=/home/gongyuqi/project/AS/rMATS/rmats2sashimiplot/rmats2sashimiplot \\--b1 SRR974968.Aligned.sortedByCoord.out.chr.bam,SRR974972.Aligned.sortedByCoord.out.chr.bam,SRR974978.Aligned.sortedByCoord.out.chr.bam,SRR974984.Aligned.sortedByCoord.out.chr.bam \\--b2 SRR974974.Aligned.sortedByCoord.out.chr.bam,SRR974976.Aligned.sortedByCoord.out.chr.bam,SRR974980.Aligned.sortedByCoord.out.chr.bam,SRR974982.Aligned.sortedByCoord.out.chr.bam \\-c chr21:+:46636438:46636547:$gff3 \\-e /home/gongyuqi/project/AS/rMATS/rmats_output/SE.plot.PRMT2.txt \\--l1 treatment --l2 control \\--exon_s 1 --intron_s 5 -t SE \\-o $output_dir/SE.plot 可视化结果如下（你会发现，咦~两组间差异不显著呀？因为我随便挑了一个基因可视化呀！我有偷偷可视化某个基因，可视化出来的差异性与我定量和半定量qPCR的结果一致哟，所以上述分析流程应该是OK的~~~嘻嘻嘻🤭不管怎样，这个基因确实存在Exon Skipping。） 总结技术上1、需要搭建两个重要的环境：运行rMATs的环境，运行rmats2sashimiplot的环境。2、选用STAR进行参考基因组的比对（注意了，参考基因组和注释文件的版本要一致哟）。3、在rMATs的环境下,结合上述生成的bam文件，运行rMATs软件，生成可变剪切事件的文件。4、在rmats2sashimiplot环境下，可视化上述生成的可变剪切事件。 心态上上午解决不了就下午解决，今天解决不了就明天解决。这个方法不行就换个方法。思考良久之后可以适当的问问老师。最关键的还是要按时吃饭按时睡觉。 SRR974972.Aligned.sortedByCoord.out.chr.bam, 12345678910gff3=/home/gongyuqi/ref/hg38/ensembl/Homo_sapiens.GRCh38.102.gff3output_dir=/home/gongyuqi/project/AS/rMATS/rmats2sashimiplot \\--b1 SRR974968.Aligned.sortedByCoord.out.chr.bam,SRR974978.Aligned.sortedByCoord.out.chr.bam,SRR974984.Aligned.sortedByCoord.out.chr.bam \\--b2 SRR974974.Aligned.sortedByCoord.out.chr.bam,SRR974976.Aligned.sortedByCoord.out.chr.bam,SRR974980.Aligned.sortedByCoord.out.chr.bam,SRR974982.Aligned.sortedByCoord.out.chr.bam \\-c chr3:+:191375061:191375589:$gff3 \\-e /home/gongyuqi/project/AS/rMATS/ABC_GCB/rmats_output/SE.plot.CCDC50.txt \\--l1 ABC --l2 GCB \\--exon_s 1 --intron_s 5 -t SE \\-o $output_dir/CCDC50_SE_new.plot","link":"/%E5%8F%AF%E5%8F%98%E5%89%AA%E5%88%87%E4%B9%8BMART/"},{"title":"test","text":"Brief Content","link":"/test/"}],"tags":[{"name":"NGS","slug":"NGS","link":"/tags/NGS/"},{"name":"tag1","slug":"tag1","link":"/tags/tag1/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"notes","slug":"notes","link":"/tags/notes/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"System","slug":"System","link":"/tags/System/"}],"categories":[{"name":"Programming Language","slug":"Programming-Language","link":"/categories/Programming-Language/"}],"pages":[{"title":"about","text":"This is Gong Yuqi’s personal blog, build with love 💗","link":"/about/index.html"}]}